<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Les Travaux Pratiques du cours Big Data"><meta name=author content="Lilia Sfaxi"><link href=http://INSATunisia.github.io/TP-BigData/tp1/ rel=canonical><link rel=icon href=../img/favicon.ico><meta name=generator content="mkdocs-1.2.3, mkdocs-material-8.1.10"><title>TP1 - Le traitement Batch avec Hadoop HDFS et Map Reduce - TP Big Data</title><link rel=stylesheet href=../assets/stylesheets/main.d6be258b.min.css><link rel=stylesheet href=../assets/stylesheets/palette.e6a45f82.min.css><meta name=theme-color content=#2094f3><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../css/timeago.css><link rel=stylesheet href=../stylesheets/extra.css><link rel=stylesheet href=../stylesheets/links.css><script>__md_scope=new URL("..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme data-md-color-primary=blue data-md-color-accent=yellow> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#telecharger-pdf class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=.. title="TP Big Data" class="md-header__button md-logo" aria-label="TP Big Data" data-md-component=logo> <img src=../img/logo.png alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> TP Big Data </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> TP1 - Le traitement Batch avec Hadoop HDFS et Map Reduce </span> </div> </div> </div> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg> </button> </nav> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/INSATunisia/TP-BigData/ title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg> </div> <div class=md-source__repository> INSATunisia/TP-BigData </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=.. title="TP Big Data" class="md-nav__button md-logo" aria-label="TP Big Data" data-md-component=logo> <img src=../img/logo.png alt=logo> </a> TP Big Data </label> <div class=md-nav__source> <a href=https://github.com/INSATunisia/TP-BigData/ title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg> </div> <div class=md-source__repository> INSATunisia/TP-BigData </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=.. class=md-nav__link> Travaux Pratiques Big Data </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" data-md-toggle=toc type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> TP1 - Le traitement Batch avec Hadoop HDFS et Map Reduce <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> TP1 - Le traitement Batch avec Hadoop HDFS et Map Reduce </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#telecharger-pdf class=md-nav__link> Télécharger PDF </a> </li> <li class=md-nav__item> <a href=#objectifs-du-tp class=md-nav__link> Objectifs du TP </a> </li> <li class=md-nav__item> <a href=#outils-et-versions class=md-nav__link> Outils et Versions </a> </li> <li class=md-nav__item> <a href=#hadoop class=md-nav__link> Hadoop </a> <nav class=md-nav aria-label=Hadoop> <ul class=md-nav__list> <li class=md-nav__item> <a href=#presentation class=md-nav__link> Présentation </a> </li> <li class=md-nav__item> <a href=#hadoop-et-docker class=md-nav__link> Hadoop et Docker </a> </li> <li class=md-nav__item> <a href=#installation class=md-nav__link> Installation </a> </li> <li class=md-nav__item> <a href=#premiers-pas-avec-hadoop class=md-nav__link> Premiers pas avec Hadoop </a> </li> <li class=md-nav__item> <a href=#interfaces-web-pour-hadoop class=md-nav__link> Interfaces web pour Hadoop </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#map-reduce class=md-nav__link> Map Reduce </a> <nav class=md-nav aria-label="Map Reduce"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#presentation_1 class=md-nav__link> Présentation </a> </li> <li class=md-nav__item> <a href=#wordcount class=md-nav__link> Wordcount </a> <nav class=md-nav aria-label=Wordcount> <ul class=md-nav__list> <li class=md-nav__item> <a href=#tester-map-reduce-en-local class=md-nav__link> Tester Map Reduce en local </a> </li> <li class=md-nav__item> <a href=#lancer-map-reduce-sur-le-cluster class=md-nav__link> Lancer Map Reduce sur le cluster </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#homework class=md-nav__link> Homework </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../tp2/ class=md-nav__link> TP2 - Traitement par Lot et Streaming avec Spark </a> </li> <li class=md-nav__item> <a href=../tp3/ class=md-nav__link> TP3 - La Collecte de Données avec le Bus Kafka </a> </li> <li class=md-nav__item> <a href=../tp4/ class=md-nav__link> TP4 - Stockage de Données dans une Base NOSQL avec HBase </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#telecharger-pdf class=md-nav__link> Télécharger PDF </a> </li> <li class=md-nav__item> <a href=#objectifs-du-tp class=md-nav__link> Objectifs du TP </a> </li> <li class=md-nav__item> <a href=#outils-et-versions class=md-nav__link> Outils et Versions </a> </li> <li class=md-nav__item> <a href=#hadoop class=md-nav__link> Hadoop </a> <nav class=md-nav aria-label=Hadoop> <ul class=md-nav__list> <li class=md-nav__item> <a href=#presentation class=md-nav__link> Présentation </a> </li> <li class=md-nav__item> <a href=#hadoop-et-docker class=md-nav__link> Hadoop et Docker </a> </li> <li class=md-nav__item> <a href=#installation class=md-nav__link> Installation </a> </li> <li class=md-nav__item> <a href=#premiers-pas-avec-hadoop class=md-nav__link> Premiers pas avec Hadoop </a> </li> <li class=md-nav__item> <a href=#interfaces-web-pour-hadoop class=md-nav__link> Interfaces web pour Hadoop </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#map-reduce class=md-nav__link> Map Reduce </a> <nav class=md-nav aria-label="Map Reduce"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#presentation_1 class=md-nav__link> Présentation </a> </li> <li class=md-nav__item> <a href=#wordcount class=md-nav__link> Wordcount </a> <nav class=md-nav aria-label=Wordcount> <ul class=md-nav__list> <li class=md-nav__item> <a href=#tester-map-reduce-en-local class=md-nav__link> Tester Map Reduce en local </a> </li> <li class=md-nav__item> <a href=#lancer-map-reduce-sur-le-cluster class=md-nav__link> Lancer Map Reduce sur le cluster </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#homework class=md-nav__link> Homework </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/INSATunisia/TP-BigData/edit/master/docs/tp1.md title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg> </a> <h1>TP1 - Le traitement Batch avec Hadoop HDFS et Map Reduce</h1> <p><center><img alt="Batch Processing" src=../img/batch.png></center></p> <h3 id=telecharger-pdf>Télécharger PDF<a class=headerlink href=#telecharger-pdf title="Permanent link">&para;</a></h3> <p><a href=../tp1.pdf><img alt="Download TP1" src=../img/pdf.png></a></p> <h3 id=objectifs-du-tp>Objectifs du TP<a class=headerlink href=#objectifs-du-tp title="Permanent link">&para;</a></h3> <p>Initiation au framework hadoop et au patron MapReduce, utilisation de docker pour lancer un cluster hadoop de 3 noeuds.</p> <h3 id=outils-et-versions>Outils et Versions<a class=headerlink href=#outils-et-versions title="Permanent link">&para;</a></h3> <ul> <li><a href=http://hadoop.apache.org/ >Apache Hadoop</a> Version: 3.3.6.</li> <li><a href=https://www.docker.com/ >Docker</a> Version <em>latest</em></li> <li><a href=https://code.visualstudio.com/ >Visual Studio Code</a> Version 1.85.1 (ou tout autre IDE de votre choix)</li> <li><a href=http://www.oracle.com/technetwork/java/javase/downloads/index.html>Java</a> Version 1.8.</li> <li>Unix-like ou Unix-based Systems (Divers Linux et MacOS)</li> </ul> <h3 id=hadoop>Hadoop<a class=headerlink href=#hadoop title="Permanent link">&para;</a></h3> <h4 id=presentation>Présentation<a class=headerlink href=#presentation title="Permanent link">&para;</a></h4> <p><a href=hadoop.apache.org>Apache Hadoop</a> est un framework open-source pour stocker et traiter les données volumineuses sur un cluster. Il est utilisé par un grand nombre de contributeurs et utilisateurs. Il a une licence Apache 2.0.</p> <p><center><img src=../img/tp1/hadoop.png width=200></center></p> <h4 id=hadoop-et-docker>Hadoop et Docker<a class=headerlink href=#hadoop-et-docker title="Permanent link">&para;</a></h4> <p>Pour déployer le framework Hadoop, nous allons utiliser des contenaires <a href=https://www.docker.com/ >Docker</a>. L'utilisation des contenaires va garantir la consistance entre les environnements de développement et permettra de réduire considérablement la complexité de configuration des machines (dans le cas d'un accès natif) ainsi que la lourdeur d'exécution (si on opte pour l'utilisation d'une machine virtuelle).</p> <p>Nous avons pour le déploiement des ressources de ce TP suivi les instructions présentées <a href=https://github.com/kiwenlau/hadoop-cluster-docker>ici</a>.</p> <h4 id=installation>Installation<a class=headerlink href=#installation title="Permanent link">&para;</a></h4> <p>Nous allons utiliser tout au long de ces TP trois contenaires représentant respectivement un noeud maître (Namenode) et deux noeuds workers (Datanodes).</p> <p>Vous devez pour cela avoir installé docker sur votre machine, et l'avoir correctement configuré. Ouvrir la ligne de commande, et taper les instructions suivantes:</p> <ol> <li>Télécharger l'image docker uploadée sur dockerhub: <div class=highlight><pre><span></span><code>docker pull liliasfaxi/hadoop-cluster:latest
</code></pre></div></li> <li> <p>Créer les trois contenaires à partir de l'image téléchargée. Pour cela:</p> <p>2.1. Créer un réseau qui permettra de relier les trois contenaires: <div class=highlight><pre><span></span><code>docker network create --driver<span class=o>=</span>bridge hadoop
</code></pre></div> 2.2. Créer et lancer les trois contenaires (les instructions -p permettent de faire un mapping entre les ports de la machine hôte et ceux du contenaire):</p> <p><div class=highlight><pre><span></span><code>docker run -itd --net<span class=o>=</span>hadoop -p <span class=m>9870</span>:9870 -p <span class=m>8088</span>:8088 -p <span class=m>7077</span>:7077 -p <span class=m>16010</span>:16010 --name hadoop-master --hostname hadoop-master liliasfaxi/hadoop-cluster:latest

docker run -itd -p <span class=m>8040</span>:8042 --net<span class=o>=</span>hadoop --name hadoop-worker1 --hostname hadoop-worker1 liliasfaxi/hadoop-cluster:latest

docker run -itd -p <span class=m>8041</span>:8042 --net<span class=o>=</span>hadoop --name hadoop-worker2 --hostname hadoop-worker2 liliasfaxi/hadoop-cluster:latest
</code></pre></div> 2.3. Vérifier que les trois contenaires tournent bien en lançant la commande <code>docker ps</code>. Un résultat semblable au suivant devra s'afficher:</p> <p><center><img src=../img/tp1/running.png></center></p> </li> <li> <p>Entrer dans le contenaire master pour commencer à l'utiliser.</p> <div class=highlight><pre><span></span><code>docker <span class=nb>exec</span> -it hadoop-master bash
</code></pre></div> </li> </ol> <p>Le résultat de cette exécution sera le suivant:</p> <div class=highlight><pre><span></span><code>root@hadoop-master:~#
</code></pre></div> <p>Vous vous retrouverez dans le shell du namenode, et vous pourrez ainsi manipuler le cluster à votre guise. La première chose à faire, une fois dans le contenaire, est de lancer hadoop et yarn. Un script est fourni pour cela, appelé <code>start-hadoop.sh</code>. Lancer ce script.</p> <div class=highlight><pre><span></span><code>./start-hadoop.sh
</code></pre></div> <p>Le résultat devra ressembler à ce qui suit: <img alt="Start Hadoop" src=../img/tp1/start-hadoop.png></p> <h4 id=premiers-pas-avec-hadoop>Premiers pas avec Hadoop<a class=headerlink href=#premiers-pas-avec-hadoop title="Permanent link">&para;</a></h4> <p>Toutes les commandes interagissant avec le système HDFS commencent par <code>hdfs dfs</code>. Ensuite, les options rajoutées sont très largement inspirées des commandes Unix standard.</p> <ul> <li>Créer un répertoire dans HDFS, appelé <em>input</em>. Pour cela, taper: <div class=highlight><pre><span></span><code>hdfs dfs –mkdir -p input
</code></pre></div></li> </ul> <details class=bug> <summary>En cas d'erreur: <em>No such file or directory</em></summary> <p>Si pour une raison ou une autre, vous n'arrivez pas à créer le répertoire <em>input</em>, avec un message ressemblant à ceci: <code>ls: `.': No such file or directory</code>, veiller à construire l'arborescence de l'utilisateur principal (root), comme suit:</p> <p><code>hdfs dfs -mkdir -p /user/root</code></p> </details> <ul> <li>Nous allons utiliser le fichier <a href=https://github.com/CodeMangler/udacity-hadoop-course/blob/ec6bbb839bdc6e701f802c523497fef4e1c206d0/Datasets/purchases.txt.gz>purchases.txt</a> comme entrée pour le traitement MapReduce. </li> <li>Commencer par décompresser le fichier sur votre machine, puis par le charger dans le contenaire <code>hadoop-master</code> avec la commande suivante: <div class=highlight><pre><span></span><code>docker cp purchases.txt hadoop-master:/root/purchases.txt
</code></pre></div></li> <li>À partir du contenaire master, charger le fichier purchases dans le répertoire input (de HDFS) que vous avez créé: <div class=highlight><pre><span></span><code>hdfs dfs –put purchases.txt input
</code></pre></div></li> <li>Pour afficher le contenu du répertoire <em>input</em>, la commande est: <div class=highlight><pre><span></span><code>hdfs dfs –ls input
</code></pre></div></li> <li>Pour afficher les dernières lignes du fichier purchases: <div class=highlight><pre><span></span><code>hdfs dfs -tail input/purchases.txt
</code></pre></div></li> </ul> <p>Le résultat suivant va donc s'afficher: <center><img src=../img/tp1/purchases-tail.png></center></p> <p>Nous présentons dans le tableau suivant les commandes les plus utilisées pour manipuler les fichiers dans HDFS:</p> <table> <thead> <tr> <th>Instruction</th> <th>Fonctionnalité</th> </tr> </thead> <tbody> <tr> <td><code>hdfs dfs –ls</code></td> <td>Afficher le contenu du répertoire racine</td> </tr> <tr> <td><code>hdfs dfs –put file.txt</code></td> <td>Upload un fichier dans hadoop (à partir du répertoire courant de votre disque local)</td> </tr> <tr> <td><code>hdfs dfs –get file.txt</code></td> <td>Download un fichier à partir de hadoop sur votre disque local</td> </tr> <tr> <td><code>hdfs dfs –tail file.txt</code></td> <td>Lire les dernières lignes du fichier</td> </tr> <tr> <td><code>hdfs dfs –cat file.txt</code></td> <td>Affiche tout le contenu du fichier</td> </tr> <tr> <td><code>hdfs dfs –mv file.txt newfile.txt</code></td> <td>Renommer (ou déplacer) le fichier</td> </tr> <tr> <td><code>hdfs dfs –rm newfile.txt</code></td> <td>Supprimer le fichier</td> </tr> <tr> <td><code>hdfs dfs –mkdir myinput</code></td> <td>Créer un répertoire</td> </tr> </tbody> </table> <h4 id=interfaces-web-pour-hadoop>Interfaces web pour Hadoop<a class=headerlink href=#interfaces-web-pour-hadoop title="Permanent link">&para;</a></h4> <p>Hadoop offre plusieurs interfaces web pour pouvoir observer le comportement de ses différentes composantes. Il est possible d'afficher ces pages directement sur notre machine hôte, et ce grâce à l'utilisation de l'option -p de la commande <code>docker run</code>. En effet, cette option permet de publier un port du contenaire sur la machine hôte. Pour pouvoir publier tous les ports exposés, vous pouvez lancer votre contenaire en utilisant l'option <code>-P</code>.</p> <p>En regardant la commande <code>docker run</code> utilisée plus haut, vous verrez que deux ports de la machine maître ont été exposés:</p> <ul> <li>Le port <strong>9870</strong>: qui permet d'afficher les informations de votre namenode.</li> <li>Le port <strong>8088</strong>: qui permet d'afficher les informations du resource manager de Yarn et visualiser le comportement des différents jobs.</li> </ul> <p>Une fois votre cluster lancé et hadoop démarré et prêt à l'emploi, vous pouvez, sur votre navigateur préféré de votre machine hôte, aller à : <a href=http://localhost:9870>http://localhost:9870</a>. Vous obtiendrez le résultat suivant:</p> <p><img alt="Namenode Info" src=../img/tp1/namenode-info.png></p> <p>Vous pouvez également visualiser l'avancement et les résultats de vos Jobs (Map Reduce ou autre) en allant à l'adresse: <a href=http://localhost:8088>http://localhost:8088</a>.</p> <p><img alt="Resource Manager Info" src=../img/tp1/resourceman-info.png></p> <h3 id=map-reduce>Map Reduce<a class=headerlink href=#map-reduce title="Permanent link">&para;</a></h3> <h4 id=presentation_1>Présentation<a class=headerlink href=#presentation_1 title="Permanent link">&para;</a></h4> <p>Un Job Map-Reduce se compose principalement de deux types de programmes:</p> <ul> <li><strong>Mappers</strong> : permettent d’extraire les données nécessaires sous forme de clef/valeur, pour pouvoir ensuite les trier selon la clef</li> <li><strong>Reducers</strong> : prennent un ensemble de données triées selon leur clef, et effectuent le traitement nécessaire sur ces données (somme, moyenne, total...)</li> </ul> <h4 id=wordcount>Wordcount<a class=headerlink href=#wordcount title="Permanent link">&para;</a></h4> <p>Nous allons tester un programme MapReduce grâce à un exemple très simple, le <em>WordCount</em>, l'équivalent du <em>HelloWorld</em> pour les applications de traitement de données. Le Wordcount permet de calculer le nombre de mots dans un fichier donné, en décomposant le calcul en deux étapes:</p> <ul> <li>L'étape de <em>Mapping</em>, qui permet de découper le texte en mots et de délivrer en sortie un flux textuel, où chaque ligne contient le mot trouvé, suivi de la valeur 1 (pour dire que le mot a été trouvé une fois)</li> <li>L'étape de <em>Reducing</em>, qui permet de faire la somme des 1 pour chaque mot, pour trouver le nombre total d'occurrences de ce mot dans le texte.</li> </ul> <p>Commençons par créer un projet Maven dans VSCode. <strong>Nous utiliserons dans notre cas JDK 1.8</strong>.</p> <details class=info> <summary>Version de JDK</summary> <p>Ceci n'est pas une suggestion: l'utilisation d'une autre version que 1.8 provoquera des erreurs sans fin. Hadoop est compilé avec cette version de Java, connue pour sa stabilité. </p> </details> <p>Pour créer un projet Maven dans VSCode: </p> <ul> <li>Prenez soin d'avoir les extensions <em>Maven for Java</em> et <em>Extension Pack for Java</em> activées.</li> <li>Créer un nouveau répertoire dans lequel vous inclurez votre code.</li> <li>Faites un clic-droit dans la fenêtre <em>Explorer</em> et choisir <em>Create Maven Project</em>. </li> <li>Choisir <em>No Archetype</em></li> <li>Définir les valeurs suivantes pour votre projet:<ul> <li><strong>GroupId</strong>: hadoop.mapreduce</li> <li><strong>ArtifactId</strong>: wordcount</li> <li><strong>Version</strong>: 1</li> </ul> </li> <li>Ouvrir le fichier <em>pom.xml</em> automatiquement créé, et ajouter les dépendances suivantes pour Hadoop, HDFS et Map Reduce:</li> </ul> <div class=highlight><pre><span></span><code>  <span class=nt>&lt;dependencies&gt;</span>
     <span class=cm>&lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common --&gt;</span>
      <span class=nt>&lt;dependency&gt;</span>
          <span class=nt>&lt;groupId&gt;</span>org.apache.hadoop<span class=nt>&lt;/groupId&gt;</span>
          <span class=nt>&lt;artifactId&gt;</span>hadoop-common<span class=nt>&lt;/artifactId&gt;</span>
          <span class=nt>&lt;version&gt;</span>3.3.6<span class=nt>&lt;/version&gt;</span>
      <span class=nt>&lt;/dependency&gt;</span>
      <span class=cm>&lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-mapreduce-client-core --&gt;</span>
      <span class=nt>&lt;dependency&gt;</span>
          <span class=nt>&lt;groupId&gt;</span>org.apache.hadoop<span class=nt>&lt;/groupId&gt;</span>
          <span class=nt>&lt;artifactId&gt;</span>hadoop-mapreduce-client-core<span class=nt>&lt;/artifactId&gt;</span>
          <span class=nt>&lt;version&gt;</span>3.3.6<span class=nt>&lt;/version&gt;</span>
      <span class=nt>&lt;/dependency&gt;</span>
      <span class=cm>&lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-hdfs --&gt;</span>
      <span class=nt>&lt;dependency&gt;</span>
          <span class=nt>&lt;groupId&gt;</span>org.apache.hadoop<span class=nt>&lt;/groupId&gt;</span>
          <span class=nt>&lt;artifactId&gt;</span>hadoop-hdfs<span class=nt>&lt;/artifactId&gt;</span>
          <span class=nt>&lt;version&gt;</span>3.3.6<span class=nt>&lt;/version&gt;</span>
      <span class=nt>&lt;/dependency&gt;</span>
      <span class=cm>&lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-mapreduce-client-common --&gt;</span>
      <span class=nt>&lt;dependency&gt;</span>
          <span class=nt>&lt;groupId&gt;</span>org.apache.hadoop<span class=nt>&lt;/groupId&gt;</span>
          <span class=nt>&lt;artifactId&gt;</span>hadoop-mapreduce-client-common<span class=nt>&lt;/artifactId&gt;</span>
          <span class=nt>&lt;version&gt;</span>3.3.6<span class=nt>&lt;/version&gt;</span>
      <span class=nt>&lt;/dependency&gt;</span>
      <span class=cm>&lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-mapreduce-client-jobclient --&gt;</span>
      <span class=nt>&lt;dependency&gt;</span>
          <span class=nt>&lt;groupId&gt;</span>org.apache.hadoop<span class=nt>&lt;/groupId&gt;</span>
          <span class=nt>&lt;artifactId&gt;</span>hadoop-mapreduce-client-jobclient<span class=nt>&lt;/artifactId&gt;</span>
          <span class=nt>&lt;version&gt;</span>3.3.6<span class=nt>&lt;/version&gt;</span>
      <span class=nt>&lt;/dependency&gt;</span>

  <span class=nt>&lt;/dependencies&gt;</span>
</code></pre></div> <ul> <li>Créer un package <em>tp1</em> sous le répertoire <em>src/main/java/hadoop/mapreduce</em></li> <li>Créer la classe <em>TokenizerMapper</em>, contenant ce code:</li> </ul> <div class=highlight><pre><span></span><code>  <span class=kn>package</span> <span class=nn>hadoop.mapreduce.tp1</span><span class=p>;</span>

  <span class=kn>import</span> <span class=nn>org.apache.hadoop.io.IntWritable</span><span class=p>;</span>
  <span class=kn>import</span> <span class=nn>org.apache.hadoop.io.Text</span><span class=p>;</span>
  <span class=kn>import</span> <span class=nn>org.apache.hadoop.mapreduce.Mapper</span><span class=p>;</span>

  <span class=kn>import</span> <span class=nn>java.io.IOException</span><span class=p>;</span>
  <span class=kn>import</span> <span class=nn>java.util.StringTokenizer</span><span class=p>;</span>

  <span class=kd>public</span> <span class=kd>class</span> <span class=nc>TokenizerMapper</span>
        <span class=kd>extends</span> <span class=n>Mapper</span><span class=o>&lt;</span><span class=n>Object</span><span class=p>,</span> <span class=n>Text</span><span class=p>,</span> <span class=n>Text</span><span class=p>,</span> <span class=n>IntWritable</span><span class=o>&gt;</span><span class=p>{</span>

    <span class=kd>private</span> <span class=kd>final</span> <span class=kd>static</span> <span class=n>IntWritable</span> <span class=n>one</span> <span class=o>=</span> <span class=k>new</span> <span class=n>IntWritable</span><span class=p>(</span><span class=mi>1</span><span class=p>);</span>
    <span class=kd>private</span> <span class=n>Text</span> <span class=n>word</span> <span class=o>=</span> <span class=k>new</span> <span class=n>Text</span><span class=p>();</span>

    <span class=kd>public</span> <span class=kt>void</span> <span class=nf>map</span><span class=p>(</span><span class=n>Object</span> <span class=n>key</span><span class=p>,</span> <span class=n>Text</span> <span class=n>value</span><span class=p>,</span> <span class=n>Mapper</span><span class=p>.</span><span class=na>Context</span> <span class=n>context</span>
    <span class=p>)</span> <span class=kd>throws</span> <span class=n>IOException</span><span class=p>,</span> <span class=n>InterruptedException</span> <span class=p>{</span>
        <span class=n>StringTokenizer</span> <span class=n>itr</span> <span class=o>=</span> <span class=k>new</span> <span class=n>StringTokenizer</span><span class=p>(</span><span class=n>value</span><span class=p>.</span><span class=na>toString</span><span class=p>());</span>
        <span class=k>while</span> <span class=p>(</span><span class=n>itr</span><span class=p>.</span><span class=na>hasMoreTokens</span><span class=p>())</span> <span class=p>{</span>
            <span class=n>word</span><span class=p>.</span><span class=na>set</span><span class=p>(</span><span class=n>itr</span><span class=p>.</span><span class=na>nextToken</span><span class=p>());</span>
            <span class=n>context</span><span class=p>.</span><span class=na>write</span><span class=p>(</span><span class=n>word</span><span class=p>,</span> <span class=n>one</span><span class=p>);</span>
        <span class=p>}</span>
    <span class=p>}</span>
  <span class=p>}</span>
</code></pre></div> <ul> <li>Créer la classe <em>IntSumReducer</em>:</li> </ul> <div class=highlight><pre><span></span><code><span class=kn>package</span> <span class=nn>hadoop.mapreduce.tp1</span><span class=p>;</span>

<span class=kn>import</span> <span class=nn>org.apache.hadoop.io.IntWritable</span><span class=p>;</span>
<span class=kn>import</span> <span class=nn>org.apache.hadoop.io.Text</span><span class=p>;</span>
<span class=kn>import</span> <span class=nn>org.apache.hadoop.mapreduce.Reducer</span><span class=p>;</span>

<span class=kn>import</span> <span class=nn>java.io.IOException</span><span class=p>;</span>

<span class=kd>public</span> <span class=kd>class</span> <span class=nc>IntSumReducer</span>
        <span class=kd>extends</span> <span class=n>Reducer</span><span class=o>&lt;</span><span class=n>Text</span><span class=p>,</span><span class=n>IntWritable</span><span class=p>,</span><span class=n>Text</span><span class=p>,</span><span class=n>IntWritable</span><span class=o>&gt;</span> <span class=p>{</span>

    <span class=kd>private</span> <span class=n>IntWritable</span> <span class=n>result</span> <span class=o>=</span> <span class=k>new</span> <span class=n>IntWritable</span><span class=p>();</span>

    <span class=kd>public</span> <span class=kt>void</span> <span class=nf>reduce</span><span class=p>(</span><span class=n>Text</span> <span class=n>key</span><span class=p>,</span> <span class=n>Iterable</span><span class=o>&lt;</span><span class=n>IntWritable</span><span class=o>&gt;</span> <span class=n>values</span><span class=p>,</span>
                       <span class=n>Context</span> <span class=n>context</span>
    <span class=p>)</span> <span class=kd>throws</span> <span class=n>IOException</span><span class=p>,</span> <span class=n>InterruptedException</span> <span class=p>{</span>
        <span class=kt>int</span> <span class=n>sum</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
        <span class=k>for</span> <span class=p>(</span><span class=n>IntWritable</span> <span class=n>val</span> <span class=p>:</span> <span class=n>values</span><span class=p>)</span> <span class=p>{</span>
            <span class=n>System</span><span class=p>.</span><span class=na>out</span><span class=p>.</span><span class=na>println</span><span class=p>(</span><span class=s>&quot;value: &quot;</span><span class=o>+</span><span class=n>val</span><span class=p>.</span><span class=na>get</span><span class=p>());</span>
            <span class=n>sum</span> <span class=o>+=</span> <span class=n>val</span><span class=p>.</span><span class=na>get</span><span class=p>();</span>
        <span class=p>}</span>
        <span class=n>System</span><span class=p>.</span><span class=na>out</span><span class=p>.</span><span class=na>println</span><span class=p>(</span><span class=s>&quot;--&gt; Sum = &quot;</span><span class=o>+</span><span class=n>sum</span><span class=p>);</span>
        <span class=n>result</span><span class=p>.</span><span class=na>set</span><span class=p>(</span><span class=n>sum</span><span class=p>);</span>
        <span class=n>context</span><span class=p>.</span><span class=na>write</span><span class=p>(</span><span class=n>key</span><span class=p>,</span> <span class=n>result</span><span class=p>);</span>
    <span class=p>}</span>
<span class=p>}</span>
</code></pre></div> <ul> <li>Enfin, créer la classe <em>WordCount</em>:</li> </ul> <div class=highlight><pre><span></span><code><span class=kn>package</span> <span class=nn>hadoop.mapreduce.tp1</span><span class=p>;</span>

<span class=kn>import</span> <span class=nn>org.apache.hadoop.conf.Configuration</span><span class=p>;</span>
<span class=kn>import</span> <span class=nn>org.apache.hadoop.fs.Path</span><span class=p>;</span>
<span class=kn>import</span> <span class=nn>org.apache.hadoop.io.IntWritable</span><span class=p>;</span>
<span class=kn>import</span> <span class=nn>org.apache.hadoop.io.Text</span><span class=p>;</span>
<span class=kn>import</span> <span class=nn>org.apache.hadoop.mapreduce.Job</span><span class=p>;</span>
<span class=kn>import</span> <span class=nn>org.apache.hadoop.mapreduce.lib.input.FileInputFormat</span><span class=p>;</span>
<span class=kn>import</span> <span class=nn>org.apache.hadoop.mapreduce.lib.output.FileOutputFormat</span><span class=p>;</span>

<span class=kd>public</span> <span class=kd>class</span> <span class=nc>WordCount</span> <span class=p>{</span>
    <span class=kd>public</span> <span class=kd>static</span> <span class=kt>void</span> <span class=nf>main</span><span class=p>(</span><span class=n>String</span><span class=o>[]</span> <span class=n>args</span><span class=p>)</span> <span class=kd>throws</span> <span class=n>Exception</span> <span class=p>{</span>
        <span class=n>Configuration</span> <span class=n>conf</span> <span class=o>=</span> <span class=k>new</span> <span class=n>Configuration</span><span class=p>();</span>
        <span class=n>Job</span> <span class=n>job</span> <span class=o>=</span> <span class=n>Job</span><span class=p>.</span><span class=na>getInstance</span><span class=p>(</span><span class=n>conf</span><span class=p>,</span> <span class=s>&quot;word count&quot;</span><span class=p>);</span>
        <span class=n>job</span><span class=p>.</span><span class=na>setJarByClass</span><span class=p>(</span><span class=n>WordCount</span><span class=p>.</span><span class=na>class</span><span class=p>);</span>
        <span class=n>job</span><span class=p>.</span><span class=na>setMapperClass</span><span class=p>(</span><span class=n>TokenizerMapper</span><span class=p>.</span><span class=na>class</span><span class=p>);</span>
        <span class=n>job</span><span class=p>.</span><span class=na>setCombinerClass</span><span class=p>(</span><span class=n>IntSumReducer</span><span class=p>.</span><span class=na>class</span><span class=p>);</span>
        <span class=n>job</span><span class=p>.</span><span class=na>setReducerClass</span><span class=p>(</span><span class=n>IntSumReducer</span><span class=p>.</span><span class=na>class</span><span class=p>);</span>
        <span class=n>job</span><span class=p>.</span><span class=na>setOutputKeyClass</span><span class=p>(</span><span class=n>Text</span><span class=p>.</span><span class=na>class</span><span class=p>);</span>
        <span class=n>job</span><span class=p>.</span><span class=na>setOutputValueClass</span><span class=p>(</span><span class=n>IntWritable</span><span class=p>.</span><span class=na>class</span><span class=p>);</span>
        <span class=n>FileInputFormat</span><span class=p>.</span><span class=na>addInputPath</span><span class=p>(</span><span class=n>job</span><span class=p>,</span> <span class=k>new</span> <span class=n>Path</span><span class=p>(</span><span class=n>args</span><span class=o>[</span><span class=mi>0</span><span class=o>]</span><span class=p>));</span>
        <span class=n>FileOutputFormat</span><span class=p>.</span><span class=na>setOutputPath</span><span class=p>(</span><span class=n>job</span><span class=p>,</span> <span class=k>new</span> <span class=n>Path</span><span class=p>(</span><span class=n>args</span><span class=o>[</span><span class=mi>1</span><span class=o>]</span><span class=p>));</span>
        <span class=n>System</span><span class=p>.</span><span class=na>exit</span><span class=p>(</span><span class=n>job</span><span class=p>.</span><span class=na>waitForCompletion</span><span class=p>(</span><span class=kc>true</span><span class=p>)</span> <span class=o>?</span> <span class=mi>0</span> <span class=p>:</span> <span class=mi>1</span><span class=p>);</span>
    <span class=p>}</span>
<span class=p>}</span>
</code></pre></div> <h5 id=tester-map-reduce-en-local>Tester Map Reduce en local<a class=headerlink href=#tester-map-reduce-en-local title="Permanent link">&para;</a></h5> <p>Dans votre projet sur VSCode:</p> <ul> <li>Créer un répertoire <em>input</em> sous le répertoire <em>resources</em> de votre projet.</li> <li>Créer un fichier de test: <em>file.txt</em> dans lequel vous insèrerez les deux lignes: <div class=highlight><pre><span></span><code>  Hello Wordcount!
  Hello Hadoop!
</code></pre></div></li> <li> <p>Nous allons maintenant définir des arguments à la méthode Main: le fichier en entrée sur lequel Map reduce va travailler, et le répertoire en sortie dans lequel le résultat sera stocké. Pour cela:</p> <ul> <li>Ouvrir le fichier launch.json de votre projet (Aller à la fenêtre <em>Run and Debug</em> <img alt="Launch and Debug" src=../image.png>, puis cliquer sur <em>create a launch.json file</em>).</li> <li>Ajouter la ligne suivante dans la configuration <strong>WordCount</strong>, dont la classe principale est <em>hadoop.mapreduce.tp1.WordCount</em>: <div class=highlight><pre><span></span><code><span class=nt>&quot;args&quot;</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=s2>&quot;wordcount/src/main/resources/input/file.txt&quot;</span><span class=p>,</span><span class=s2>&quot;wordcount/src/main/resources/output&quot;</span><span class=p>]</span><span class=w></span>
</code></pre></div></li> </ul> <details class=note> <summary>Arguments</summary> <p>Il est à noter que, dans mon cas, le fichier <em>launch.json</em> a été créé sous le répertoire <em>TP1</em>, c'est pour cette raison que le chemin des fichiers commence par "wordcount". Si vous créez la configuration directement sous le répertoire <em>wordcount</em>, il faudra commencer le chemin par <em>src</em>.</p> </details> </li> <li> <p>Sélectionner ensuite, dans la liste des configurations du projet, <em>WordCount</em> comme configuration par défaut:</p> </li> </ul> <p><img alt="Default Launch Configuration" src=../img/tp1/launchConfig.png></p> <ul> <li>Lancer le programme. Un répertoire <em>output</em> sera créé dans le répertoire <em>resources</em>, contenant notamment un fichier <em>part-r-00000</em>, dont le contenu devrait être le suivant:</li> </ul> <div class=highlight><pre><span></span><code>Hadoop!   1
Hello 2
Wordcount!    1
</code></pre></div> <h5 id=lancer-map-reduce-sur-le-cluster>Lancer Map Reduce sur le cluster<a class=headerlink href=#lancer-map-reduce-sur-le-cluster title="Permanent link">&para;</a></h5> <p>Dans votre projet VSCode:</p> <ul> <li>Pour pouvoir encapsuler toutes les dépendances du projet dans le fichier JAR à exporter, ajouter le plugin suivant dans le fichier <em>pom.xml</em> de votre projet:</li> </ul> <div class=highlight><pre><span></span><code><span class=nt>&lt;build&gt;</span>
  <span class=nt>&lt;plugins&gt;</span>
    <span class=nt>&lt;plugin&gt;</span>
      <span class=nt>&lt;groupId&gt;</span>org.apache.maven.plugins<span class=nt>&lt;/groupId&gt;</span>
      <span class=nt>&lt;artifactId&gt;</span>maven-assembly-plugin<span class=nt>&lt;/artifactId&gt;</span>
      <span class=nt>&lt;version&gt;</span>3.6.0<span class=nt>&lt;/version&gt;</span> <span class=cm>&lt;!-- Use latest version --&gt;</span>
      <span class=nt>&lt;configuration&gt;</span>
        <span class=nt>&lt;archive&gt;</span>
          <span class=nt>&lt;manifest&gt;</span>
<span class=hll>            <span class=nt>&lt;mainClass&gt;</span>hadoop.mapreduce.tp1.WordCount<span class=nt>&lt;/mainClass&gt;</span>
</span>          <span class=nt>&lt;/manifest&gt;</span>
        <span class=nt>&lt;/archive&gt;</span>
        <span class=nt>&lt;descriptorRefs&gt;</span>
          <span class=nt>&lt;descriptorRef&gt;</span>jar-with-dependencies<span class=nt>&lt;/descriptorRef&gt;</span>
        <span class=nt>&lt;/descriptorRefs&gt;</span>
      <span class=nt>&lt;/configuration&gt;</span>
      <span class=nt>&lt;executions&gt;</span>
        <span class=nt>&lt;execution&gt;</span>
          <span class=nt>&lt;id&gt;</span>make-assembly<span class=nt>&lt;/id&gt;</span> <span class=cm>&lt;!-- this is used for inheritance merges --&gt;</span>
          <span class=nt>&lt;phase&gt;</span>package<span class=nt>&lt;/phase&gt;</span> <span class=cm>&lt;!-- bind to the packaging phase --&gt;</span>
          <span class=nt>&lt;goals&gt;</span>
            <span class=nt>&lt;goal&gt;</span>single<span class=nt>&lt;/goal&gt;</span>
          <span class=nt>&lt;/goals&gt;</span>
        <span class=nt>&lt;/execution&gt;</span>
      <span class=nt>&lt;/executions&gt;</span>
    <span class=nt>&lt;/plugin&gt;</span>
  <span class=nt>&lt;/plugins&gt;</span>
<span class=nt>&lt;/build&gt;</span>
</code></pre></div> <ul> <li>Aller dans l'Explorer, sous Maven, puis ouvrir le <em>Lifecycle</em> du projet wordcount.</li> <li>Cliquer sur <code>package</code> pour compiler et packager le projet dans un fichier JAR. Un fichier <em>wordcount-1.0-SNAPSHOT-jar-with-dependencies.jar</em> sera créé sous le répertoire <em>target</em> du projet.</li> <li> <p>Copier le fichier jar créé dans le contenaire master. Pour cela:</p> <ul> <li>Ouvrir le terminal sur le répertoire du projet <em>wordcount</em>. Cela peut être fait avec VSCode en allant au menu <em>Terminal -&gt; New Terminal</em>.</li> <li>Taper la commande suivante: <div class=highlight><pre><span></span><code>docker cp target/wordcount-1.0-SNAPSHOT-jar-with-dependencies.jar hadoop-master:/root/wordcount.jar
</code></pre></div></li> </ul> </li> <li> <p>Revenir au shell du contenaire master, et lancer le job map reduce avec cette commande:</p> </li> </ul> <div class=highlight><pre><span></span><code>  hadoop jar wordcount.jar input output
</code></pre></div> <p>Le Job sera lancé sur le fichier <em>purchases.txt</em> que vous aviez préalablement chargé dans le répertoire <em>input</em> de HDFS. Une fois le Job terminé, un répertoire <em>output</em> sera créé. Si tout se passe bien, vous obtiendrez un affichage ressemblant au suivant: <img alt="Résultat Map Reduce" src=../img/tp1/resultat-mapreduce.png></p> <p>En affichant les dernières lignes du fichier généré <em>output/part-r-00000</em>, avec <code>hdfs dfs -tail output/part-r-00000</code>, vous obtiendrez l'affichage suivant:</p> <p><img alt="Affichage Map Reduce" src=../img/tp1/tail.png></p> <p>Il vous est possible de monitorer vos Jobs Map Reduce, en allant à la page: <code>http://localhost:8088</code>. Vous trouverez votre Job dans la liste des applications comme suit:</p> <p><img alt="Job MR" src=../img/tp1/job-mr.png></p> <p>Il est également possible de voir le comportement des noeuds workers, en allant à l'adresse: <code>http://localhost:8041</code> pour <em>worker1</em>, et <code>http://localhost:8042</code> pour <em>worker2</em>. Vous obtiendrez ce qui suit:</p> <p><img alt="Job MR" src=../img/tp1/worker-mr.png></p> <div class="admonition note"> <p class=admonition-title>Application</p> <p>Écrire un Job Map Reduce permettant, à partir du fichier <em>purchases</em> initial, de déterminer le total des ventes par magasin. Il est à noter que la structure du fichier <em>purchases</em> est de la forme suivante: <div class=highlight><pre><span></span><code>  date   temps   magasin   produit   cout   paiement
</code></pre></div> Veiller à toujours tester votre code en local avant de lancer un job sur le cluster!</p> </div> <h3 id=homework>Homework<a class=headerlink href=#homework title="Permanent link">&para;</a></h3> <p>Vous allez, pour ce cours, réaliser un projet en trinôme ou quadrinôme, qui consiste en la construction d'une architecture Big Data supportant le streaming, le batch processing, et le dashboarding temps réel. Pour la séance prochaine, vous allez commencer par mettre les premières pierres à l'édifice:</p> <ul> <li>Choisir la source de données sur laquelle vous allez travailler. Je vous invite à consulter les datasets offerts par <a href=https://www.kaggle.com/ >Kaggle</a> par exemple, ou chercher une source de streaming tel que Twitter.</li> <li>Réfléchir à l'architecture cible. La pipeline devrait intégrer des traitements en batch, des traitements en streaming et une visualisation.</li> </ul> <hr> <div class=md-source-file> <small> Last update: <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-timeago"><span class=timeago datetime=2024-04-24T21:35:58+01:00 locale=en></span></span><span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_date">2024-04-24</span> </small> </div> </article> </div> </div> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=.. class="md-footer__link md-footer__link--prev" aria-label="Previous: Travaux Pratiques Big Data" rel=prev> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </div> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction> Previous </span> Travaux Pratiques Big Data </div> </div> </a> <a href=../tp2/ class="md-footer__link md-footer__link--next" aria-label="Next: TP2 - Traitement par Lot et Streaming avec Spark" rel=next> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction> Next </span> TP2 - Traitement par Lot et Streaming avec Spark </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2017 - 2024 Lilia Sfaxi </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": [], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../assets/javascripts/workers/search.092fa1f6.min.js"}</script> <script src=../assets/javascripts/bundle.e3b2bf44.min.js></script> <script src=../js/timeago.min.js></script> <script src=../js/timeago_mkdocs_material.js></script> </body> </html>