
<!DOCTYPE html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Les Travaux Pratiques du cours Big Data">
      
      
        <link rel="canonical" href="http://INSATunisia.github.io/TP-BigData/tp2/">
      
      
        <meta name="author" content="Lilia Sfaxi">
      
      
        <link rel="shortcut icon" href="../img/favicon.ico">
      
      <meta name="generator" content="mkdocs-0.17.2, mkdocs-material-1.7.1">
    
    
      
        <title>TP2 - Apache Spark - TP Big Data</title>
      
    
    
      <script src="../assets/javascripts/modernizr-1df76c4e58.js"></script>
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application-2421e7e627.css">
      
        <link rel="stylesheet" href="../assets/stylesheets/application-8817cfa535.palette.css">
      
    
    
      
        
        
        
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
      <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
    
    
    

    <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
    <link rel="stylesheet" href="../css/highlight.css">
    <link rel="stylesheet" href="../css/codehilite.css">
    <link rel="stylesheet" href="../css/tooltip.css">

    <link rel="stylesheet" href="../css/customizations.css">
    <link rel="stylesheet" href="../css/site-customizations.css">
  </head>
  
  
  
  
    <body data-md-color-primary="blue" data-md-color-accent="yellow">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="drawer">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="search">
    <label class="md-overlay" data-md-component="overlay" for="drawer"></label>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <a href="http://INSATunisia.github.io/TP-BigData/" title="TP Big Data" class="md-logo md-header-nav__button">
            <img src="../img/logo.png" width="24" height="24">
          </a>
        
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <span class="md-flex__ellipsis md-header-nav__title">
          
            
              
            
            TP2 - Apache Spark
          
        </span>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="search"></label>
          
<div class="md-search" data-md-component="search">
  <label class="md-search__overlay" for="search"></label>
  <div class="md-search__inner">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" required placeholder="Search" accesskey="s" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query">
      <label class="md-icon md-search__icon" for="search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset">close</button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result" data-md-lang-search="">
          <div class="md-search-result__meta" data-md-lang-result-none="No matching documents" data-md-lang-result-one="1 matching document" data-md-lang-result-other="# matching documents">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <div class="md-header-nav__source">
          
            


  


  <a href="https://github.com/INSATunisia/TP-BigData/" title="Go to repository" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      INSATunisia/TP-BigData
    </div>
  </a>

          
        </div>
      </div>
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="drawer">
    
      <i class="md-logo md-nav__button">
        <img src="../img/logo.png">
      </i>
    
    TP Big Data
  </label>
  
    <div class="md-nav__source">
      


  


  <a href="https://github.com/INSATunisia/TP-BigData/" title="Go to repository" class="md-source" data-md-source="github">
    
      <div class="md-source__icon">
        <svg viewBox="0 0 24 24" width="24" height="24">
          <use xlink:href="#github" width="24" height="24"></use>
        </svg>
      </div>
    
    <div class="md-source__repository">
      INSATunisia/TP-BigData
    </div>
  </a>

    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="Travaux Pratiques Big Data" class="md-nav__link">
      Travaux Pratiques Big Data
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../tp1/" title="TP1 - Hadoop et Map Reduce" class="md-nav__link">
      TP1 - Hadoop et Map Reduce
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="toc">
        TP2 - Apache Spark
      </label>
    
    <a href="./" title="TP2 - Apache Spark" class="md-nav__link md-nav__link--active">
      TP2 - Apache Spark
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#telecharger-pdf" title="Télécharger PDF" class="md-nav__link">
    Télécharger PDF
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#objectifs-du-tp" title="Objectifs du TP" class="md-nav__link">
    Objectifs du TP
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#outils-et-versions" title="Outils et Versions" class="md-nav__link">
    Outils et Versions
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#spark" title="Spark" class="md-nav__link">
    Spark
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#presentation" title="Présentation" class="md-nav__link">
    Présentation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#spark-et-hadoop" title="Spark et Hadoop" class="md-nav__link">
    Spark et Hadoop
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#installation" title="Installation" class="md-nav__link">
    Installation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#test-de-spark-avec-spark-shell" title="Test de Spark avec Spark-Shell" class="md-nav__link">
    Test de Spark avec Spark-Shell
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lapi-de-spark" title="L'API de Spark" class="md-nav__link">
    L'API de Spark
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#exemple" title="Exemple" class="md-nav__link">
    Exemple
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#spark-batch-en-java" title="Spark Batch en Java" class="md-nav__link">
    Spark Batch en Java
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#preparation-de-lenvironnement-et-code" title="Préparation de l'environnement et Code" class="md-nav__link">
    Préparation de l'environnement et Code
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test-du-code-en-local" title="Test du code en local" class="md-nav__link">
    Test du code en local
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lancement-du-code-sur-le-cluster" title="Lancement du code sur le cluster" class="md-nav__link">
    Lancement du code sur le cluster
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#spark-streaming" title="Spark Streaming" class="md-nav__link">
    Spark Streaming
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#environnement-et-code" title="Environnement et Code" class="md-nav__link">
    Environnement et Code
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test-du-code-en-local_1" title="Test du code en Local" class="md-nav__link">
    Test du code en Local
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lancement-du-code-sur-le-cluster_1" title="Lancement du code sur le cluster" class="md-nav__link">
    Lancement du code sur le cluster
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#homework" title="Homework" class="md-nav__link">
    Homework
  </a>
  
</li>
      
      
      
    </ul>
  
</nav>
    
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../tp3/" title="TP3 - Apacke Kafka" class="md-nav__link">
      TP3 - Apacke Kafka
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../tp4/" title="TP4 - HBase" class="md-nav__link">
      TP4 - HBase
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#telecharger-pdf" title="Télécharger PDF" class="md-nav__link">
    Télécharger PDF
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#objectifs-du-tp" title="Objectifs du TP" class="md-nav__link">
    Objectifs du TP
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#outils-et-versions" title="Outils et Versions" class="md-nav__link">
    Outils et Versions
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#spark" title="Spark" class="md-nav__link">
    Spark
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#presentation" title="Présentation" class="md-nav__link">
    Présentation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#spark-et-hadoop" title="Spark et Hadoop" class="md-nav__link">
    Spark et Hadoop
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#installation" title="Installation" class="md-nav__link">
    Installation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#test-de-spark-avec-spark-shell" title="Test de Spark avec Spark-Shell" class="md-nav__link">
    Test de Spark avec Spark-Shell
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lapi-de-spark" title="L'API de Spark" class="md-nav__link">
    L'API de Spark
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#exemple" title="Exemple" class="md-nav__link">
    Exemple
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#spark-batch-en-java" title="Spark Batch en Java" class="md-nav__link">
    Spark Batch en Java
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#preparation-de-lenvironnement-et-code" title="Préparation de l'environnement et Code" class="md-nav__link">
    Préparation de l'environnement et Code
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test-du-code-en-local" title="Test du code en local" class="md-nav__link">
    Test du code en local
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lancement-du-code-sur-le-cluster" title="Lancement du code sur le cluster" class="md-nav__link">
    Lancement du code sur le cluster
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#spark-streaming" title="Spark Streaming" class="md-nav__link">
    Spark Streaming
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#environnement-et-code" title="Environnement et Code" class="md-nav__link">
    Environnement et Code
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test-du-code-en-local_1" title="Test du code en Local" class="md-nav__link">
    Test du code en Local
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lancement-du-code-sur-le-cluster_1" title="Lancement du code sur le cluster" class="md-nav__link">
    Lancement du code sur le cluster
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#homework" title="Homework" class="md-nav__link">
    Homework
  </a>
  
</li>
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/INSATunisia/TP-BigData/edit/master/docs/tp2.md" title="Edit this page" class="md-icon md-content__icon">edit</a>
                
                
                <h1 id="tp2-traitement-par-lot-et-streaming-avec-spark">TP2 - Traitement par Lot et Streaming avec Spark<a class="headerlink" href="#tp2-traitement-par-lot-et-streaming-avec-spark" title="Permanent link">&para;</a></h1>
<p><center><img alt="Stream Processing" src="../img/stream.png" /></center></p>
<h2 id="telecharger-pdf">Télécharger PDF<a class="headerlink" href="#telecharger-pdf" title="Permanent link">&para;</a></h2>
<p><a href="../tp2.pdf"><img alt="Download TP2" src="../img/pdf.png" /></a></p>
<h2 id="objectifs-du-tp">Objectifs du TP<a class="headerlink" href="#objectifs-du-tp" title="Permanent link">&para;</a></h2>
<p>Utilisation de Spark pour réaliser des traitements par lot et des traitements en streaming.</p>
<h2 id="outils-et-versions">Outils et Versions<a class="headerlink" href="#outils-et-versions" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="http://hadoop.apache.org/">Apache Hadoop</a> Version: 2.7.2</li>
<li><a href="https://spark.apache.org/">Apache Spark</a> Version: 2.2.1</li>
<li><a href="https://www.docker.com/">Docker</a> Version 17.09.1</li>
<li><a href="https://www.jetbrains.com/idea/download/">IntelliJ IDEA</a> Version Ultimate 2016.1 (ou tout autre IDE de votre choix)</li>
<li><a href="http://www.oracle.com/technetwork/java/javase/downloads/index.html">Java</a> Version 1.8</li>
<li>Unix-like ou Unix-based Systems (Divers Linux et MacOS)</li>
</ul>
<h2 id="spark">Spark<a class="headerlink" href="#spark" title="Permanent link">&para;</a></h2>
<h3 id="presentation">Présentation<a class="headerlink" href="#presentation" title="Permanent link">&para;</a></h3>
<p><a href="https://spark.apache.org/">Spark</a> est un système de traitement rapide et parallèle. Il fournit des APIs de haut niveau en Java, Scala, Python et R, et un moteur optimisé qui supporte l'exécution des graphes. Il supporte également un ensemble d'outils de haut niveau tels que <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> pour le support du traitement de données structurées, <a href="https://spark.apache.org/docs/latest/ml-guide.html">MLlib</a> pour l'apprentissage des données, <a href="https://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> pour le traitement des graphes, et <a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a> pour le traitment des données en streaming.</p>
<p><center><img src="../img/tp2/spark.png" width="200"></center><br />
<center><img src="../img/tp2/spark-layers.png" width="500"></center></p>
<h3 id="spark-et-hadoop">Spark et Hadoop<a class="headerlink" href="#spark-et-hadoop" title="Permanent link">&para;</a></h3>
<p>Spark peut s'exécuter sur plusieurs plateformes: Hadoop, Mesos, en standalone ou sur le cloud. Il peut également accéder diverses sources de données, comme HDFS, Cassandra, HBase et S3.</p>
<p>Dans ce TP, nous allons exécuter Spark sur Hadoop YARN. YARN s'occupera ainsi de la gestion des ressources pour le déclenchement et l'exécution des Jobs Spark.</p>
<h3 id="installation">Installation<a class="headerlink" href="#installation" title="Permanent link">&para;</a></h3>
<p>Nous avons procédé à l'installation de Spark sur le cluster Hadoop utilisé dans le <a href="../tp1/index.html">TP1</a>. Voici les étapes nécessaires pour le lancer:</p>
<ol>
<li>Cloner le repo github contenant les fichiers nécessaires pour le lancement des contenaires et leur configuration:<br />
<div class="codehilite"><pre><span></span>  git clone https://github.com/liliasfaxi/hadoop-cluster-docker
</pre></div></li>
<li>Construire l'image Docker à partir du fichier Dockerfile fourni.<br />
<div class="codehilite"><pre><span></span>  <span class="nb">cd</span> hadoop-cluster-docker
  ./build-image.sh
</pre></div></li>
<li>Démarrer les trois contenaires:<br />
<div class="codehilite"><pre><span></span>  sudo ./start-container.sh
</pre></div><br />
Le résultat de cette exécution sera le suivant:<br />
<div class="codehilite"><pre><span></span>  start hadoop-master container...
  start hadoop-slave1 container...
  start hadoop-slave2 container...
  root@hadoop-master:~#
</pre></div></li>
<li>Lancer les démons yarn et hdfs en lançant:<br />
<div class="codehilite"><pre><span></span>  ./start-hadoop.sh
</pre></div></li>
</ol>
<p>Vous pourrez vérifier que tous les démons sont lancés en tapant: <code>jps</code>. Un résultat semblable au suivant pourra être visible:<br />
<div class="codehilite"><pre><span></span>  <span class="m">880</span> Jps
  <span class="m">257</span> NameNode
  <span class="m">613</span> ResourceManager
  <span class="m">456</span> SecondaryNameNode
</pre></div></p>
<p>La même opération sur les noeuds esclaves (auquels vous accédez à partir de votre machine hôte en tapant <code>docker exec -it hadoop-slave1 bash</code>) devrait donner:<br />
<div class="codehilite"><pre><span></span>  <span class="m">176</span> NodeManager
  <span class="m">65</span> DataNode
  <span class="m">311</span> Jps
</pre></div></p>
<h2 id="test-de-spark-avec-spark-shell">Test de Spark avec Spark-Shell<a class="headerlink" href="#test-de-spark-avec-spark-shell" title="Permanent link">&para;</a></h2>
<p>Dans le but de tester l'exécution de spark, commencer par créer un fichier <em>file1.txt</em> dans votre noeud master, contenant le texte suivant:<br />
<div class="codehilite"><pre><span></span>  Hello Spark Wordcount!
  Hello Hadoop Also :)
</pre></div></p>
<p>Charger ensuite ce fichier dans HDFS:<br />
<div class="codehilite"><pre><span></span>  hadoop fs -put file1.txt
</pre></div></p>
<p>Pour vérifier que spark est bien installé, taper la commande suivante:<br />
<div class="codehilite"><pre><span></span>  spark-shell
</pre></div></p>
<p>Vous devriez avoir un résultat semblable au suivant:<br />
<img alt="Spark Shell" src="../img/tp2/spark-shell.png" /></p>
<p>Vous pourrez tester spark avec un code scala simple comme suit (à exécuter ligne par ligne):</p>
<div class="codehilite"><pre><span></span>  <span class="k">val</span> <span class="n">lines</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="o">(</span><span class="s">&quot;file1.txt&quot;</span><span class="o">)</span>
  <span class="k">val</span> <span class="n">words</span> <span class="k">=</span> <span class="n">lines</span><span class="o">.</span><span class="n">flatMap</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&quot;\\s+&quot;</span><span class="o">))</span>
  <span class="k">val</span> <span class="n">wc</span> <span class="k">=</span> <span class="n">words</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">w</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">w</span><span class="o">,</span> <span class="mi">1</span><span class="o">)).</span><span class="n">reduceByKey</span><span class="o">(</span><span class="k">_</span> <span class="o">+</span> <span class="k">_</span><span class="o">)</span>
  <span class="n">wc</span><span class="o">.</span><span class="n">saveAsTextFile</span><span class="o">(</span><span class="s">&quot;file1.count&quot;</span><span class="o">)</span>
</pre></div>

<p>Ce code vient de (1) charger le fichier <em>file1.txt</em> de HDFS, (2) séparer les mots selon les caractères d'espacement, (3) appliquer un <em>map</em> sur les mots obtenus qui produit le couple (<em>&lt;mot></em>, 1), puis un <em>reduce</em> qui permet de faire la somme des 1 des mots identiques.</p>
<p>Pour afficher le résultat, sortir de spark-shell en cliquant sur <em>Ctrl-C</em>. Télécharger ensuite le répertoire <em>file1.count</em> créé dans HDFS comme suit:<br />
<div class="codehilite"><pre><span></span>  hadoop fs -get file1.count
</pre></div><br />
Le contenu des deux fichiers <em>part-00000</em> et <em>part-00001</em> ressemble à ce qui suit:</p>
<p><center><img src="../img/tp2/spark-shell-result.png" width=400px></center></p>
<h2 id="lapi-de-spark">L'API de Spark<a class="headerlink" href="#lapi-de-spark" title="Permanent link">&para;</a></h2>
<p>A un haut niveau d'abstraction, chaque application Spark consiste en un programme <em>driver</em> qui exécute la fonction <em>main</em> de l'utilisateur et lance plusieurs opérations parallèles sur le cluster. L'abstraction principale fournie par Spark est un RDD (<em>Resilient Distributed Dataset</em>), qui représente une collection d'éléments partitionnés à travers les noeuds du cluster, et sur lesquelles on peut opérer en parallèle. Les RDDs sont créés à partir d'un fichier dans HDFS par exemple, puis le transforment. Les utilisateurs peuvent demander à Spark de sauvegarder un RDD en mémoire, lui permettant ainsi d'être réutilisé efficacement à travers plusieurs opérations parallèles.</p>
<p><center><img src="../img/tp2/RDD.png" width="100"></center></p>
<p>Les RDDs supportent deux types d'opérations:</p>
<ul>
<li>les <em>transformations</em>, qui permettent de créer un nouveau Dataset à partir d'un Dataset existant</li>
<li>les <em>actions</em>, qui retournent une valeur au programme <em>driver</em> arès avoir exécuté un calcul sur le Dataset.</li>
</ul>
<p>Par exemple, un <em>map</em> est une transformation qui passe chaque élément du dataset via une fontion, et retourne un nouvel RDD représentant les résultats. Un <em>reduce</em> est une action qui agrège tous les éléments du RDD en utilisant une certaine fonction et retourne le résultat final au programme.</p>
<p>Toutes les transformations dans Spark sont <em>lazy</em>, car elles ne calculent pas le résultat immédiatement. Elles se souviennent des transformations appliquées à un dataset de base (par ex. un fichier). Les transformations ne sont calculées que quand une action nécessite qu'un résultat soit retourné au programme principal. Cela permet à Spark de s'exécuter plus efficacement.</p>
<p><center><img src="../img/tp2/RDD-trans-action.png" width="500"></center></p>
<h3 id="exemple">Exemple<a class="headerlink" href="#exemple" title="Permanent link">&para;</a></h3>
<p>L'exemple que nous allons présenter ici par étapes permet de relever les mots les plus fréquents dans un fichier. Pour cela, le code suivant est utilisé:</p>
<p><div class="codehilite"><pre><span></span>  <span class="c1">//Etape 1 - Créer un RDD à partir d&#39;un fichier texte de Hadoop</span>
  <span class="k">val</span> <span class="n">docs</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">textFile</span><span class="o">(</span><span class="s">&quot;/docs&quot;</span><span class="o">)</span>
</pre></div><br />
<center><img src="../img/tp2/ex1.png" width="500"></center></p>
<p><div class="codehilite"><pre><span></span>  <span class="c1">//Etape 2 - Convertir les lignes en minuscule</span>
  <span class="k">val</span> <span class="n">lower</span> <span class="k">=</span> <span class="n">docs</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">line</span> <span class="k">=&gt;</span> <span class="n">line</span><span class="o">.</span><span class="n">toLowerCase</span><span class="o">)</span>
</pre></div><br />
<center><img src="../img/tp2/ex2.png" width="500"></center></p>
<p><div class="codehilite"><pre><span></span>  <span class="c1">//Etape 3 - Séparer les lignes en mots</span>
  <span class="k">val</span> <span class="n">words</span> <span class="k">=</span> <span class="n">lower</span><span class="o">.</span><span class="n">flatMap</span><span class="o">(</span><span class="n">line</span> <span class="k">=&gt;</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&quot;\\s+&quot;</span><span class="o">))</span>
</pre></div><br />
<center><img src="../img/tp2/ex3.png" width="500"></center></p>
<p><div class="codehilite"><pre><span></span>  <span class="c1">//Etape 4 - produire les tuples (mot, 1)</span>
  <span class="k">val</span> <span class="n">counts</span> <span class="k">=</span> <span class="n">words</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">word</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">word</span><span class="o">,</span><span class="mi">1</span><span class="o">))</span>
</pre></div><br />
<center><img src="../img/tp2/ex4.png" width="500"></center></p>
<p><div class="codehilite"><pre><span></span>  <span class="c1">//Etape 5 - Compter tous les mots</span>
  <span class="k">val</span> <span class="n">freq</span> <span class="k">=</span> <span class="n">counts</span><span class="o">.</span><span class="n">reduceByKey</span><span class="o">(</span><span class="k">_</span> <span class="o">+</span> <span class="k">_</span><span class="o">)</span>
</pre></div><br />
<center><img src="../img/tp2/ex5.png" width="500"></center></p>
<p><div class="codehilite"><pre><span></span>  <span class="c1">//Etape 6 - Inverser les tuples (transformation avec swap)</span>
  <span class="n">freq</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">swap</span><span class="o">)</span>
</pre></div><br />
<center><img src="../img/tp2/ex6.png" width="400"></center></p>
<p><div class="codehilite"><pre><span></span>  <span class="c1">//Etape 6 - Inverser les tuples (action de sélection des n premiers)</span>
  <span class="k">val</span> <span class="n">top</span> <span class="k">=</span> <span class="n">freq</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="nc">_swap</span><span class="o">).</span><span class="n">top</span><span class="o">(</span><span class="n">N</span><span class="o">)</span>
</pre></div><br />
<center><img src="../img/tp2/ex7.png" width="500"></center></p>
<h2 id="spark-batch-en-java">Spark Batch en Java<a class="headerlink" href="#spark-batch-en-java" title="Permanent link">&para;</a></h2>
<h3 id="preparation-de-lenvironnement-et-code">Préparation de l'environnement et Code<a class="headerlink" href="#preparation-de-lenvironnement-et-code" title="Permanent link">&para;</a></h3>
<p>Nous allons dans cette partie créer un projet Spark Batch en Java (un simple WordCount), le charger sur le cluster et lancer le job.</p>
<ol>
<li>Créer un projet Maven avec IntelliJ IDEA, en utilisant la config suivante:<br />
  <div class="codehilite"><pre><span></span>  <span class="nt">&lt;groupId&gt;</span>spark.batch<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>wordcount<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1<span class="nt">&lt;/version&gt;</span>
</pre></div></li>
<li>Rajouter dans le fichier pom les dépendances nécessaires, et indiquer la version du compilateur Java:<br />
  <div class="codehilite"><pre><span></span>  <span class="nt">&lt;properties&gt;</span>
      <span class="nt">&lt;maven.compiler.source&gt;</span>1.8<span class="nt">&lt;/maven.compiler.source&gt;</span>
      <span class="nt">&lt;maven.compiler.target&gt;</span>1.8<span class="nt">&lt;/maven.compiler.target&gt;</span>
  <span class="nt">&lt;/properties&gt;</span>
  <span class="nt">&lt;dependencies&gt;</span>
      <span class="nt">&lt;dependency&gt;</span>
          <span class="nt">&lt;groupId&gt;</span>org.apache.spark<span class="nt">&lt;/groupId&gt;</span>
          <span class="nt">&lt;artifactId&gt;</span>spark-core_2.11<span class="nt">&lt;/artifactId&gt;</span>
          <span class="nt">&lt;version&gt;</span>2.1.0<span class="nt">&lt;/version&gt;</span>
      <span class="nt">&lt;/dependency&gt;</span>
      <span class="nt">&lt;dependency&gt;</span>
          <span class="nt">&lt;groupId&gt;</span>org.slf4j<span class="nt">&lt;/groupId&gt;</span>
          <span class="nt">&lt;artifactId&gt;</span>slf4j-log4j12<span class="nt">&lt;/artifactId&gt;</span>
          <span class="nt">&lt;version&gt;</span>1.7.22<span class="nt">&lt;/version&gt;</span>
      <span class="nt">&lt;/dependency&gt;</span>
  <span class="nt">&lt;/dependencies&gt;</span>
</pre></div></li>
<li>Sous le répertoire java, créer un package que vous appellerez <em>tn.insat.tp21</em>, et dedans, une classe appelée <em>WordCountTask</em>.</li>
<li>
<p>Écrire le code suivant dans <em>WordCountTask</em>:<br />
  <div class="codehilite"><pre><span></span><span class="kd">public</span> <span class="kd">class</span> <span class="nc">WordCountTask</span> <span class="o">{</span>
      <span class="kd">private</span> <span class="kd">static</span> <span class="kd">final</span> <span class="n">Logger</span> <span class="n">LOGGER</span> <span class="o">=</span> <span class="n">LoggerFactory</span><span class="o">.</span><span class="na">getLogger</span><span class="o">(</span><span class="n">WordCountTask</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>

      <span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="n">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> <span class="o">{</span>
          <span class="n">checkArgument</span><span class="o">(</span><span class="n">args</span><span class="o">.</span><span class="na">length</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="o">,</span> <span class="s">&quot;Please provide the path of input file and output dir as parameters.&quot;</span><span class="o">);</span>
          <span class="k">new</span> <span class="n">WordCountTask</span><span class="o">().</span><span class="na">run</span><span class="o">(</span><span class="n">args</span><span class="o">[</span><span class="mi">0</span><span class="o">],</span> <span class="n">args</span><span class="o">[</span><span class="mi">1</span><span class="o">]);</span>
      <span class="o">}</span>

      <span class="kd">public</span> <span class="kt">void</span> <span class="nf">run</span><span class="o">(</span><span class="n">String</span> <span class="n">inputFilePath</span><span class="o">,</span> <span class="n">String</span> <span class="n">outputDir</span><span class="o">)</span> <span class="o">{</span>
          <span class="n">String</span> <span class="n">master</span> <span class="o">=</span> <span class="s">&quot;local[*]&quot;</span><span class="o">;</span>
          <span class="n">SparkConf</span> <span class="n">conf</span> <span class="o">=</span> <span class="k">new</span> <span class="n">SparkConf</span><span class="o">()</span>
                  <span class="o">.</span><span class="na">setAppName</span><span class="o">(</span><span class="n">WordCountTask</span><span class="o">.</span><span class="na">class</span><span class="o">.</span><span class="na">getName</span><span class="o">())</span>
                  <span class="o">.</span><span class="na">setMaster</span><span class="o">(</span><span class="n">master</span><span class="o">);</span>
          <span class="n">JavaSparkContext</span> <span class="n">sc</span> <span class="o">=</span> <span class="k">new</span> <span class="n">JavaSparkContext</span><span class="o">(</span><span class="n">conf</span><span class="o">);</span>

          <span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">textFile</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="na">textFile</span><span class="o">(</span><span class="n">inputFilePath</span><span class="o">);</span>
          <span class="n">JavaPairRDD</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">textFile</span>
                  <span class="o">.</span><span class="na">flatMap</span><span class="o">(</span><span class="n">s</span> <span class="o">-&gt;</span> <span class="n">Arrays</span><span class="o">.</span><span class="na">asList</span><span class="o">(</span><span class="n">s</span><span class="o">.</span><span class="na">split</span><span class="o">(</span><span class="s">&quot; &quot;</span><span class="o">)).</span><span class="na">iterator</span><span class="o">())</span>
                  <span class="o">.</span><span class="na">mapToPair</span><span class="o">(</span><span class="n">word</span> <span class="o">-&gt;</span> <span class="k">new</span> <span class="n">Tuple2</span><span class="o">&lt;&gt;(</span><span class="n">word</span><span class="o">,</span> <span class="mi">1</span><span class="o">))</span>
                  <span class="o">.</span><span class="na">reduceByKey</span><span class="o">((</span><span class="n">a</span><span class="o">,</span> <span class="n">b</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="o">);</span>
          <span class="n">counts</span><span class="o">.</span><span class="na">saveAsTextFile</span><span class="o">(</span><span class="n">outputDir</span><span class="o">);</span>
      <span class="o">}</span>
  <span class="o">}</span>
</pre></div><br />
  La première chose à faire dans un programme Spark est de créer un objet <em>JavaSparkContext</em>, qui indique à Spark comment accéder à un cluster. Pour créer ce contexte, vous aurez besoin de construire un objet <em>SparkConf</em> qui contient toutes les informations sur l'application.</p>
<ul>
<li><em>appName</em>: est de nom de l'application</li>
<li><em>master</em> est une URL d'un cluster Spark, Mesos ou YARN, ou bien une chaîne spéciale <em>local</em> pour lancer le job en mode local.</li>
</ul>
</li>
</ol>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Nous avons indiqué ici que notre master est <em>local</em> pour les besoins du test, mais plus tard, en le packageant pour le cluster, nous allons enlever cette indication. Il est en effet déconseillé de la hard-coder dans le programme, il faudrait plutôt l'indiquer comme option de commande à chaque fois que nous lançons le job.</p>
<p>Le reste du code de l'application est la version en Java de l'exemple en scala que nous avions fait avec spark-shell.</p>
</div>
<h3 id="test-du-code-en-local">Test du code en local<a class="headerlink" href="#test-du-code-en-local" title="Permanent link">&para;</a></h3>
<p>Pour tester le code sur votre machine, procéder aux étapes suivantes:</p>
<ol>
<li>Insérer un fichier texte de votre choix (par exemple le fameux <a href="https://s3-eu-west-1.amazonaws.com/insat.lilia.bigdata.bucket/data/loremipsum.txt">loremipsum.txt</a>) dans le répertoire src/main/resources.</li>
<li>Créer une nouvelle configuration de type "Application" (<em>Run-&gt;Edit Configurations</em>) que vous appellerez <em>WordCountTask</em>, et définir les arguments suivants (fichier de départ et répertoire d'arrivée) comme <em>Program arguments</em>:<br />
  <div class="codehilite"><pre><span></span>  src/main/resources/loremipsum.txt src/main/resources/out
</pre></div></li>
<li>Cliquer sur OK, et lancer la configuration. Si tout se passe bien, un répertoire <em>out</em> sera créé sous <em>resources</em>, qui contient deux fichiers: part-00000, part-00001.</li>
</ol>
<p><img alt="Resultat Batch Local" src="../img/tp2/resultat-batch-local.png" /></p>
<h3 id="lancement-du-code-sur-le-cluster">Lancement du code sur le cluster<a class="headerlink" href="#lancement-du-code-sur-le-cluster" title="Permanent link">&para;</a></h3>
<p>Pour exécuter le code sur le cluster, modifier comme indiqué les lignes en jaune dans ce qui suit:</p>
<div class="codehilite"><pre><span></span><span class="kd">public</span> <span class="kd">class</span> <span class="nc">WordCountTask</span> <span class="o">{</span>
  <span class="kd">private</span> <span class="kd">static</span> <span class="kd">final</span> <span class="n">Logger</span> <span class="n">LOGGER</span> <span class="o">=</span> <span class="n">LoggerFactory</span><span class="o">.</span><span class="na">getLogger</span><span class="o">(</span><span class="n">WordCountTask</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>

  <span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="n">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">checkArgument</span><span class="o">(</span><span class="n">args</span><span class="o">.</span><span class="na">length</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="o">,</span> <span class="s">&quot;Please provide the path of input file and output dir as parameters.&quot;</span><span class="o">);</span>
      <span class="k">new</span> <span class="n">WordCountTask</span><span class="o">().</span><span class="na">run</span><span class="o">(</span><span class="n">args</span><span class="o">[</span><span class="mi">0</span><span class="o">],</span> <span class="n">args</span><span class="o">[</span><span class="mi">1</span><span class="o">]);</span>
  <span class="o">}</span>

  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">run</span><span class="o">(</span><span class="n">String</span> <span class="n">inputFilePath</span><span class="o">,</span> <span class="n">String</span> <span class="n">outputDir</span><span class="o">)</span> <span class="o">{</span>

<span class="hll">      <span class="n">SparkConf</span> <span class="n">conf</span> <span class="o">=</span> <span class="k">new</span> <span class="n">SparkConf</span><span class="o">()</span>
</span><span class="hll">              <span class="o">.</span><span class="na">setAppName</span><span class="o">(</span><span class="n">WordCountTask</span><span class="o">.</span><span class="na">class</span><span class="o">.</span><span class="na">getName</span><span class="o">());</span>
</span>
      <span class="n">JavaSparkContext</span> <span class="n">sc</span> <span class="o">=</span> <span class="k">new</span> <span class="n">JavaSparkContext</span><span class="o">(</span><span class="n">conf</span><span class="o">);</span>

      <span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">textFile</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="na">textFile</span><span class="o">(</span><span class="n">inputFilePath</span><span class="o">);</span>
      <span class="n">JavaPairRDD</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">textFile</span>
<span class="hll">              <span class="o">.</span><span class="na">flatMap</span><span class="o">(</span><span class="n">s</span> <span class="o">-&gt;</span> <span class="n">Arrays</span><span class="o">.</span><span class="na">asList</span><span class="o">(</span><span class="n">s</span><span class="o">.</span><span class="na">split</span><span class="o">(</span><span class="s">&quot;\t&quot;</span><span class="o">)).</span><span class="na">iterator</span><span class="o">())</span>
</span>              <span class="o">.</span><span class="na">mapToPair</span><span class="o">(</span><span class="n">word</span> <span class="o">-&gt;</span> <span class="k">new</span> <span class="n">Tuple2</span><span class="o">&lt;&gt;(</span><span class="n">word</span><span class="o">,</span> <span class="mi">1</span><span class="o">))</span>
              <span class="o">.</span><span class="na">reduceByKey</span><span class="o">((</span><span class="n">a</span><span class="o">,</span> <span class="n">b</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="o">);</span>
      <span class="n">counts</span><span class="o">.</span><span class="na">saveAsTextFile</span><span class="o">(</span><span class="n">outputDir</span><span class="o">);</span>
  <span class="o">}</span>
<span class="o">}</span>
</pre></div>

<p>Lancer ensuite une configuration de type Maven, avec les commandes <em>package install</em>. Un fichier intitulé <em>worcount-1.jar</em> sera créé sous le répertoire target.</p>
<p>Nous allons maintenant copier ce fichier dans docker. Pour cela, naviguer vers le répertoire du projet avec votre terminal (ou plus simplement utiliser le terminal dans IntelliJ), et taper la commande suivante:</p>
<div class="codehilite"><pre><span></span>  docker cp target/wordcount-1 hadoop-master:/root/wordcount-1.jar
</pre></div>

<p>Revenir à votre contenaire master, et lancer un job Spark en utilisant ce fichier jar généré, avec la commande <code>spark-submit</code>, un script utilisé pour lancer des applications spark sur un cluster.</p>
<div class="codehilite"><pre><span></span>  spark-submit  --class tn.insat.tp21.WordCountTask
                --master <span class="nb">local</span>
                --driver-memory 4g --executor-memory 2g --executor-cores <span class="m">1</span>
                wordcount.jar
                input/purchases.txt
                output
</pre></div>

<ul>
<li>Nous allons lancer le job en mode local, pour commencer.</li>
<li>Le fichier en entrée est le fichier purchases.txt (que vous trouverez déjà chargé sur le contenaire master), et le résultat sera stocké dans un répertoire <em>output</em>.</li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Attention</p>
<p>Vérifiez bien que le fichier <em>purchases</em> existe dans le répertoire input de HDFS, et que le répertoire <em>output</em> n'existe pas!</p>
</div>
<p>Si tout se passe bien, vous devriez trouver, dans le répertoire <em>output</em>, deux fichiers part-00000 et part-00001, qui ressemblent à ce qui suit:</p>
<p><center><img src="../img/tp2/output-batch.png" width="300"></center></p>
<p>Nous allons maintenant tester le comportement de <em>spark-submit</em> si on l'exécute en mode <em>cluster</em> sur YARN. Pour cela, exécuter le code suivant:<br />
<div class="codehilite"><pre><span></span>  spark-submit  --class tn.insat.tp21.WordCountTask
                --master yarn
                --deploy-mode cluster
                --driver-memory 4g --executor-memory 2g --executor-cores <span class="m">1</span>
                wordcount.jar
                input/purchases.txt
                output2
</pre></div></p>
<ul>
<li>En lançant le job sur Yarn, deux modes de déploiement sont possibles:<ul>
<li>Mode cluster: où tout le job s'exécute dans le cluster, c'est à dire les Spark Executors (qui exécutent les vraies tâches) et le Spark Driver (qui ordonnance les Executors). Ce dernier sera encapsulé dans un YARN Application Master.</li>
<li>Mode client : où Spark Driver s'exécute sur la machine cliente (tel que votre propre ordinateur portable). Si votre machine s'éteint, le job s'arrête. Ce mode est approprié pour les jobs interactifs.</li>
</ul>
</li>
</ul>
<p>Si tout se passe bien, vous devriez obtenir un répertoire output2 dans HDFS avec les fichiers usuels.</p>
<div class="admonition bug">
<p class="admonition-title">Erreur</p>
<p>En cas d'erreur ou d'interruption du job sur Yarn, vous pourrez consulter les fichiers logs pour chercher le message d'erreur (le message affiché sur la console n'est pas assez explicite). Pour cela, sur votre navigateur, aller à l'adresse: <code>http://localhost:8041/logs/userlogs</code>et suivez toujours les derniers liens jusqu'à <em>stderr</em>.</p>
</div>
<h2 id="spark-streaming">Spark Streaming<a class="headerlink" href="#spark-streaming" title="Permanent link">&para;</a></h2>
<p>Spark est connu pour supporter également le traitement des données en streaming. Les données peuvent être lues à partir de plusieurs sources tel que Kafka, Flume, Kinesis ou des sockets TCP, et peuvent être traitées en utilisant des algorithmes complexes. Ensuite, les données traitées peuvent être stockées sur des systèmes de fichiers, des bases de données ou des dashboards. Il est même possible de réaliser des algorithmes de machine learning et du traitement de graphes sur les flux de données.</p>
<p><center><img src="../img/tp2/streaming.png" width="400"></center></p>
<p>En interne, il fonctionne comme suit. Spark Streaming reçoit des données en streaming et les divise en micro-batches, qui sont ensuite calculés par le moteur de spark pour générer le flux final de résultats.</p>
<p><center><img src="../img/tp2/micro-batch.png" width="500"></center></p>
<h3 id="environnement-et-code">Environnement et Code<a class="headerlink" href="#environnement-et-code" title="Permanent link">&para;</a></h3>
<p>Nous allons commencer par tester le streaming en local, comme d'habitude. Pour cela:</p>
<ol>
<li>Commencer par créer un nouveau projet Maven, avec le fichier pom suivant:<br />
  <div class="codehilite"><pre><span></span><span class="cp">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span>
<span class="nt">&lt;project</span> <span class="na">xmlns=</span><span class="s">&quot;http://maven.apache.org/POM/4.0.0&quot;</span>
       <span class="na">xmlns:xsi=</span><span class="s">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span>
       <span class="na">xsi:schemaLocation=</span><span class="s">&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;</span><span class="nt">&gt;</span>
  <span class="nt">&lt;modelVersion&gt;</span>4.0.0<span class="nt">&lt;/modelVersion&gt;</span>

  <span class="nt">&lt;groupId&gt;</span>spark.streaming<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>stream<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1<span class="nt">&lt;/version&gt;</span>

  <span class="nt">&lt;dependencies&gt;</span>
      <span class="nt">&lt;dependency&gt;</span>
          <span class="nt">&lt;groupId&gt;</span>org.apache.spark<span class="nt">&lt;/groupId&gt;</span>
          <span class="nt">&lt;artifactId&gt;</span>spark-core_2.11<span class="nt">&lt;/artifactId&gt;</span>
          <span class="nt">&lt;version&gt;</span>2.2.1<span class="nt">&lt;/version&gt;</span>
      <span class="nt">&lt;/dependency&gt;</span>
      <span class="nt">&lt;dependency&gt;</span>
          <span class="nt">&lt;groupId&gt;</span>org.apache.spark<span class="nt">&lt;/groupId&gt;</span>
          <span class="nt">&lt;artifactId&gt;</span>spark-streaming_2.11<span class="nt">&lt;/artifactId&gt;</span>
          <span class="nt">&lt;version&gt;</span>2.2.1<span class="nt">&lt;/version&gt;</span>
      <span class="nt">&lt;/dependency&gt;</span>
  <span class="nt">&lt;/dependencies&gt;</span>
  <span class="nt">&lt;build&gt;</span>
      <span class="nt">&lt;plugins&gt;</span>
          <span class="nt">&lt;plugin&gt;</span>
              <span class="nt">&lt;groupId&gt;</span>org.apache.maven.plugins<span class="nt">&lt;/groupId&gt;</span>
              <span class="nt">&lt;artifactId&gt;</span>maven-compiler-plugin<span class="nt">&lt;/artifactId&gt;</span>
              <span class="nt">&lt;version&gt;</span>3.1<span class="nt">&lt;/version&gt;</span>
              <span class="nt">&lt;configuration&gt;</span>
                  <span class="nt">&lt;source&gt;</span>1.8<span class="nt">&lt;/source&gt;</span>
                  <span class="nt">&lt;target&gt;</span>1.8<span class="nt">&lt;/target&gt;</span>
              <span class="nt">&lt;/configuration&gt;</span>
          <span class="nt">&lt;/plugin&gt;</span>
      <span class="nt">&lt;/plugins&gt;</span>
  <span class="nt">&lt;/build&gt;</span>

<span class="nt">&lt;/project&gt;</span>
</pre></div></li>
<li>Créer une classe <em>tn.insat.tp22.Stream</em> avec le code suivant:</li>
</ol>
<div class="codehilite"><pre><span></span>    <span class="kd">public</span> <span class="kd">class</span> <span class="nc">Stream</span> <span class="o">{</span>
    <span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="n">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">InterruptedException</span> <span class="o">{</span>
        <span class="n">SparkConf</span> <span class="n">conf</span> <span class="o">=</span> <span class="k">new</span> <span class="n">SparkConf</span><span class="o">()</span>
            <span class="o">.</span><span class="na">setAppName</span><span class="o">(</span><span class="s">&quot;NetworkWordCount&quot;</span><span class="o">)</span>
            <span class="o">.</span><span class="na">setMaster</span><span class="o">(</span><span class="s">&quot;local&quot;</span><span class="o">);</span>
        <span class="n">JavaStreamingContext</span> <span class="n">jssc</span> <span class="o">=</span>
            <span class="k">new</span> <span class="n">JavaStreamingContext</span><span class="o">(</span><span class="n">conf</span><span class="o">,</span> <span class="n">Durations</span><span class="o">.</span><span class="na">seconds</span><span class="o">(</span><span class="mi">1</span><span class="o">));</span>

        <span class="n">JavaReceiverInputDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">lines</span> <span class="o">=</span>
            <span class="n">jssc</span><span class="o">.</span><span class="na">socketTextStream</span><span class="o">(</span><span class="s">&quot;localhost&quot;</span><span class="o">,</span> <span class="mi">9999</span><span class="o">);</span>

        <span class="n">JavaDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">words</span> <span class="o">=</span>
            <span class="n">lines</span><span class="o">.</span><span class="na">flatMap</span><span class="o">(</span><span class="n">x</span> <span class="o">-&gt;</span> <span class="n">Arrays</span><span class="o">.</span><span class="na">asList</span><span class="o">(</span><span class="n">x</span><span class="o">.</span><span class="na">split</span><span class="o">(</span><span class="s">&quot; &quot;</span><span class="o">)).</span><span class="na">iterator</span><span class="o">());</span>
        <span class="n">JavaPairDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;</span> <span class="n">pairs</span> <span class="o">=</span>
            <span class="n">words</span><span class="o">.</span><span class="na">mapToPair</span><span class="o">(</span><span class="n">s</span> <span class="o">-&gt;</span> <span class="k">new</span> <span class="n">Tuple2</span><span class="o">&lt;&gt;(</span><span class="n">s</span><span class="o">,</span> <span class="mi">1</span><span class="o">));</span>
        <span class="n">JavaPairDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;</span> <span class="n">wordCounts</span> <span class="o">=</span>
            <span class="n">pairs</span><span class="o">.</span><span class="na">reduceByKey</span><span class="o">((</span><span class="n">i1</span><span class="o">,</span> <span class="n">i2</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="n">i1</span> <span class="o">+</span> <span class="n">i2</span><span class="o">);</span>

        <span class="n">wordCounts</span><span class="o">.</span><span class="na">print</span><span class="o">();</span>
        <span class="n">jssc</span><span class="o">.</span><span class="na">start</span><span class="o">();</span>
        <span class="n">jssc</span><span class="o">.</span><span class="na">awaitTermination</span><span class="o">();</span>
    <span class="o">}</span>
  <span class="o">}</span>
</pre></div>

<p>Ce code permet de calculer le nombre de mots dans un stream de données toutes les secondes.</p>
<h3 id="test-du-code-en-local_1">Test du code en Local<a class="headerlink" href="#test-du-code-en-local_1" title="Permanent link">&para;</a></h3>
<p>Le stream ici sera diffusé par une petite commande utilitaire qui se trouve dans la majorité des systèmes Unix-like.</p>
<ul>
<li>Ouvrir un terminal, et taper la commande suivante pour créer le stream:<br />
  <div class="codehilite"><pre><span></span>  nc -lk <span class="m">9999</span>
</pre></div><br />
  Vous pourrez alors taper les entrées de votre choix.</li>
<li>Exécuter votre classe <em>Stream</em>. Vous verrez défiler sur votre console des lignes en continu: l'application est en écoute sur localhost:9999.</li>
</ul>
<p>A chaque fois que vous entrez quelque chose sur le terminal, l'application l'intercepte, et l'affichage sur l'écran de la console change, comme suit:</p>
<p><img alt="Test Streaming" src="../img/tp2/stream-intercepted.png" /></p>
<p>Ensuite, pour voir le résultat final du comptage, arrêter l'exécution en cliquant sur le carré rouge, puis observer la console, vous verrez un affichage qui ressemble à ceci:</p>
<p><img alt="Test Streaming" src="../img/tp2/stream-result.png" /></p>
<h3 id="lancement-du-code-sur-le-cluster_1">Lancement du code sur le cluster<a class="headerlink" href="#lancement-du-code-sur-le-cluster_1" title="Permanent link">&para;</a></h3>
<p>Pour lancer le code précédent sur le cluster, il faudra d'abord faire des petites modifications:</p>
<div class="admonition warning">
<p class="admonition-title">Attention</p>
<p>Veillez à mettre l'IP de votre machine locale (sur laquelle vous allez lancer le flux avec <em>nc</em>) à la place de &lt;votre-ip>. Vous pourrez trouver votre IP avec la commande ifconfig.</p>
</div>
<ul>
<li>Lancer un <code>mvn package install</code>pour créer le fichier jar.</li>
<li>Copier le fichier jar sur le contenaire hadoop.</li>
<li>Lancer la commande suivante:</li>
</ul>
<p><div class="codehilite"><pre><span></span>    spark-submit --class tn.insat.tp22.Stream
                 --master <span class="nb">local</span>
                 --driver-memory 4g --executor-memory 2g --executor-cores <span class="m">1</span>
                 stream-1.jar
</pre></div><br />
Observer le résultat.</p>
<h2 id="homework">Homework<a class="headerlink" href="#homework" title="Permanent link">&para;</a></h2>
<p>Vous allez, pour ce cours, réaliser un projet en binôme, qui consiste en la construction d'une architecture Big Data supportant le streaming, le batch processing, et le dashboarding temps réel. Pour la séance prochaine, vous allez réfléchir au type de traitement que vous voulez réaliser (le flux de données en entrée, et les résultats en sortie). Vous allez commencer par utiliser Spark pour réaliser ces traitements, avec un stockage sur HDFS au besoin.</p>
                
                  
                
              
              
                
              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../tp1/" title="TP1 - Hadoop et Map Reduce" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                TP1 - Hadoop et Map Reduce
              </span>
            </div>
          </a>
        
        
          <a href="../tp3/" title="TP3 - Apacke Kafka" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                TP3 - Apacke Kafka
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2017 - 2018 Lilia Sfaxi
          </div>
        
        powered by
        <a href="http://www.mkdocs.org" title="MkDocs">MkDocs</a>
        and
        <a href="http://squidfunk.github.io/mkdocs-material/" title="Material for MkDocs">
          Material for MkDocs</a>
      </div>
      
        
  <div class="md-footer-social">
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    
      <a href="http://liliasfaxi.wix.com/liliasfaxi" class="md-footer-social__link fa fa-globe"></a>
    
      <a href="https://github.com/liliasfaxi" class="md-footer-social__link fa fa-github-alt"></a>
    
      <a href="https://twitter.com/lillitou" class="md-footer-social__link fa fa-twitter"></a>
    
      <a href="https://www.linkedin.com/in/liliasfaxi/" class="md-footer-social__link fa fa-linkedin"></a>
    
  </div>

      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/raphael-min.js"></script>
      <script src="../assets/javascripts/flowchart.js"></script>
      <script src="../assets/javascripts/application-e3caa82af6.js"></script>
      
      
      <script>app.initialize({url:{base:".."}})</script>
      
        <script src="../search/require.js"></script>
      
        <script src="../search/search.js"></script>
      
    
    
      
      <script>!function(e,t,a,n,o,c,i){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,c=t.createElement(a),i=t.getElementsByTagName(a)[0],c.async=1,c.src=n,i.parentNode.insertBefore(c,i)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","None","auto"),ga("set","anonymizeIp",!0),ga("send","pageview");var links=document.getElementsByTagName("a");Array.prototype.map.call(links,function(e){e.host!=document.location.host&&e.addEventListener("click",function(){var t=e.getAttribute("data-md-action")||"follow";ga("send","event","outbound",t,e.href)})});var query=document.forms.search.query;query.addEventListener("blur",function(){if(this.value){var e=document.location.pathname;ga("send","pageview",e+"?q="+this.value)}})</script>
      
    
  </body>
</html>