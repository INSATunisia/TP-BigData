<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Les Travaux Pratiques du cours Big Data"><meta name=author content="Lilia Sfaxi"><link href=http://INSATunisia.github.io/TP-BigData/tp2/ rel=canonical><link rel=icon href=../img/favicon.ico><meta name=generator content="mkdocs-1.2.3, mkdocs-material-8.1.10"><title>TP2 - Traitement par Lot et Streaming avec Spark - TP Big Data</title><link rel=stylesheet href=../assets/stylesheets/main.d6be258b.min.css><link rel=stylesheet href=../assets/stylesheets/palette.e6a45f82.min.css><meta name=theme-color content=#2094f3><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../css/timeago.css><link rel=stylesheet href=../stylesheets/extra.css><link rel=stylesheet href=../stylesheets/links.css><script>__md_scope=new URL("..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme data-md-color-primary=blue data-md-color-accent=yellow> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#telecharger-pdf class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=.. title="TP Big Data" class="md-header__button md-logo" aria-label="TP Big Data" data-md-component=logo> <img src=../img/logo.png alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> TP Big Data </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> TP2 - Traitement par Lot et Streaming avec Spark </span> </div> </div> </div> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg> </button> </nav> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/INSATunisia/TP-BigData/ title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg> </div> <div class=md-source__repository> INSATunisia/TP-BigData </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=.. title="TP Big Data" class="md-nav__button md-logo" aria-label="TP Big Data" data-md-component=logo> <img src=../img/logo.png alt=logo> </a> TP Big Data </label> <div class=md-nav__source> <a href=https://github.com/INSATunisia/TP-BigData/ title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg> </div> <div class=md-source__repository> INSATunisia/TP-BigData </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=.. class=md-nav__link> Travaux Pratiques Big Data </a> </li> <li class=md-nav__item> <a href=../tp1/ class=md-nav__link> TP1 - Le traitement Batch avec Hadoop HDFS et Map Reduce </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" data-md-toggle=toc type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> TP2 - Traitement par Lot et Streaming avec Spark <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> TP2 - Traitement par Lot et Streaming avec Spark </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#telecharger-pdf class=md-nav__link> Télécharger PDF </a> </li> <li class=md-nav__item> <a href=#objectifs-du-tp class=md-nav__link> Objectifs du TP </a> </li> <li class=md-nav__item> <a href=#outils-et-versions class=md-nav__link> Outils et Versions </a> </li> <li class=md-nav__item> <a href=#spark class=md-nav__link> Spark </a> <nav class=md-nav aria-label=Spark> <ul class=md-nav__list> <li class=md-nav__item> <a href=#presentation class=md-nav__link> Présentation </a> </li> <li class=md-nav__item> <a href=#spark-et-hadoop class=md-nav__link> Spark et Hadoop </a> </li> <li class=md-nav__item> <a href=#installation class=md-nav__link> Installation </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#test-de-spark-avec-spark-shell class=md-nav__link> Test de Spark avec Spark-Shell </a> </li> <li class=md-nav__item> <a href=#lapi-de-spark class=md-nav__link> L'API de Spark </a> <nav class=md-nav aria-label="L'API de Spark"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#exemple class=md-nav__link> Exemple </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#spark-batch-en-java class=md-nav__link> Spark Batch en Java </a> <nav class=md-nav aria-label="Spark Batch en Java"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#preparation-de-lenvironnement-et-code class=md-nav__link> Préparation de l'environnement et Code </a> </li> <li class=md-nav__item> <a href=#test-du-code-en-local class=md-nav__link> Test du code en local </a> </li> <li class=md-nav__item> <a href=#lancement-du-code-sur-le-cluster class=md-nav__link> Lancement du code sur le cluster </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#spark-streaming class=md-nav__link> Spark Streaming </a> <nav class=md-nav aria-label="Spark Streaming"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#environnement-et-code class=md-nav__link> Environnement et Code </a> </li> <li class=md-nav__item> <a href=#test-du-code-en-local_1 class=md-nav__link> Test du code en Local </a> </li> <li class=md-nav__item> <a href=#lancement-du-code-sur-le-cluster_1 class=md-nav__link> Lancement du code sur le cluster </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#homework class=md-nav__link> Homework </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../tp3/ class=md-nav__link> TP3 - La Collecte de Données avec le Bus Kafka </a> </li> <li class=md-nav__item> <a href=../tp4/ class=md-nav__link> TP4 - Stockage de Données dans une Base NOSQL avec HBase </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#telecharger-pdf class=md-nav__link> Télécharger PDF </a> </li> <li class=md-nav__item> <a href=#objectifs-du-tp class=md-nav__link> Objectifs du TP </a> </li> <li class=md-nav__item> <a href=#outils-et-versions class=md-nav__link> Outils et Versions </a> </li> <li class=md-nav__item> <a href=#spark class=md-nav__link> Spark </a> <nav class=md-nav aria-label=Spark> <ul class=md-nav__list> <li class=md-nav__item> <a href=#presentation class=md-nav__link> Présentation </a> </li> <li class=md-nav__item> <a href=#spark-et-hadoop class=md-nav__link> Spark et Hadoop </a> </li> <li class=md-nav__item> <a href=#installation class=md-nav__link> Installation </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#test-de-spark-avec-spark-shell class=md-nav__link> Test de Spark avec Spark-Shell </a> </li> <li class=md-nav__item> <a href=#lapi-de-spark class=md-nav__link> L'API de Spark </a> <nav class=md-nav aria-label="L'API de Spark"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#exemple class=md-nav__link> Exemple </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#spark-batch-en-java class=md-nav__link> Spark Batch en Java </a> <nav class=md-nav aria-label="Spark Batch en Java"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#preparation-de-lenvironnement-et-code class=md-nav__link> Préparation de l'environnement et Code </a> </li> <li class=md-nav__item> <a href=#test-du-code-en-local class=md-nav__link> Test du code en local </a> </li> <li class=md-nav__item> <a href=#lancement-du-code-sur-le-cluster class=md-nav__link> Lancement du code sur le cluster </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#spark-streaming class=md-nav__link> Spark Streaming </a> <nav class=md-nav aria-label="Spark Streaming"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#environnement-et-code class=md-nav__link> Environnement et Code </a> </li> <li class=md-nav__item> <a href=#test-du-code-en-local_1 class=md-nav__link> Test du code en Local </a> </li> <li class=md-nav__item> <a href=#lancement-du-code-sur-le-cluster_1 class=md-nav__link> Lancement du code sur le cluster </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#homework class=md-nav__link> Homework </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/INSATunisia/TP-BigData/edit/master/docs/tp2.md title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg> </a> <h1>TP2 - Traitement par Lot et Streaming avec Spark</h1> <p><center><img alt="Stream Processing" src=../img/stream.png></center></p> <h3 id=telecharger-pdf>Télécharger PDF<a class=headerlink href=#telecharger-pdf title="Permanent link">&para;</a></h3> <p><a href=../tp2.pdf><img alt="Download TP2" src=../img/pdf.png></a></p> <h3 id=objectifs-du-tp>Objectifs du TP<a class=headerlink href=#objectifs-du-tp title="Permanent link">&para;</a></h3> <p>Utilisation de Spark pour réaliser des traitements par lot et des traitements en streaming.</p> <h3 id=outils-et-versions>Outils et Versions<a class=headerlink href=#outils-et-versions title="Permanent link">&para;</a></h3> <ul> <li><a href=http://hadoop.apache.org/ >Apache Hadoop</a> Version: 3.3.6</li> <li><a href=https://spark.apache.org/ >Apache Spark</a> Version: 3.5.0</li> <li><a href=https://www.docker.com/ >Docker</a> Version <em>latest</em></li> <li><a href=https://code.visualstudio.com/ >Visual Studio Code</a> Version 1.85.1 (ou tout autre IDE de votre choix)</li> <li><a href=http://www.oracle.com/technetwork/java/javase/downloads/index.html>Java</a> Version 1.8.</li> <li>Unix-like ou Unix-based Systems (Divers Linux et MacOS)</li> </ul> <h3 id=spark>Spark<a class=headerlink href=#spark title="Permanent link">&para;</a></h3> <h4 id=presentation>Présentation<a class=headerlink href=#presentation title="Permanent link">&para;</a></h4> <p><a href=https://spark.apache.org/ >Spark</a> est un système de traitement rapide et parallèle. Il fournit des APIs de haut niveau en Java, Scala, Python et R, et un moteur optimisé qui supporte l'exécution des graphes. Il supporte également un ensemble d'outils de haut niveau tels que <a href=https://spark.apache.org/docs/latest/sql-programming-guide.html>Spark SQL</a> pour le support du traitement de données structurées, <a href=https://spark.apache.org/docs/latest/ml-guide.html>MLlib</a> pour l'apprentissage des données, <a href=https://spark.apache.org/docs/latest/graphx-programming-guide.html>GraphX</a> pour le traitement des graphes, et <a href=https://spark.apache.org/docs/latest/streaming-programming-guide.html>Spark Streaming</a> pour le traitment des données en streaming.</p> <p><center><img src=../img/tp2/spark.png width=200></center> <center><img src=../img/tp2/spark-layers.png width=500></center></p> <h4 id=spark-et-hadoop>Spark et Hadoop<a class=headerlink href=#spark-et-hadoop title="Permanent link">&para;</a></h4> <p>Spark peut s'exécuter sur plusieurs plateformes: Hadoop, Mesos, en standalone ou sur le cloud. Il peut également accéder à diverses sources de données, comme HDFS, Cassandra, HBase et S3.</p> <p>Dans ce TP, nous allons exécuter Spark sur Hadoop YARN. YARN s'occupera ainsi de la gestion des ressources pour le déclenchement et l'exécution des Jobs Spark.</p> <h4 id=installation>Installation<a class=headerlink href=#installation title="Permanent link">&para;</a></h4> <p>Nous avons procédé à l'installation de Spark sur le cluster Hadoop utilisé dans le <a href=tp1/index.html>TP1</a>. Suivre les étapes décrites dans la partie <em>Installation</em> du <a href=tp1/index.html#installation>TP1</a> pour télécharger l'image et exécuter les trois contenaires. Si cela est déjà fait, il suffit de lancer vos machines grâce aux commandes suivantes:</p> <div class=highlight><pre><span></span><code>docker start hadoop-master hadoop-worker1 hadoop-worker2
</code></pre></div> <p>puis d'entrer dans le contenaire master:</p> <div class=highlight><pre><span></span><code>docker <span class=nb>exec</span> -it hadoop-master bash
</code></pre></div> <p>Lancer ensuite les démons yarn et hdfs: <div class=highlight><pre><span></span><code>./start-hadoop.sh
</code></pre></div></p> <p>Vous pourrez vérifier que tous les démons sont lancés en tapant: <code>jps</code>. Un résultat semblable au suivant pourra être visible: <div class=highlight><pre><span></span><code><span class=m>880</span> Jps
<span class=m>257</span> NameNode
<span class=m>613</span> ResourceManager
<span class=m>456</span> SecondaryNameNode
</code></pre></div></p> <p>La même opération sur les noeuds workers (auxquels vous accédez à partir de votre machine hôte de la même façon que le noeud maître, c'est à dire en tapant par exemple <code>docker exec -it hadoop-worker1 bash</code>) devrait donner: <div class=highlight><pre><span></span><code><span class=m>176</span> NodeManager
<span class=m>65</span> DataNode
<span class=m>311</span> Jps
</code></pre></div></p> <h3 id=test-de-spark-avec-spark-shell>Test de Spark avec Spark-Shell<a class=headerlink href=#test-de-spark-avec-spark-shell title="Permanent link">&para;</a></h3> <p>Dans le but de tester l'exécution de spark, commencer par créer un fichier <em>file1.txt</em> dans votre noeud master, contenant le texte suivant: <div class=highlight><pre><span></span><code>Hello Spark Wordcount!
Hello Hadoop Also :)
</code></pre></div></p> <p>Charger ensuite ce fichier dans HDFS: <div class=highlight><pre><span></span><code>hdfs dfs -put file1.txt
</code></pre></div></p> <details class=error> <summary>Erreur possible</summary> <p>Si le message suivant s'affiche: <code>put: `.': No such file or directory</code>, c'est parce que l'arborescence du répertoire principal n'est pas créée dans HDFS. Pour le faire, il suffit d'exécuter la commande suivante avant la commande de chargement : <code>hadoop fs mkdir -p .</code></p> </details> <p>Pour vérifier que spark est bien installé, taper la commande suivante: <div class=highlight><pre><span></span><code>spark-shell
</code></pre></div></p> <p>Vous devriez avoir un résultat semblable au suivant: <img alt="Spark Shell" src=../img/tp2/spark-shell.png></p> <p>Vous pourrez tester spark avec un code scala simple comme suit (à exécuter ligne par ligne):</p> <table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal>1</span>
<span class=normal>2</span>
<span class=normal>3</span>
<span class=normal>4</span></pre></div></td><td class=code><div class=highlight><pre><span></span><code><span class=kd>val</span> <span class=n>lines</span> <span class=o>=</span> <span class=n>sc</span><span class=p>.</span><span class=n>textFile</span><span class=p>(</span><span class=s>&quot;file1.txt&quot;</span><span class=p>)</span>
<span class=kd>val</span> <span class=n>words</span> <span class=o>=</span> <span class=n>lines</span><span class=p>.</span><span class=n>flatMap</span><span class=p>(</span><span class=n>_</span><span class=p>.</span><span class=n>split</span><span class=p>(</span><span class=s>&quot;\\s+&quot;</span><span class=p>))</span>
<span class=kd>val</span> <span class=n>wc</span> <span class=o>=</span> <span class=n>words</span><span class=p>.</span><span class=n>map</span><span class=p>(</span><span class=n>w</span> <span class=o>=&gt;</span> <span class=p>(</span><span class=n>w</span><span class=p>,</span> <span class=mi>1</span><span class=p>)).</span><span class=n>reduceByKey</span><span class=p>(</span><span class=n>_</span> <span class=o>+</span> <span class=n>_</span><span class=p>)</span>
<span class=n>wc</span><span class=p>.</span><span class=n>saveAsTextFile</span><span class=p>(</span><span class=s>&quot;file1.count&quot;</span><span class=p>)</span>
</code></pre></div> </td></tr></table> <p>Ce code vient de (1) charger le fichier <em>file1.txt</em> de HDFS, (2) séparer les mots selon les caractères d'espacement, (3) appliquer un <em>map</em> sur les mots obtenus qui produit le couple (<em>&lt;mot></em>, 1), puis un <em>reduce</em> qui permet de faire la somme des 1 des mots identiques.</p> <p>Pour afficher le résultat, sortir de spark-shell en cliquant sur <em>Ctrl-C</em>. Télécharger ensuite le répertoire <em>file1.count</em> créé dans HDFS comme suit: <div class=highlight><pre><span></span><code>hdfs dfs -get file1.count
</code></pre></div> Le contenu des deux fichiers <em>part-00000</em> et <em>part-00001</em> ressemble à ce qui suit:</p> <p><center><img src=../img/tp2/spark-shell-result.png width=400px></center></p> <h3 id=lapi-de-spark>L'API de Spark<a class=headerlink href=#lapi-de-spark title="Permanent link">&para;</a></h3> <p>A un haut niveau d'abstraction, chaque application Spark consiste en un programme <em>driver</em> qui exécute la fonction <em>main</em> de l'utilisateur et lance plusieurs opérations parallèles sur le cluster. L'abstraction principale fournie par Spark est un RDD (<em>Resilient Distributed Dataset</em>), qui représente une collection d'éléments partitionnés à travers les noeuds du cluster, et sur lesquelles on peut opérer en parallèle. Les RDDs sont créés à partir d'un fichier dans HDFS par exemple, puis le transforment. Les utilisateurs peuvent demander à Spark de sauvegarder un RDD en mémoire, lui permettant ainsi d'être réutilisé efficacement à travers plusieurs opérations parallèles.</p> <p><center><img src=../img/tp2/RDD.png width=100></center></p> <p>Les RDDs supportent deux types d'opérations:</p> <ul> <li>les <em>transformations</em>, qui permettent de créer un nouveau Dataset à partir d'un Dataset existant</li> <li>les <em>actions</em>, qui retournent une valeur au programme <em>driver</em> après avoir exécuté un calcul sur le Dataset.</li> </ul> <p>Par exemple, un <em>map</em> est une transformation qui passe chaque élément du dataset via une fonction, et retourne un nouvel RDD représentant les résultats. Un <em>reduce</em> est une action qui agrège tous les éléments du RDD en utilisant une certaine fonction et retourne le résultat final au programme.</p> <p>Toutes les transformations dans Spark sont <em>lazy</em>, car elles ne calculent pas le résultat immédiatement. Elles se souviennent des transformations appliquées à un dataset de base (par ex. un fichier). Les transformations ne sont calculées que quand une action nécessite qu'un résultat soit retourné au programme principal. Cela permet à Spark de s'exécuter plus efficacement.</p> <p><center><img src=../img/tp2/RDD-trans-action.png width=500></center></p> <h4 id=exemple>Exemple<a class=headerlink href=#exemple title="Permanent link">&para;</a></h4> <p>L'exemple que nous allons présenter ici par étapes permet de relever les mots les plus fréquents dans un fichier. Pour cela, le code suivant est utilisé:</p> <p><div class=highlight><pre><span></span><code><span class=c1>//Etape 1 - Créer un RDD à partir d&#39;un fichier texte de Hadoop</span>
<span class=kd>val</span> <span class=n>docs</span> <span class=o>=</span> <span class=n>sc</span><span class=p>.</span><span class=n>textFile</span><span class=p>(</span><span class=s>&quot;file1.txt&quot;</span><span class=p>)</span>
</code></pre></div> <center><img src=../img/tp2/ex1.png width=500></center></p> <p><div class=highlight><pre><span></span><code><span class=c1>//Etape 2 - Convertir les lignes en minuscule</span>
<span class=kd>val</span> <span class=n>lower</span> <span class=o>=</span> <span class=n>docs</span><span class=p>.</span><span class=n>map</span><span class=p>(</span><span class=n>line</span> <span class=o>=&gt;</span> <span class=n>line</span><span class=p>.</span><span class=n>toLowerCase</span><span class=p>)</span>
</code></pre></div> <center><img src=../img/tp2/ex2.png width=500></center></p> <p><div class=highlight><pre><span></span><code><span class=c1>//Etape 3 - Séparer les lignes en mots</span>
<span class=kd>val</span> <span class=n>words</span> <span class=o>=</span> <span class=n>lower</span><span class=p>.</span><span class=n>flatMap</span><span class=p>(</span><span class=n>line</span> <span class=o>=&gt;</span> <span class=n>line</span><span class=p>.</span><span class=n>split</span><span class=p>(</span><span class=s>&quot;\\s+&quot;</span><span class=p>))</span>
</code></pre></div> <center><img src=../img/tp2/ex3.png width=500></center></p> <p><div class=highlight><pre><span></span><code><span class=c1>//Etape 4 - produire les tuples (mot, 1)</span>
<span class=kd>val</span> <span class=n>counts</span> <span class=o>=</span> <span class=n>words</span><span class=p>.</span><span class=n>map</span><span class=p>(</span><span class=n>word</span> <span class=o>=&gt;</span> <span class=p>(</span><span class=n>word</span><span class=p>,</span><span class=mi>1</span><span class=p>))</span>
</code></pre></div> <center><img src=../img/tp2/ex4.png width=500></center></p> <p><div class=highlight><pre><span></span><code><span class=c1>//Etape 5 - Compter tous les mots</span>
<span class=kd>val</span> <span class=n>freq</span> <span class=o>=</span> <span class=n>counts</span><span class=p>.</span><span class=n>reduceByKey</span><span class=p>(</span><span class=n>_</span> <span class=o>+</span> <span class=n>_</span><span class=p>)</span>
</code></pre></div> <center><img src=../img/tp2/ex5.png width=500></center></p> <p><div class=highlight><pre><span></span><code><span class=c1>//Etape 6 - Inverser les tuples (transformation avec swap)</span>
<span class=n>freq</span><span class=p>.</span><span class=n>map</span><span class=p>(</span><span class=n>_</span><span class=p>.</span><span class=n>swap</span><span class=p>)</span>
</code></pre></div> <center><img src=../img/tp2/ex6.png width=400></center></p> <p><div class=highlight><pre><span></span><code><span class=c1>//Etape 6 - Inverser les tuples (action de sélection des 3 premiers)</span>
<span class=kd>val</span> <span class=n>top</span> <span class=o>=</span> <span class=n>freq</span><span class=p>.</span><span class=n>map</span><span class=p>(</span><span class=n>_</span><span class=p>.</span><span class=n>swap</span><span class=p>).</span><span class=n>top</span><span class=p>(</span><span class=mi>3</span><span class=p>)</span>
</code></pre></div> <center><img src=../img/tp2/ex7.png width=500></center></p> <h3 id=spark-batch-en-java>Spark Batch en Java<a class=headerlink href=#spark-batch-en-java title="Permanent link">&para;</a></h3> <h4 id=preparation-de-lenvironnement-et-code>Préparation de l'environnement et Code<a class=headerlink href=#preparation-de-lenvironnement-et-code title="Permanent link">&para;</a></h4> <p>Nous allons dans cette partie créer un projet Spark Batch en Java (un simple WordCount), le charger sur le cluster et lancer le job.</p> <ol> <li>Créer un projet Maven avec VSCode, en utilisant la config suivante: <div class=highlight><pre><span></span><code><span class=nt>&lt;groupId&gt;</span>spark.batch<span class=nt>&lt;/groupId&gt;</span>
<span class=nt>&lt;artifactId&gt;</span>wordcount-spark<span class=nt>&lt;/artifactId&gt;</span>
</code></pre></div></li> <li>Rajouter dans le fichier pom les dépendances nécessaires, et indiquer la version du compilateur Java: <div class=highlight><pre><span></span><code><span class=nt>&lt;properties&gt;</span>
    <span class=nt>&lt;maven.compiler.source&gt;</span>1.8<span class=nt>&lt;/maven.compiler.source&gt;</span>
    <span class=nt>&lt;maven.compiler.target&gt;</span>1.8<span class=nt>&lt;/maven.compiler.target&gt;</span>
<span class=nt>&lt;/properties&gt;</span>
<span class=nt>&lt;dependencies&gt;</span>
    <span class=nt>&lt;dependency&gt;</span>
        <span class=nt>&lt;groupId&gt;</span>org.apache.spark<span class=nt>&lt;/groupId&gt;</span>
        <span class=nt>&lt;artifactId&gt;</span>spark-core_2.13<span class=nt>&lt;/artifactId&gt;</span>
        <span class=nt>&lt;version&gt;</span>3.5.0<span class=nt>&lt;/version&gt;</span>
    <span class=nt>&lt;/dependency&gt;</span>
    <span class=nt>&lt;dependency&gt;</span>
        <span class=nt>&lt;groupId&gt;</span>org.slf4j<span class=nt>&lt;/groupId&gt;</span>
        <span class=nt>&lt;artifactId&gt;</span>slf4j-reload4j<span class=nt>&lt;/artifactId&gt;</span>
        <span class=nt>&lt;version&gt;</span>2.1.0-alpha1<span class=nt>&lt;/version&gt;</span>
        <span class=nt>&lt;scope&gt;</span>test<span class=nt>&lt;/scope&gt;</span>
    <span class=nt>&lt;/dependency&gt;</span>
<span class=nt>&lt;/dependencies&gt;</span>
</code></pre></div></li> <li>Sous le répertoire java, créer un package que vous appellerez <em>spark.batch.tp21</em>, et dedans, une classe appelée <em>WordCountTask</em>.</li> <li> <p>Écrire le code suivant dans <em>WordCountTask.java</em> : <div class=highlight><pre><span></span><code><span class=kn>import</span> <span class=nn>org.apache.spark.SparkConf</span><span class=p>;</span>
<span class=kn>import</span> <span class=nn>org.apache.spark.api.java.JavaPairRDD</span><span class=p>;</span>
<span class=kn>import</span> <span class=nn>org.apache.spark.api.java.JavaRDD</span><span class=p>;</span>
<span class=kn>import</span> <span class=nn>org.apache.spark.api.java.JavaSparkContext</span><span class=p>;</span>
<span class=kn>import</span> <span class=nn>org.slf4j.Logger</span><span class=p>;</span>
<span class=kn>import</span> <span class=nn>org.slf4j.LoggerFactory</span><span class=p>;</span>
<span class=kn>import</span> <span class=nn>scala.Tuple2</span><span class=p>;</span>

<span class=kn>import</span> <span class=nn>java.util.Arrays</span><span class=p>;</span>
<span class=kn>import</span> <span class=nn>com.google.common.base.Preconditions</span><span class=p>;</span>

<span class=kd>public</span> <span class=kd>class</span> <span class=nc>WordCountTask</span> <span class=p>{</span>
      <span class=kd>private</span> <span class=kd>static</span> <span class=kd>final</span> <span class=n>Logger</span> <span class=n>LOGGER</span> <span class=o>=</span> <span class=n>LoggerFactory</span><span class=p>.</span><span class=na>getLogger</span><span class=p>(</span><span class=n>WordCountTask</span><span class=p>.</span><span class=na>class</span><span class=p>);</span>

      <span class=kd>public</span> <span class=kd>static</span> <span class=kt>void</span> <span class=nf>main</span><span class=p>(</span><span class=n>String</span><span class=o>[]</span> <span class=n>args</span><span class=p>)</span> <span class=p>{</span>
          <span class=n>Preconditions</span><span class=p>.</span><span class=na>checkArgument</span><span class=p>(</span><span class=n>args</span><span class=p>.</span><span class=na>length</span> <span class=o>&gt;</span> <span class=mi>1</span><span class=p>,</span> <span class=s>&quot;Please provide the path of input file and output dir as parameters.&quot;</span><span class=p>);</span>
          <span class=k>new</span> <span class=n>WordCountTask</span><span class=p>().</span><span class=na>run</span><span class=p>(</span><span class=n>args</span><span class=o>[</span><span class=mi>0</span><span class=o>]</span><span class=p>,</span> <span class=n>args</span><span class=o>[</span><span class=mi>1</span><span class=o>]</span><span class=p>);</span>
      <span class=p>}</span>

      <span class=kd>public</span> <span class=kt>void</span> <span class=nf>run</span><span class=p>(</span><span class=n>String</span> <span class=n>inputFilePath</span><span class=p>,</span> <span class=n>String</span> <span class=n>outputDir</span><span class=p>)</span> <span class=p>{</span>
          <span class=n>String</span> <span class=n>master</span> <span class=o>=</span> <span class=s>&quot;local[*]&quot;</span><span class=p>;</span>
          <span class=n>SparkConf</span> <span class=n>conf</span> <span class=o>=</span> <span class=k>new</span> <span class=n>SparkConf</span><span class=p>()</span>
                  <span class=p>.</span><span class=na>setAppName</span><span class=p>(</span><span class=n>WordCountTask</span><span class=p>.</span><span class=na>class</span><span class=p>.</span><span class=na>getName</span><span class=p>())</span>
                  <span class=p>.</span><span class=na>setMaster</span><span class=p>(</span><span class=n>master</span><span class=p>);</span>
          <span class=n>JavaSparkContext</span> <span class=n>sc</span> <span class=o>=</span> <span class=k>new</span> <span class=n>JavaSparkContext</span><span class=p>(</span><span class=n>conf</span><span class=p>);</span>

          <span class=n>JavaRDD</span><span class=o>&lt;</span><span class=n>String</span><span class=o>&gt;</span> <span class=n>textFile</span> <span class=o>=</span> <span class=n>sc</span><span class=p>.</span><span class=na>textFile</span><span class=p>(</span><span class=n>inputFilePath</span><span class=p>);</span>
          <span class=n>JavaPairRDD</span><span class=o>&lt;</span><span class=n>String</span><span class=p>,</span> <span class=n>Integer</span><span class=o>&gt;</span> <span class=n>counts</span> <span class=o>=</span> <span class=n>textFile</span>
                  <span class=p>.</span><span class=na>flatMap</span><span class=p>(</span><span class=n>s</span> <span class=o>-&gt;</span> <span class=n>Arrays</span><span class=p>.</span><span class=na>asList</span><span class=p>(</span><span class=n>s</span><span class=p>.</span><span class=na>split</span><span class=p>(</span><span class=s>&quot; &quot;</span><span class=p>)).</span><span class=na>iterator</span><span class=p>())</span>
                  <span class=p>.</span><span class=na>mapToPair</span><span class=p>(</span><span class=n>word</span> <span class=o>-&gt;</span> <span class=k>new</span> <span class=n>Tuple2</span><span class=o>&lt;&gt;</span><span class=p>(</span><span class=n>word</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
                  <span class=p>.</span><span class=na>reduceByKey</span><span class=p>((</span><span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>a</span> <span class=o>+</span> <span class=n>b</span><span class=p>);</span>
          <span class=n>counts</span><span class=p>.</span><span class=na>saveAsTextFile</span><span class=p>(</span><span class=n>outputDir</span><span class=p>);</span>
      <span class=p>}</span>
  <span class=p>}</span>
</code></pre></div> La première chose à faire dans un programme Spark est de créer un objet <em>JavaSparkContext</em>, qui indique à Spark comment accéder à un cluster. Pour créer ce contexte, vous aurez besoin de construire un objet <em>SparkConf</em> qui contient toutes les informations sur l'application.</p> <ul> <li><em>appName</em> est le nom de l'application</li> <li><em>master</em> est une URL d'un cluster Spark, Mesos ou YARN, ou bien une chaîne spéciale <em>local</em> pour lancer le job en mode local.</li> </ul> </li> </ol> <div class="admonition warning"> <p class=admonition-title>Warning</p> <p>Nous avons indiqué ici que notre master est <em>local</em> pour les besoins du test, mais plus tard, en le packageant pour le cluster, nous allons enlever cette indication. Il est en effet déconseillé de la hard-coder dans le programme, il faudrait plutôt l'indiquer comme option de commande à chaque fois que nous lançons le job.</p> <p>Le reste du code de l'application est la version en Java de l'exemple en scala que nous avions fait avec spark-shell.</p> </div> <h4 id=test-du-code-en-local>Test du code en local<a class=headerlink href=#test-du-code-en-local title="Permanent link">&para;</a></h4> <p>Pour tester le code sur votre machine, procéder aux étapes suivantes:</p> <ol> <li>Insérer un fichier texte de votre choix (par exemple le fameux <a href=https://generator.lorem-ipsum.info/ >loremipsum.txt</a>) dans le répertoire src/main/resources.</li> <li>Lancer le programme en utilisant les arguments suivants:<ol> <li><strong>Arg1</strong>: le chemin du fichier <em>loremipsum.txt</em></li> <li><strong>Arg2</strong>: le chemin d'un répertoire <em>out</em> sous <em>resources</em> (vous ne devez pas le créer)</li> </ol> </li> <li>Cliquer sur OK, et lancer la configuration. Si tout se passe bien, un répertoire <em>out</em> sera créé sous <em>resources</em>, qui contient (entre autres) deux fichiers: part-00000, part-00001.</li> </ol> <p><center><img src=../img/tp2/resultat-batch-local.png width=200></center></p> <h4 id=lancement-du-code-sur-le-cluster>Lancement du code sur le cluster<a class=headerlink href=#lancement-du-code-sur-le-cluster title="Permanent link">&para;</a></h4> <p>Pour exécuter le code sur le cluster, modifier comme indiqué les lignes en jaune dans ce qui suit:</p> <div class=highlight><pre><span></span><code><span class=kd>public</span> <span class=kd>class</span> <span class=nc>WordCountTask</span> <span class=p>{</span>
  <span class=kd>private</span> <span class=kd>static</span> <span class=kd>final</span> <span class=n>Logger</span> <span class=n>LOGGER</span> <span class=o>=</span> <span class=n>LoggerFactory</span><span class=p>.</span><span class=na>getLogger</span><span class=p>(</span><span class=n>WordCountTask</span><span class=p>.</span><span class=na>class</span><span class=p>);</span>

  <span class=kd>public</span> <span class=kd>static</span> <span class=kt>void</span> <span class=nf>main</span><span class=p>(</span><span class=n>String</span><span class=o>[]</span> <span class=n>args</span><span class=p>)</span> <span class=p>{</span>
      <span class=n>checkArgument</span><span class=p>(</span><span class=n>args</span><span class=p>.</span><span class=na>length</span> <span class=o>&gt;</span> <span class=mi>1</span><span class=p>,</span> <span class=s>&quot;Please provide the path of input file and output dir as parameters.&quot;</span><span class=p>);</span>
      <span class=k>new</span> <span class=n>WordCountTask</span><span class=p>().</span><span class=na>run</span><span class=p>(</span><span class=n>args</span><span class=o>[</span><span class=mi>0</span><span class=o>]</span><span class=p>,</span> <span class=n>args</span><span class=o>[</span><span class=mi>1</span><span class=o>]</span><span class=p>);</span>
  <span class=p>}</span>

  <span class=kd>public</span> <span class=kt>void</span> <span class=nf>run</span><span class=p>(</span><span class=n>String</span> <span class=n>inputFilePath</span><span class=p>,</span> <span class=n>String</span> <span class=n>outputDir</span><span class=p>)</span> <span class=p>{</span>

<span class=hll>      <span class=n>SparkConf</span> <span class=n>conf</span> <span class=o>=</span> <span class=k>new</span> <span class=n>SparkConf</span><span class=p>()</span>
</span><span class=hll>              <span class=p>.</span><span class=na>setAppName</span><span class=p>(</span><span class=n>WordCountTask</span><span class=p>.</span><span class=na>class</span><span class=p>.</span><span class=na>getName</span><span class=p>());</span>
</span>
      <span class=n>JavaSparkContext</span> <span class=n>sc</span> <span class=o>=</span> <span class=k>new</span> <span class=n>JavaSparkContext</span><span class=p>(</span><span class=n>conf</span><span class=p>);</span>

      <span class=n>JavaRDD</span><span class=o>&lt;</span><span class=n>String</span><span class=o>&gt;</span> <span class=n>textFile</span> <span class=o>=</span> <span class=n>sc</span><span class=p>.</span><span class=na>textFile</span><span class=p>(</span><span class=n>inputFilePath</span><span class=p>);</span>
      <span class=n>JavaPairRDD</span><span class=o>&lt;</span><span class=n>String</span><span class=p>,</span> <span class=n>Integer</span><span class=o>&gt;</span> <span class=n>counts</span> <span class=o>=</span> <span class=n>textFile</span>
<span class=hll>              <span class=p>.</span><span class=na>flatMap</span><span class=p>(</span><span class=n>s</span> <span class=o>-&gt;</span> <span class=n>Arrays</span><span class=p>.</span><span class=na>asList</span><span class=p>(</span><span class=n>s</span><span class=p>.</span><span class=na>split</span><span class=p>(</span><span class=s>&quot;\t&quot;</span><span class=p>)).</span><span class=na>iterator</span><span class=p>())</span>
</span>              <span class=p>.</span><span class=na>mapToPair</span><span class=p>(</span><span class=n>word</span> <span class=o>-&gt;</span> <span class=k>new</span> <span class=n>Tuple2</span><span class=o>&lt;&gt;</span><span class=p>(</span><span class=n>word</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
              <span class=p>.</span><span class=na>reduceByKey</span><span class=p>((</span><span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>a</span> <span class=o>+</span> <span class=n>b</span><span class=p>);</span>
      <span class=n>counts</span><span class=p>.</span><span class=na>saveAsTextFile</span><span class=p>(</span><span class=n>outputDir</span><span class=p>);</span>
  <span class=p>}</span>
<span class=p>}</span>
</code></pre></div> <p>Lancer ensuite une configuration de type Maven, avec la commande <em>package</em>. Un fichier intitulé <em>wordcount-spark-1.0-SNAPSHOT.jar</em> sera créé sous le répertoire target.</p> <p>Nous allons maintenant copier ce fichier dans docker. Pour cela, naviguer vers le répertoire du projet avec votre terminal (ou plus simplement utiliser le terminal dans VSCode), et taper la commande suivante:</p> <div class=highlight><pre><span></span><code>docker cp target/wordcount-spark-1.0-SNAPSHOT.jar hadoop-master:/root/wordcount-spark.jar
</code></pre></div> <p>Revenir à votre contenaire master, et lancer un job Spark en utilisant ce fichier jar généré, avec la commande <code>spark-submit</code>, un script utilisé pour lancer des applications spark sur un cluster.</p> <div class=highlight><pre><span></span><code>spark-submit  --class spark.batch.tp21.WordCountTask --master <span class=nb>local</span> wordcount-spark.jar input/purchases.txt out-spark
</code></pre></div> <ul> <li>Nous allons lancer le job en mode local, pour commencer.</li> <li>Le fichier en entrée est le fichier purchases.txt (que vous déjà chargé dans HDFS dans le TP précédent), et le résultat sera stocké dans un nouveau répertoire <em>out-spark</em>.</li> </ul> <div class="admonition warning"> <p class=admonition-title>Attention</p> <p>Vérifiez bien que le fichier <em>purchases</em> existe dans le répertoire input de HDFS (et que le répertoire <em>out-spark</em> n'existe pas)! Si ce n'est pas le cas, vous pouvez le charger avec les commandes suivantes: <div class=highlight><pre><span></span><code>hdfs dfs -mkdir -p input
hdfs dfs -put purchases.txt input
</code></pre></div></p> </div> <p>Si tout se passe bien, vous devriez trouver, dans le répertoire <em>out-spark</em>, deux fichiers part-00000 et part-00001, qui ressemblent à ce qui suit:</p> <p><center><img src=../img/tp2/output-batch.png width=300></center></p> <p>Nous allons maintenant tester le comportement de <em>spark-submit</em> si on l'exécute en mode <em>cluster</em> sur YARN. Pour cela, exécuter le code suivant: <div class=highlight><pre><span></span><code>  spark-submit  --class spark.batch.tp21.WordCountTask --master yarn --deploy-mode cluster wordcount-spark.jar input/purchases.txt out-spark2
</code></pre></div></p> <ul> <li>En lançant le job sur Yarn, deux modes de déploiement sont possibles:<ul> <li><strong>Mode cluster</strong>: où tout le job s'exécute dans le cluster, c'est à dire les Spark Executors (qui exécutent les vraies tâches) et le Spark Driver (qui ordonnance les Executors). Ce dernier sera encapsulé dans un YARN Application Master.</li> <li><strong>Mode client</strong> : où Spark Driver s'exécute sur la machine cliente (tel que votre propre ordinateur portable). Si votre machine s'éteint, le job s'arrête. Ce mode est approprié pour les jobs interactifs.</li> </ul> </li> </ul> <p>Si tout se passe bien, vous devriez obtenir un répertoire out-spark2 dans HDFS avec les fichiers usuels.</p> <details class=bug> <summary>En cas d'erreur: consulter les logs!</summary> <p>En cas d'erreur ou d'interruption du job sur Yarn, vous pourrez consulter les fichiers logs pour chercher le message d'erreur (le message affiché sur la console n'est pas assez explicite). Pour cela, sur votre navigateur, aller à l'adresse: <code>http://localhost:8041/logs/userlogs</code>et suivez toujours les derniers liens jusqu'à <em>stderr</em>.</p> </details> <h3 id=spark-streaming>Spark Streaming<a class=headerlink href=#spark-streaming title="Permanent link">&para;</a></h3> <p>Spark est connu pour supporter également le traitement des données en streaming. Les données peuvent être lues à partir de plusieurs sources tel que Kafka, Flume, Kinesis ou des sockets TCP, et peuvent être traitées en utilisant des algorithmes complexes. Ensuite, les données traitées peuvent être stockées sur des systèmes de fichiers, des bases de données ou des dashboards. Il est même possible de réaliser des algorithmes de machine learning et de traitement de graphes sur les flux de données.</p> <p><center><img src=../img/tp2/streaming.png width=400></center></p> <p>En interne, il fonctionne comme suit: Spark Streaming reçoit des données en streaming et les divise en micro-batches, qui sont ensuite calculés par le moteur de spark pour générer le flux final de résultats.</p> <p><center><img src=../img/tp2/micro-batch.png width=500></center></p> <h4 id=environnement-et-code>Environnement et Code<a class=headerlink href=#environnement-et-code title="Permanent link">&para;</a></h4> <p>Nous allons commencer par tester le streaming en local, comme d'habitude. Pour cela:</p> <ol> <li>Commencer par créer un nouveau projet Maven, avec le fichier pom suivant: <div class=highlight><pre><span></span><code><span class=cp>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span>
<span class=nt>&lt;project</span> <span class=na>xmlns=</span><span class=s>&quot;http://maven.apache.org/POM/4.0.0&quot;</span>
       <span class=na>xmlns:xsi=</span><span class=s>&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span>
       <span class=na>xsi:schemaLocation=</span><span class=s>&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;</span><span class=nt>&gt;</span>
  <span class=nt>&lt;modelVersion&gt;</span>4.0.0<span class=nt>&lt;/modelVersion&gt;</span>

  <span class=nt>&lt;groupId&gt;</span>spark.streaming<span class=nt>&lt;/groupId&gt;</span>
  <span class=nt>&lt;artifactId&gt;</span>stream<span class=nt>&lt;/artifactId&gt;</span>
  <span class=nt>&lt;version&gt;</span>1.0-SNAPSHOT<span class=nt>&lt;/version&gt;</span>

  <span class=nt>&lt;properties&gt;</span>
      <span class=nt>&lt;maven.compiler.source&gt;</span>1.8<span class=nt>&lt;/maven.compiler.source&gt;</span>
      <span class=nt>&lt;maven.compiler.target&gt;</span>1.8<span class=nt>&lt;/maven.compiler.target&gt;</span>
  <span class=nt>&lt;/properties&gt;</span>

  <span class=nt>&lt;dependencies&gt;</span>
      <span class=nt>&lt;dependency&gt;</span>
          <span class=nt>&lt;groupId&gt;</span>org.apache.spark<span class=nt>&lt;/groupId&gt;</span>
          <span class=nt>&lt;artifactId&gt;</span>spark-core_2.13<span class=nt>&lt;/artifactId&gt;</span>
          <span class=nt>&lt;version&gt;</span>3.5.0<span class=nt>&lt;/version&gt;</span>
      <span class=nt>&lt;/dependency&gt;</span>
      <span class=nt>&lt;dependency&gt;</span>
        <span class=nt>&lt;groupId&gt;</span>org.apache.spark<span class=nt>&lt;/groupId&gt;</span>
        <span class=nt>&lt;artifactId&gt;</span>spark-streaming_2.13<span class=nt>&lt;/artifactId&gt;</span>
        <span class=nt>&lt;version&gt;</span>3.5.0<span class=nt>&lt;/version&gt;</span>
    <span class=nt>&lt;/dependency&gt;</span>

  <span class=nt>&lt;/dependencies&gt;</span>
<span class=nt>&lt;/project&gt;</span>
</code></pre></div></li> <li>Créer une classe <em>spark.streaming.tp22.Stream</em> avec le code suivant:</li> </ol> <div class=highlight><pre><span></span><code><span class=kn>import</span> <span class=nn>org.apache.spark.sql.Dataset</span><span class=p>;</span>
<span class=kn>import</span> <span class=nn>org.apache.spark.sql.Encoders</span><span class=p>;</span>
<span class=kn>import</span> <span class=nn>org.apache.spark.sql.SparkSession</span><span class=p>;</span>
<span class=kn>import</span> <span class=nn>org.apache.spark.sql.streaming.StreamingQuery</span><span class=p>;</span>
<span class=kn>import</span> <span class=nn>org.apache.spark.sql.streaming.StreamingQueryException</span><span class=p>;</span>
<span class=kn>import</span> <span class=nn>org.apache.spark.sql.streaming.Trigger</span><span class=p>;</span>

<span class=kn>import</span> <span class=nn>java.util.concurrent.TimeoutException</span><span class=p>;</span>
<span class=kn>import</span> <span class=nn>java.util.Arrays</span><span class=p>;</span>

<span class=kd>public</span> <span class=kd>class</span> <span class=nc>Stream</span> <span class=p>{</span>
    <span class=kd>public</span> <span class=kd>static</span> <span class=kt>void</span> <span class=nf>main</span><span class=p>(</span><span class=n>String</span><span class=o>[]</span> <span class=n>args</span><span class=p>)</span> <span class=kd>throws</span> <span class=n>StreamingQueryException</span><span class=p>,</span> <span class=n>TimeoutException</span>  <span class=p>{</span>
        <span class=n>SparkSession</span> <span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span>
            <span class=p>.</span><span class=na>builder</span><span class=p>()</span>
            <span class=p>.</span><span class=na>appName</span><span class=p>(</span><span class=s>&quot;NetworkWordCount&quot;</span><span class=p>)</span>
            <span class=p>.</span><span class=na>master</span><span class=p>(</span><span class=s>&quot;local[*]&quot;</span><span class=p>)</span>
            <span class=p>.</span><span class=na>getOrCreate</span><span class=p>();</span>

        <span class=c1>// Create DataFrame representing the stream of input lines from connection to localhost:9999</span>
        <span class=n>Dataset</span><span class=o>&lt;</span><span class=n>String</span><span class=o>&gt;</span> <span class=n>lines</span> <span class=o>=</span> <span class=n>spark</span>
            <span class=p>.</span><span class=na>readStream</span><span class=p>()</span>
            <span class=p>.</span><span class=na>format</span><span class=p>(</span><span class=s>&quot;socket&quot;</span><span class=p>)</span>
            <span class=p>.</span><span class=na>option</span><span class=p>(</span><span class=s>&quot;host&quot;</span><span class=p>,</span> <span class=s>&quot;localhost&quot;</span><span class=p>)</span>
            <span class=p>.</span><span class=na>option</span><span class=p>(</span><span class=s>&quot;port&quot;</span><span class=p>,</span> <span class=mi>9999</span><span class=p>)</span>
            <span class=p>.</span><span class=na>load</span><span class=p>()</span>
            <span class=p>.</span><span class=na>as</span><span class=p>(</span><span class=n>Encoders</span><span class=p>.</span><span class=na>STRING</span><span class=p>());</span>

        <span class=c1>// Split the lines into words</span>
        <span class=n>Dataset</span><span class=o>&lt;</span><span class=n>String</span><span class=o>&gt;</span> <span class=n>words</span> <span class=o>=</span> <span class=n>lines</span><span class=p>.</span><span class=na>flatMap</span><span class=p>(</span>
            <span class=p>(</span><span class=n>String</span> <span class=n>x</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Arrays</span><span class=p>.</span><span class=na>asList</span><span class=p>(</span><span class=n>x</span><span class=p>.</span><span class=na>split</span><span class=p>(</span><span class=s>&quot; &quot;</span><span class=p>)).</span><span class=na>iterator</span><span class=p>(),</span>
            <span class=n>Encoders</span><span class=p>.</span><span class=na>STRING</span><span class=p>());</span>

        <span class=c1>// Generate running word count</span>
        <span class=n>Dataset</span><span class=o>&lt;</span><span class=n>org</span><span class=p>.</span><span class=na>apache</span><span class=p>.</span><span class=na>spark</span><span class=p>.</span><span class=na>sql</span><span class=p>.</span><span class=na>Row</span><span class=o>&gt;</span> <span class=n>wordCounts</span> <span class=o>=</span> <span class=n>words</span><span class=p>.</span><span class=na>groupBy</span><span class=p>(</span><span class=s>&quot;value&quot;</span><span class=p>).</span><span class=na>count</span><span class=p>();</span>

        <span class=c1>// Start running the query that prints the running counts to the console</span>
        <span class=n>StreamingQuery</span> <span class=n>query</span> <span class=o>=</span> <span class=n>wordCounts</span><span class=p>.</span><span class=na>writeStream</span><span class=p>()</span>
            <span class=p>.</span><span class=na>outputMode</span><span class=p>(</span><span class=s>&quot;complete&quot;</span><span class=p>)</span>
            <span class=p>.</span><span class=na>format</span><span class=p>(</span><span class=s>&quot;console&quot;</span><span class=p>)</span>
            <span class=p>.</span><span class=na>trigger</span><span class=p>(</span><span class=n>Trigger</span><span class=p>.</span><span class=na>ProcessingTime</span><span class=p>(</span><span class=s>&quot;1 second&quot;</span><span class=p>))</span>
            <span class=p>.</span><span class=na>start</span><span class=p>();</span>

        <span class=n>query</span><span class=p>.</span><span class=na>awaitTermination</span><span class=p>();</span>
    <span class=p>}</span>
<span class=p>}</span>
</code></pre></div> <p>Ce code permet de calculer le nombre de mots dans un stream de données (provenant du port localhost:9999) chaque seconde. Dans sa version actuelle, Spark encourage l'utilisation de <em>Structured Streaming</em>,une API de haut niveau qui fournit un traitement plus efficace, et qui est construite au dessus de Spark SQL, en intégrant les structures DataFrame et Dataset.</p> <details class=info> <summary>Trigger Interval</summary> <p>Dans Spark Structured Streaming, le concept de microbatch est utilisé pour traiter les données en continu par petits lots incrémentaux. La durée de chaque micro-lot est configurable et détermine la fréquence de traitement des données en continu. Cette durée est appelée "intervalle de déclenchement". Si vous ne spécifiez pas explicitement d'intervalle de déclenchement, le trigger par défaut est <em>ProcessingTime(0)</em>, qui est aussi connu comme le mode de traitement par micro-lots. Ce paramètre par défaut signifie que Spark essaiera de traiter les données aussi rapidement que possible, sans délai fixe entre les micro-lots.</p> </details> <h4 id=test-du-code-en-local_1>Test du code en Local<a class=headerlink href=#test-du-code-en-local_1 title="Permanent link">&para;</a></h4> <p>Le stream ici sera diffusé par une petite commande utilitaire qui se trouve dans la majorité des systèmes Unix-like.</p> <ul> <li>Ouvrir un terminal, et taper la commande suivante pour créer le stream: <div class=highlight><pre><span></span><code>  nc -lk <span class=m>9999</span>
</code></pre></div></li> <li>Exécuter votre classe <em>Stream</em>. L'application est en écoute sur localhost:9999.</li> <li>Commencer à écrire des messages sur la console de votre terminal (là où vous avez lancé la commande nc)</li> </ul> <p>A chaque fois que vous entrez quelque chose sur le terminal, l'application Stream l'intercepte, et l'affichage sur l'écran de la console change, comme suit: <center><img src=../img/tp2/stream-intercepted.png width=500></center></p> <h4 id=lancement-du-code-sur-le-cluster_1>Lancement du code sur le cluster<a class=headerlink href=#lancement-du-code-sur-le-cluster_1 title="Permanent link">&para;</a></h4> <p>Pour lancer le code précédent sur le cluster, il faudra d'abord faire une petite modification: <strong>changer la valeur <em>localhost</em> par l'IP de votre machine hote</strong> (celle que vous utilisez pour lancer la commande <em>nc</em>). </p> <ul> <li>Générer le fichier jar.</li> <li>Copier le fichier jar sur le contenaire master.</li> <li>Assurez-vous que la commande <em>nc</em> tourne bien sur votre machine, en attente de messages.</li> <li>Sur votre noeud master, lancer la commande suivante:</li> </ul> <p><div class=highlight><pre><span></span><code>spark-submit  --class spark.streaming.tp22.Stream --master <span class=nb>local</span> stream.jar
</code></pre></div> Cette fois, énormément de texte est généré en continu sur la console. Comme nous avons défini dans l'application <em>console</em> comme sortie, le résultat du traitement s'affichera au milieu de tout ce texte. Une fois que vous aurez saisi le texte à tester, arrêter l'application (avec Ctrl-C), et chercher dans le texte la chaîne "<strong>Batch:</strong>". Vous trouverez normalement un résultat semblable au suivant:</p> <p><center><img src=../img/tp2/stream-output.png width=700></center></p> <h3 id=homework>Homework<a class=headerlink href=#homework title="Permanent link">&para;</a></h3> <p>Vous allez maintenant appliquer des traitements sur votre projet selon votre besoin. Vos contraintes ici est d'avoir les deux types de traitement: Batch et Streaming. Vous pouvez utiliser Map Reduce ou Spark pour le traitement en Batch. </p> <hr> <div class=md-source-file> <small> Last update: <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-timeago"><span class=timeago datetime=2024-03-01T09:30:32+01:00 locale=en></span></span><span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_date">2024-03-01</span> </small> </div> </article> </div> </div> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../tp1/ class="md-footer__link md-footer__link--prev" aria-label="Previous: TP1 - Le traitement Batch avec Hadoop HDFS et Map Reduce" rel=prev> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </div> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction> Previous </span> TP1 - Le traitement Batch avec Hadoop HDFS et Map Reduce </div> </div> </a> <a href=../tp3/ class="md-footer__link md-footer__link--next" aria-label="Next: TP3 - La Collecte de Données avec le Bus Kafka" rel=next> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction> Next </span> TP3 - La Collecte de Données avec le Bus Kafka </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2017 - 2024 Lilia Sfaxi </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": [], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../assets/javascripts/workers/search.092fa1f6.min.js"}</script> <script src=../assets/javascripts/bundle.e3b2bf44.min.js></script> <script src=../js/timeago.min.js></script> <script src=../js/timeago_mkdocs_material.js></script> </body> </html>