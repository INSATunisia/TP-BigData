{
    "docs": [
        {
            "location": "/", 
            "text": "Travaux Pratiques Big Data\n\n\nCe(tte) \u0153uvre est mise \u00e0 disposition selon les termes de la \nLicence Creative Commons Attribution - Pas d\u2019Utilisation Commerciale - Partage dans les M\u00eames Conditions 4.0 International\n.\n\n\n\n\nGL4 - INSAT\n\n\n\n\n\n\nCours Disponible ici: \nhttp://liliasfaxi.wixsite.com/liliasfaxi/big-data\n\n\nEdmodo : \nhttps://edmodo.com/public/big-data/group_id/26293086\n\n\nRepo Github :  \nhttps://github.com/INSATunisia\n\n\n\n\nOverview\n\n\nL'objectif de ce cours est d'initier les \u00e9tudiants aux architectures Big Data: Les notions, technologies, patrons et bonnes pratiques.\n\nLe cours comportera les chapitres suivants:\n\n\n\n\nIntroduction au Big Data\n\n\nHadoop et Map Reduce\n\n\nTraitement de donn\u00e9es (Batch, Streaming, Temps R\u00e9el, etc.)\n\n\nBases de donn\u00e9es NoSQL\n\n\nPutting it all together\n\n\n\n\nCe cours comporte cinq s\u00e9ances de travaux pratiques:\n\n\n\n\nTBD", 
            "title": "Travaux Pratiques Big Data"
        }, 
        {
            "location": "/#travaux-pratiques-big-data", 
            "text": "Ce(tte) \u0153uvre est mise \u00e0 disposition selon les termes de la  Licence Creative Commons Attribution - Pas d\u2019Utilisation Commerciale - Partage dans les M\u00eames Conditions 4.0 International .", 
            "title": "Travaux Pratiques Big Data"
        }, 
        {
            "location": "/#gl4-insat", 
            "text": "Cours Disponible ici:  http://liliasfaxi.wixsite.com/liliasfaxi/big-data  Edmodo :  https://edmodo.com/public/big-data/group_id/26293086  Repo Github :   https://github.com/INSATunisia", 
            "title": "GL4 - INSAT"
        }, 
        {
            "location": "/#overview", 
            "text": "L'objectif de ce cours est d'initier les \u00e9tudiants aux architectures Big Data: Les notions, technologies, patrons et bonnes pratiques. \nLe cours comportera les chapitres suivants:   Introduction au Big Data  Hadoop et Map Reduce  Traitement de donn\u00e9es (Batch, Streaming, Temps R\u00e9el, etc.)  Bases de donn\u00e9es NoSQL  Putting it all together   Ce cours comporte cinq s\u00e9ances de travaux pratiques:   TBD", 
            "title": "Overview"
        }, 
        {
            "location": "/tp1/", 
            "text": "TP1 - Le traitement Batch avec Hadoop HDFS et Map Reduce\n\n\n\n\nT\u00e9l\u00e9charger PDF\n\n\n\n\nObjectifs du TP\n\n\nInitiation au framework hadoop et au patron MapReduce, utilisation de docker pour lancer un cluster hadoop de 3 noeuds.\n\n\nOutils et Versions\n\n\n\n\nApache Hadoop\n Version: 2.7.2.\n\n\nDocker\n Version 17.09.1\n\n\nIntelliJ IDEA\n Version Ultimate 2016.1 (ou tout autre IDE de votre choix)\n\n\nJava\n Version 1.8.\n\n\nUnix-like ou Unix-based Systems (Divers Linux et MacOS)\n\n\n\n\nHadoop\n\n\nPr\u00e9sentation\n\n\nApache Hadoop\n est un framework open-source pour stocker et traiter les donne\u0301es volumineuses sur un cluster. Il est utilise\u0301 par un grand nombre de contributeurs et utilisateurs. Il a une licence Apache 2.0.\n\n\n\n\nHadoop et Docker\n\n\nPour d\u00e9ployer le framework Hadoop, nous allons utiliser des contenaires \nDocker\n. L'utilisation des contenaires va garantir la consistance entre les environnements de d\u00e9veloppement et permettra de r\u00e9duire consid\u00e9rablement la complexit\u00e9 de configuration des machines (dans le cas d'un acc\u00e8s natif) ainsi que la lourdeur d'ex\u00e9cution (si on opte pour l'utilisation d'une machine virtuelle).\n\n\nNous avons pour le d\u00e9ploiement des ressources de ce TP suivi les instructions pr\u00e9sent\u00e9es \nici\n.\n\n\nInstallation\n\n\nNous allons utiliser tout au long de ce TP trois contenaires repr\u00e9sentant respectivement un noeud ma\u00eetre (Namenode) et deux noeuds esclaves (Datanodes).\n\n\nVous devez pour cela avoir install\u00e9 docker sur votre machine, et l'avoir correctement configur\u00e9. Ouvrir la ligne de commande, et taper les instructions suivantes:\n\n\n\n\nFaire un pull de l'image docker:\n\n\n  sudo docker pull kiwenlau/hadoop:1.0\n\n\n\nCloner le repo github contenant les fichiers n\u00e9cessaires pour le lancement des contenaires et leur configuration:\n\n\n  git clone https://github.com/liliasfaxi/hadoop-cluster-docker\n\n\n\nCr\u00e9er le r\u00e9seau Hadoop:\n\n\n  sudo docker network create --driver\n=\nbridge hadoop\n\n\n\nD\u00e9marrer les trois contenaires:\n\n\n  \ncd\n hadoop-cluster-docker\n  sudo ./start-container.sh\n\n\n\n\n\n\n\nAttention\n\n\nLe script \nstart-container.sh\n va r\u00e9initialiser les trois contenaires. Si vous voulez red\u00e9marrer un contenaire d\u00e9j\u00e0 cr\u00e9\u00e9, il ne faut pas l'ex\u00e9cuter de nouveau: tout sera effac\u00e9. Au lieu de cela, taper simplement (pour le master container, par exemple):\n\n\ndocker exec -it hadoop-master bash\n\n\n\n\nLe r\u00e9sultat de cette ex\u00e9cution sera le suivant:\n\n\n  start hadoop-master container...\n  start hadoop-slave1 container...\n  start hadoop-slave2 container...\n  root@hadoop-master:~#\n\n\n\n\nVous vous retrouverez dans le shell du namenode, et vous pourrez ainsi manipuler le cluster \u00e0 votre guise. La premi\u00e8re chose \u00e0 faire, une fois dans le contenaire, est de lancer hadoop et yarn. Un script est fourni pour cela, appel\u00e9 \nstart-hadoop.sh\n. Lancer ce script.\n\n\n  ./start-hadoop.sh\n\n\n\n\nLe r\u00e9sultat devra ressembler \u00e0 ce qui suit:\n\n\n\n\nPremiers pas avec Hadoop\n\n\nToutes les commandes interagissant avec le syste\u0300me Hadoop commencent par hadoop fs. Ensuite, les options rajoute\u0301es sont tre\u0300s largement inspire\u0301es des commandes Unix standard.\n\n\n\n\nCre\u0301er un re\u0301pertoire dans HDFS, appele\u0301 \ninput\n. Pour cela, taper:\n\n\n  hadoop fs \u2013mkdir -p input\n\n\n\n\n\n\n\nErreur\n\n\nSi pour une raison ou une autre, vous n'arrivez pas \u00e0 cr\u00e9er le r\u00e9pertoire \ninput\n, avec un message ressemblant \u00e0 ceci: \nls: `.': No such file or directory\n, veiller \u00e0 construire l'arborescence de l'utilisateur principal (root), comme suit:\n\n\nhadoop fs -mkdir -p /user/root\n\n\n\n\n\n\nNous allons utiliser le fichier \npurchases.txt\n comme entr\u00e9e pour le traitement MapReduce. Pour copier le fichier \npurchases.txt\n dans HDFS sous le re\u0301pertoire \ninput\n, t\u00e9l\u00e9charger ce fichier dans la machine master:\n\n  \n  wget https://s3-eu-west-1.amazonaws.com/insat.lilia.bigdata.bucket/data/purchases.txt\n\n\n\nCharger le fichier purchases dans le r\u00e9pertoire input que vous avez cr\u00e9\u00e9:\n\n  \n  hadoop fs \u2013put purchases.txt input\n\n\n\nPour afficher le contenu du re\u0301pertoire \ninput\n, la commande est:\n\n  \n  hadoop fs \u2013ls input\n\n\n\nPour afficher les derni\u00e8res lignes du fichier purchases:\n\n  \n  hadoop fs -tail input/purchases.txt\n\n\n\n\n\nLe r\u00e9sultat suivant va donc s'afficher:\n\n    \n\n\nNous pr\u00e9sentons dans le tableau suivant les commandes les plus utilis\u00e9es pour manipuler les fichiers dans HDFS:\n\n\n\n\n\n\n\n\nInstruction\n\n\nFonctionnalit\u00e9\n\n\n\n\n\n\n\n\n\n\nhadoop fs \u2013ls\n\n\nAfficher le contenu du re\u0301pertoire racine\n\n\n\n\n\n\nhadoop fs \u2013put file.txt\n\n\nUpload un fichier dans hadoop (a\u0300 partir du re\u0301pertoire courant linux)\n\n\n\n\n\n\nhadoop fs \u2013get file.txt\n\n\nDownload un fichier a\u0300 partir de hadoop sur votre disque local\n\n\n\n\n\n\nhadoop fs \u2013tail file.txt\n\n\nLire les dernie\u0300res lignes du fichier\n\n\n\n\n\n\nhadoop fs \u2013cat file.txt\n\n\nAffiche tout le contenu du fichier\n\n\n\n\n\n\nhadoop fs \u2013mv file.txt newfile.txt\n\n\nRenommer le fichier\n\n\n\n\n\n\nhadoop fs \u2013rm newfile.txt\n\n\nSupprimer le fichier\n\n\n\n\n\n\nhadoop fs \u2013mkdir myinput\n\n\nCre\u0301er un re\u0301pertoire\n\n\n\n\n\n\nhadoop fs \u2013cat file.txt \\| less\n\n\nLire le fichier page par page\n\n\n\n\n\n\n\n\nInterfaces web pour Hadoop\n\n\nHadoop offre plusieurs interfaces web pour pouvoir observer le comportement de ses diff\u00e9rentes composantes. Vous pouvez afficher ces pages en local sur votre machine gr\u00e2ce \u00e0 l'option -p de la commande \ndocker run\n. En effet, cette option permet de publier un port du contenaire sur la machine h\u00f4te. Pour pouvoir publier tous les ports expos\u00e9s, vous pouvez lancer votre contenaire en utilisant l'option \n-P\n.\n\n\nEn regardant le contenu du fichier \nstart-container.sh\n fourni dans le projet, vous verrez que deux ports de la machine ma\u00eetre ont \u00e9t\u00e9 expos\u00e9s:\n\n\n\n\nLe port \n50070\n: qui permet d'afficher les informations de votre namenode.\n\n\nLe port \n8088\n: qui permet d'afficher les informations du resource manager de Yarn et visualiser le comportement des diff\u00e9rents jobs.\n\n\n\n\nUne fois votre cluster lanc\u00e9 et pr\u00eat \u00e0 l'emploi, vous pouvez, sur votre navigateur pr\u00e9f\u00e9r\u00e9 de votre machine h\u00f4te, aller \u00e0 : \nhttp://localhost:50070\n. Vous obtiendrez le r\u00e9sultat suivant:\n\n\n\n\nVous pouvez \u00e9galement visualiser l'avancement et les r\u00e9sultats de vos Jobs (Map Reduce ou autre) en allant \u00e0 l'adresse: \nhttp://localhost:8088\n\n\n\n\nMap Reduce\n\n\nPr\u00e9sentation\n\n\nUn Job Map-Reduce se compose principalement de deux types de programmes:\n\n\n\n\nMappers\n : permettent d\u2019extraire les donne\u0301es ne\u0301cessaires sous forme de clef/valeur, pour pouvoir ensuite les trier selon la clef\n\n\nReducers\n : prennent un ensemble de donne\u0301es trie\u0301es selon leur clef, et effectuent le traitement ne\u0301cessaire sur ces donne\u0301es (somme, moyenne, total...)\n\n\n\n\nWordcount\n\n\nNous allons tester un programme MapReduce gr\u00e2ce \u00e0 un exemple tr\u00e8s simple, le \nWordCount\n, l'\u00e9quivalent du \nHelloWorld\n pour les applications de traitement de donn\u00e9es. Le Wordcount permet de calculer le nombre de mots dans un fichier donn\u00e9, en d\u00e9composant le calcul en deux \u00e9tapes:\n\n\n\n\nL'\u00e9tape de \nMapping\n, qui permet de d\u00e9couper le texte en mots et de d\u00e9livrer en sortie un flux textuel, o\u00f9 chaque ligne contient le mot trouv\u00e9, suivi de la valeur 1 (pour dire que le mot a \u00e9t\u00e9 trouv\u00e9 une fois)\n\n\nL'\u00e9tape de \nReducing\n, qui permet de faire la somme des 1 pour chaque mot, pour trouver le nombre total d'occurrences de ce mot dans le texte.\n\n\n\n\nCommen\u00e7ons par cr\u00e9er un projet Maven dans IntelliJ IDEA. Nous utiliserons dans notre cas JDK 1.8.\n\n\n\n\nD\u00e9finir les valeurs suivantes pour votre projet:\n\n\nGroupId\n: hadoop.mapreduce\n\n\nArtifactId\n: wordcount\n\n\nVersion\n: 1\n\n\n\n\n\n\nOuvrir le fichier \npom.xml\n, et ajouter les d\u00e9pendances suivantes pour Hadoop, HDFS et Map Reduce:\n\n\n\n\n  \ndependencies\n\n      \ndependency\n\n          \ngroupId\norg.apache.hadoop\n/groupId\n\n          \nartifactId\nhadoop-common\n/artifactId\n\n          \nversion\n2.7.2\n/version\n\n      \n/dependency\n\n      \n!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-mapreduce-client-core --\n\n      \ndependency\n\n          \ngroupId\norg.apache.hadoop\n/groupId\n\n          \nartifactId\nhadoop-mapreduce-client-core\n/artifactId\n\n          \nversion\n2.7.2\n/version\n\n      \n/dependency\n\n      \n!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-hdfs --\n\n      \ndependency\n\n          \ngroupId\norg.apache.hadoop\n/groupId\n\n          \nartifactId\nhadoop-hdfs\n/artifactId\n\n          \nversion\n2.7.2\n/version\n\n      \n/dependency\n\n      \ndependency\n\n            \ngroupId\norg.apache.hadoop\n/groupId\n\n            \nartifactId\nhadoop-mapreduce-client-common\n/artifactId\n\n            \nversion\n2.7.2\n/version\n\n        \n/dependency\n\n  \n/dependencies\n\n\n\n\n\n\n\nCr\u00e9er un package \ntn.insat.tp1\n sous le r\u00e9pertoire \nsrc/main/java\n\n\nCr\u00e9er la classe \nTokenizerMapper\n, contenant ce code:\n\n\n\n\n  \npackage\n \ntn.insat.tp1\n;\n\n\n  \nimport\n \norg.apache.hadoop.io.IntWritable\n;\n\n  \nimport\n \norg.apache.hadoop.io.Text\n;\n\n  \nimport\n \norg.apache.hadoop.mapreduce.Mapper\n;\n\n\n  \nimport\n \njava.io.IOException\n;\n\n  \nimport\n \njava.util.StringTokenizer\n;\n\n\n  \npublic\n \nclass\n \nTokenizerMapper\n\n        \nextends\n \nMapper\nObject\n,\n \nText\n,\n \nText\n,\n \nIntWritable\n{\n\n\n    \nprivate\n \nfinal\n \nstatic\n \nIntWritable\n \none\n \n=\n \nnew\n \nIntWritable\n(\n1\n);\n\n    \nprivate\n \nText\n \nword\n \n=\n \nnew\n \nText\n();\n\n\n    \npublic\n \nvoid\n \nmap\n(\nObject\n \nkey\n,\n \nText\n \nvalue\n,\n \nMapper\n.\nContext\n \ncontext\n\n    \n)\n \nthrows\n \nIOException\n,\n \nInterruptedException\n \n{\n\n        \nStringTokenizer\n \nitr\n \n=\n \nnew\n \nStringTokenizer\n(\nvalue\n.\ntoString\n());\n\n        \nwhile\n \n(\nitr\n.\nhasMoreTokens\n())\n \n{\n\n            \nword\n.\nset\n(\nitr\n.\nnextToken\n());\n\n            \ncontext\n.\nwrite\n(\nword\n,\n \none\n);\n\n        \n}\n\n    \n}\n\n  \n}\n\n\n\n\n\n\n\nCr\u00e9er la classe \nIntSumReducer\n:\n\n\n\n\npackage\n \ntn.insat.tp1\n;\n\n\n\nimport\n \norg.apache.hadoop.io.IntWritable\n;\n\n\nimport\n \norg.apache.hadoop.io.Text\n;\n\n\nimport\n \norg.apache.hadoop.mapreduce.Reducer\n;\n\n\n\nimport\n \njava.io.IOException\n;\n\n\n\npublic\n \nclass\n \nIntSumReducer\n\n        \nextends\n \nReducer\nText\n,\nIntWritable\n,\nText\n,\nIntWritable\n \n{\n\n\n    \nprivate\n \nIntWritable\n \nresult\n \n=\n \nnew\n \nIntWritable\n();\n\n\n    \npublic\n \nvoid\n \nreduce\n(\nText\n \nkey\n,\n \nIterable\nIntWritable\n \nvalues\n,\n\n                       \nContext\n \ncontext\n\n    \n)\n \nthrows\n \nIOException\n,\n \nInterruptedException\n \n{\n\n        \nint\n \nsum\n \n=\n \n0\n;\n\n        \nfor\n \n(\nIntWritable\n \nval\n \n:\n \nvalues\n)\n \n{\n\n            \nSystem\n.\nout\n.\nprintln\n(\nvalue: \n+\nval\n.\nget\n());\n\n            \nsum\n \n+=\n \nval\n.\nget\n();\n\n        \n}\n\n        \nSystem\n.\nout\n.\nprintln\n(\n--\n Sum = \n+\nsum\n);\n\n        \nresult\n.\nset\n(\nsum\n);\n\n        \ncontext\n.\nwrite\n(\nkey\n,\n \nresult\n);\n\n    \n}\n\n\n}\n\n\n\n\n\n\n\nEnfin, cr\u00e9er la classe \nWordCount\n:\n\n\n\n\npackage\n \ntn.insat.tp1\n;\n\n\n\nimport\n \norg.apache.hadoop.conf.Configuration\n;\n\n\nimport\n \norg.apache.hadoop.fs.Path\n;\n\n\nimport\n \norg.apache.hadoop.io.IntWritable\n;\n\n\nimport\n \norg.apache.hadoop.io.Text\n;\n\n\nimport\n \norg.apache.hadoop.mapreduce.Job\n;\n\n\nimport\n \norg.apache.hadoop.mapreduce.lib.input.FileInputFormat\n;\n\n\nimport\n \norg.apache.hadoop.mapreduce.lib.output.FileOutputFormat\n;\n\n\n\npublic\n \nclass\n \nWordCount\n \n{\n\n    \npublic\n \nstatic\n \nvoid\n \nmain\n(\nString\n[]\n \nargs\n)\n \nthrows\n \nException\n \n{\n\n        \nConfiguration\n \nconf\n \n=\n \nnew\n \nConfiguration\n();\n\n        \nJob\n \njob\n \n=\n \nJob\n.\ngetInstance\n(\nconf\n,\n \nword count\n);\n\n        \njob\n.\nsetJarByClass\n(\nWordCount\n.\nclass\n);\n\n        \njob\n.\nsetMapperClass\n(\nTokenizerMapper\n.\nclass\n);\n\n        \njob\n.\nsetCombinerClass\n(\nIntSumReducer\n.\nclass\n);\n\n        \njob\n.\nsetReducerClass\n(\nIntSumReducer\n.\nclass\n);\n\n        \njob\n.\nsetOutputKeyClass\n(\nText\n.\nclass\n);\n\n        \njob\n.\nsetOutputValueClass\n(\nIntWritable\n.\nclass\n);\n\n        \nFileInputFormat\n.\naddInputPath\n(\njob\n,\n \nnew\n \nPath\n(\nargs\n[\n0\n]));\n\n        \nFileOutputFormat\n.\nsetOutputPath\n(\njob\n,\n \nnew\n \nPath\n(\nargs\n[\n1\n]));\n\n        \nSystem\n.\nexit\n(\njob\n.\nwaitForCompletion\n(\ntrue\n)\n \n?\n \n0\n \n:\n \n1\n);\n\n    \n}\n\n\n}\n\n\n\n\n\nTester Map Reduce en local\n\n\nDans votre projet sur IntelliJ:\n\n\n\n\nCr\u00e9er un r\u00e9pertoire \ninput\n sous le r\u00e9pertoire \nresources\n de votre projet.\n\n\nCr\u00e9er un fichier de test: \nfile.txt\n dans lequel vous ins\u00e8rerez les deux lignes:\n\n\n  Hello Wordcount!\n  Hello Hadoop!\n\n\n\nCr\u00e9er une configuration de type \nApplication\n (\nRun-\nEdit Configurations...-\n+-\nApplication\n).\n\n\nD\u00e9finir comme \nMain Class\n: tn.insat.tp1.WordCount, et comme \nProgram Arguments\n: \nsrc/main/resources/input/file.txt src/main/resources/output\n\n\nLancer le programme. Un r\u00e9pertoire \noutput\n sera cr\u00e9\u00e9 dans le r\u00e9pertoire \nresources\n, contenant notamment un fichier \npart-r-00000\n, dont le contenu devrait \u00eatre le suivant:\n\n\n\n\nHadoop! 1\nHello   2\nWordcount!  1\n\n\n\n\nLancer Map Reduce sur le cluster\n\n\nDans votre projet IntelliJ:\n\n\n\n\nCr\u00e9er une configuration Maven avec la ligne de commande: \npackage install\n\n\nLancer la configuration. Un fichier \nwordcount-1.jar\n sera cr\u00e9\u00e9 dans le r\u00e9pertoire \ntarget\n du projet.\n\n\n\n\nCopier le fichier jar cr\u00e9\u00e9 dans le contenaire master. Pour cela:\n\n\n\n\nOuvrir le terminal sur le r\u00e9pertoire du projet. Cela peut \u00eatre fait avec IntelliJ en ouvrant la vue \nTerminal\n situ\u00e9e en bas \u00e0 gauche de la fen\u00eatre principale.\n\n\n\n\nTaper la commande suivante:\n\n\n  docker cp target/wordcount-1.jar hadoop-master:/root/wordcount-1.jar\n\n\n\n\n\n\n\n\n\nRevenir au shell du contenaire master, et lancer le job map reduce avec cette commande:\n\n\n\n\n\n\n  hadoop jar wordcount-1.jar tn.insat.tp1.WordCount input output\n\n\n\n\nLe Job sera lanc\u00e9 sur le fichier \npurchases.txt\n que vous aviez pr\u00e9alablement charg\u00e9 dans le r\u00e9pertoire \ninput\n de HDFS. Une fois le Job termin\u00e9, un r\u00e9pertoire \noutput\n sera cr\u00e9\u00e9. Si tout se passe bien, vous obtiendrez un affichage ressemblant au suivant:\n\n\n\n\nEn affichant les derni\u00e8res lignes du fichier g\u00e9n\u00e9r\u00e9 \noutput/part-r-00000\n, avec \nhadoop fs -tail output/part-r-00000\n, vous obtiendrez l'affichage suivant:\n\n\n\n\nIl vous est possible de monitorer vos Jobs Map Reduce, en allant \u00e0 la page: \nhttp://localhost:8088\n. Vous trouverez votre Job dans la liste des applications comme suit:\n\n\n\n\nIl est \u00e9galement possible de voir le comportement des noeuds esclaves, en allant \u00e0 l'adresse: \nhttp://localhost:8041\n pour \nslave1\n, et \nhttp://localhost:8042\n pour \nslave2\n. Vous obtiendrez ce qui suit:\n\n\n\n\n\n\nApplication\n\n\n\u00c9crire un Job Map Reduce permettant, \u00e0 partir du fichier purchases initial, de de\u0301terminer le total des ventes par magasin. La structure du fichier purchases est de la forme suivante:\n\n\n  date   temps   magasin   produit   cout   paiement\n\n\nVeiller \u00e0 toujours tester votre code en local avant de lancer un job sur le cluster!\n\n\n\n\nHomework\n\n\nPour la s\u00e9ance prochaine, l'objectif est d'utiliser un cluster AWS-EMR (Elastic Map Reduce) de Amazon pour ex\u00e9cuter un Job Map Reduce de votre choix sur un vrai cluster distribu\u00e9. Pour cela, utiliser les comptes \nRosettaHub\n qui vous ont \u00e9t\u00e9 fournis.", 
            "title": "TP1"
        }, 
        {
            "location": "/tp1/#tp1-le-traitement-batch-avec-hadoop-hdfs-et-map-reduce", 
            "text": "", 
            "title": "TP1 - Le traitement Batch avec Hadoop HDFS et Map Reduce"
        }, 
        {
            "location": "/tp1/#telecharger-pdf", 
            "text": "", 
            "title": "T\u00e9l\u00e9charger PDF"
        }, 
        {
            "location": "/tp1/#objectifs-du-tp", 
            "text": "Initiation au framework hadoop et au patron MapReduce, utilisation de docker pour lancer un cluster hadoop de 3 noeuds.", 
            "title": "Objectifs du TP"
        }, 
        {
            "location": "/tp1/#outils-et-versions", 
            "text": "Apache Hadoop  Version: 2.7.2.  Docker  Version 17.09.1  IntelliJ IDEA  Version Ultimate 2016.1 (ou tout autre IDE de votre choix)  Java  Version 1.8.  Unix-like ou Unix-based Systems (Divers Linux et MacOS)", 
            "title": "Outils et Versions"
        }, 
        {
            "location": "/tp1/#hadoop", 
            "text": "", 
            "title": "Hadoop"
        }, 
        {
            "location": "/tp1/#presentation", 
            "text": "Apache Hadoop  est un framework open-source pour stocker et traiter les donne\u0301es volumineuses sur un cluster. Il est utilise\u0301 par un grand nombre de contributeurs et utilisateurs. Il a une licence Apache 2.0.", 
            "title": "Pr\u00e9sentation"
        }, 
        {
            "location": "/tp1/#hadoop-et-docker", 
            "text": "Pour d\u00e9ployer le framework Hadoop, nous allons utiliser des contenaires  Docker . L'utilisation des contenaires va garantir la consistance entre les environnements de d\u00e9veloppement et permettra de r\u00e9duire consid\u00e9rablement la complexit\u00e9 de configuration des machines (dans le cas d'un acc\u00e8s natif) ainsi que la lourdeur d'ex\u00e9cution (si on opte pour l'utilisation d'une machine virtuelle).  Nous avons pour le d\u00e9ploiement des ressources de ce TP suivi les instructions pr\u00e9sent\u00e9es  ici .", 
            "title": "Hadoop et Docker"
        }, 
        {
            "location": "/tp1/#installation", 
            "text": "Nous allons utiliser tout au long de ce TP trois contenaires repr\u00e9sentant respectivement un noeud ma\u00eetre (Namenode) et deux noeuds esclaves (Datanodes).  Vous devez pour cela avoir install\u00e9 docker sur votre machine, et l'avoir correctement configur\u00e9. Ouvrir la ligne de commande, et taper les instructions suivantes:   Faire un pull de l'image docker:    sudo docker pull kiwenlau/hadoop:1.0  Cloner le repo github contenant les fichiers n\u00e9cessaires pour le lancement des contenaires et leur configuration:    git clone https://github.com/liliasfaxi/hadoop-cluster-docker  Cr\u00e9er le r\u00e9seau Hadoop:    sudo docker network create --driver = bridge hadoop  D\u00e9marrer les trois contenaires:     cd  hadoop-cluster-docker\n  sudo ./start-container.sh    Attention  Le script  start-container.sh  va r\u00e9initialiser les trois contenaires. Si vous voulez red\u00e9marrer un contenaire d\u00e9j\u00e0 cr\u00e9\u00e9, il ne faut pas l'ex\u00e9cuter de nouveau: tout sera effac\u00e9. Au lieu de cela, taper simplement (pour le master container, par exemple):  docker exec -it hadoop-master bash   Le r\u00e9sultat de cette ex\u00e9cution sera le suivant:    start hadoop-master container...\n  start hadoop-slave1 container...\n  start hadoop-slave2 container...\n  root@hadoop-master:~#  Vous vous retrouverez dans le shell du namenode, et vous pourrez ainsi manipuler le cluster \u00e0 votre guise. La premi\u00e8re chose \u00e0 faire, une fois dans le contenaire, est de lancer hadoop et yarn. Un script est fourni pour cela, appel\u00e9  start-hadoop.sh . Lancer ce script.    ./start-hadoop.sh  Le r\u00e9sultat devra ressembler \u00e0 ce qui suit:", 
            "title": "Installation"
        }, 
        {
            "location": "/tp1/#premiers-pas-avec-hadoop", 
            "text": "Toutes les commandes interagissant avec le syste\u0300me Hadoop commencent par hadoop fs. Ensuite, les options rajoute\u0301es sont tre\u0300s largement inspire\u0301es des commandes Unix standard.   Cre\u0301er un re\u0301pertoire dans HDFS, appele\u0301  input . Pour cela, taper:    hadoop fs \u2013mkdir -p input    Erreur  Si pour une raison ou une autre, vous n'arrivez pas \u00e0 cr\u00e9er le r\u00e9pertoire  input , avec un message ressemblant \u00e0 ceci:  ls: `.': No such file or directory , veiller \u00e0 construire l'arborescence de l'utilisateur principal (root), comme suit:  hadoop fs -mkdir -p /user/root    Nous allons utiliser le fichier  purchases.txt  comme entr\u00e9e pour le traitement MapReduce. Pour copier le fichier  purchases.txt  dans HDFS sous le re\u0301pertoire  input , t\u00e9l\u00e9charger ce fichier dans la machine master: \n     wget https://s3-eu-west-1.amazonaws.com/insat.lilia.bigdata.bucket/data/purchases.txt  Charger le fichier purchases dans le r\u00e9pertoire input que vous avez cr\u00e9\u00e9: \n     hadoop fs \u2013put purchases.txt input  Pour afficher le contenu du re\u0301pertoire  input , la commande est: \n     hadoop fs \u2013ls input  Pour afficher les derni\u00e8res lignes du fichier purchases: \n     hadoop fs -tail input/purchases.txt   Le r\u00e9sultat suivant va donc s'afficher: \n      Nous pr\u00e9sentons dans le tableau suivant les commandes les plus utilis\u00e9es pour manipuler les fichiers dans HDFS:     Instruction  Fonctionnalit\u00e9      hadoop fs \u2013ls  Afficher le contenu du re\u0301pertoire racine    hadoop fs \u2013put file.txt  Upload un fichier dans hadoop (a\u0300 partir du re\u0301pertoire courant linux)    hadoop fs \u2013get file.txt  Download un fichier a\u0300 partir de hadoop sur votre disque local    hadoop fs \u2013tail file.txt  Lire les dernie\u0300res lignes du fichier    hadoop fs \u2013cat file.txt  Affiche tout le contenu du fichier    hadoop fs \u2013mv file.txt newfile.txt  Renommer le fichier    hadoop fs \u2013rm newfile.txt  Supprimer le fichier    hadoop fs \u2013mkdir myinput  Cre\u0301er un re\u0301pertoire    hadoop fs \u2013cat file.txt \\| less  Lire le fichier page par page", 
            "title": "Premiers pas avec Hadoop"
        }, 
        {
            "location": "/tp1/#interfaces-web-pour-hadoop", 
            "text": "Hadoop offre plusieurs interfaces web pour pouvoir observer le comportement de ses diff\u00e9rentes composantes. Vous pouvez afficher ces pages en local sur votre machine gr\u00e2ce \u00e0 l'option -p de la commande  docker run . En effet, cette option permet de publier un port du contenaire sur la machine h\u00f4te. Pour pouvoir publier tous les ports expos\u00e9s, vous pouvez lancer votre contenaire en utilisant l'option  -P .  En regardant le contenu du fichier  start-container.sh  fourni dans le projet, vous verrez que deux ports de la machine ma\u00eetre ont \u00e9t\u00e9 expos\u00e9s:   Le port  50070 : qui permet d'afficher les informations de votre namenode.  Le port  8088 : qui permet d'afficher les informations du resource manager de Yarn et visualiser le comportement des diff\u00e9rents jobs.   Une fois votre cluster lanc\u00e9 et pr\u00eat \u00e0 l'emploi, vous pouvez, sur votre navigateur pr\u00e9f\u00e9r\u00e9 de votre machine h\u00f4te, aller \u00e0 :  http://localhost:50070 . Vous obtiendrez le r\u00e9sultat suivant:   Vous pouvez \u00e9galement visualiser l'avancement et les r\u00e9sultats de vos Jobs (Map Reduce ou autre) en allant \u00e0 l'adresse:  http://localhost:8088", 
            "title": "Interfaces web pour Hadoop"
        }, 
        {
            "location": "/tp1/#map-reduce", 
            "text": "", 
            "title": "Map Reduce"
        }, 
        {
            "location": "/tp1/#presentation_1", 
            "text": "Un Job Map-Reduce se compose principalement de deux types de programmes:   Mappers  : permettent d\u2019extraire les donne\u0301es ne\u0301cessaires sous forme de clef/valeur, pour pouvoir ensuite les trier selon la clef  Reducers  : prennent un ensemble de donne\u0301es trie\u0301es selon leur clef, et effectuent le traitement ne\u0301cessaire sur ces donne\u0301es (somme, moyenne, total...)", 
            "title": "Pr\u00e9sentation"
        }, 
        {
            "location": "/tp1/#wordcount", 
            "text": "Nous allons tester un programme MapReduce gr\u00e2ce \u00e0 un exemple tr\u00e8s simple, le  WordCount , l'\u00e9quivalent du  HelloWorld  pour les applications de traitement de donn\u00e9es. Le Wordcount permet de calculer le nombre de mots dans un fichier donn\u00e9, en d\u00e9composant le calcul en deux \u00e9tapes:   L'\u00e9tape de  Mapping , qui permet de d\u00e9couper le texte en mots et de d\u00e9livrer en sortie un flux textuel, o\u00f9 chaque ligne contient le mot trouv\u00e9, suivi de la valeur 1 (pour dire que le mot a \u00e9t\u00e9 trouv\u00e9 une fois)  L'\u00e9tape de  Reducing , qui permet de faire la somme des 1 pour chaque mot, pour trouver le nombre total d'occurrences de ce mot dans le texte.   Commen\u00e7ons par cr\u00e9er un projet Maven dans IntelliJ IDEA. Nous utiliserons dans notre cas JDK 1.8.   D\u00e9finir les valeurs suivantes pour votre projet:  GroupId : hadoop.mapreduce  ArtifactId : wordcount  Version : 1    Ouvrir le fichier  pom.xml , et ajouter les d\u00e9pendances suivantes pour Hadoop, HDFS et Map Reduce:      dependencies \n       dependency \n           groupId org.apache.hadoop /groupId \n           artifactId hadoop-common /artifactId \n           version 2.7.2 /version \n       /dependency \n       !-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-mapreduce-client-core -- \n       dependency \n           groupId org.apache.hadoop /groupId \n           artifactId hadoop-mapreduce-client-core /artifactId \n           version 2.7.2 /version \n       /dependency \n       !-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-hdfs -- \n       dependency \n           groupId org.apache.hadoop /groupId \n           artifactId hadoop-hdfs /artifactId \n           version 2.7.2 /version \n       /dependency \n       dependency \n             groupId org.apache.hadoop /groupId \n             artifactId hadoop-mapreduce-client-common /artifactId \n             version 2.7.2 /version \n         /dependency \n   /dependencies    Cr\u00e9er un package  tn.insat.tp1  sous le r\u00e9pertoire  src/main/java  Cr\u00e9er la classe  TokenizerMapper , contenant ce code:      package   tn.insat.tp1 ; \n\n   import   org.apache.hadoop.io.IntWritable ; \n   import   org.apache.hadoop.io.Text ; \n   import   org.apache.hadoop.mapreduce.Mapper ; \n\n   import   java.io.IOException ; \n   import   java.util.StringTokenizer ; \n\n   public   class   TokenizerMapper \n         extends   Mapper Object ,   Text ,   Text ,   IntWritable { \n\n     private   final   static   IntWritable   one   =   new   IntWritable ( 1 ); \n     private   Text   word   =   new   Text (); \n\n     public   void   map ( Object   key ,   Text   value ,   Mapper . Context   context \n     )   throws   IOException ,   InterruptedException   { \n         StringTokenizer   itr   =   new   StringTokenizer ( value . toString ()); \n         while   ( itr . hasMoreTokens ())   { \n             word . set ( itr . nextToken ()); \n             context . write ( word ,   one ); \n         } \n     } \n   }    Cr\u00e9er la classe  IntSumReducer :   package   tn.insat.tp1 ;  import   org.apache.hadoop.io.IntWritable ;  import   org.apache.hadoop.io.Text ;  import   org.apache.hadoop.mapreduce.Reducer ;  import   java.io.IOException ;  public   class   IntSumReducer \n         extends   Reducer Text , IntWritable , Text , IntWritable   { \n\n     private   IntWritable   result   =   new   IntWritable (); \n\n     public   void   reduce ( Text   key ,   Iterable IntWritable   values , \n                        Context   context \n     )   throws   IOException ,   InterruptedException   { \n         int   sum   =   0 ; \n         for   ( IntWritable   val   :   values )   { \n             System . out . println ( value:  + val . get ()); \n             sum   +=   val . get (); \n         } \n         System . out . println ( --  Sum =  + sum ); \n         result . set ( sum ); \n         context . write ( key ,   result ); \n     }  }    Enfin, cr\u00e9er la classe  WordCount :   package   tn.insat.tp1 ;  import   org.apache.hadoop.conf.Configuration ;  import   org.apache.hadoop.fs.Path ;  import   org.apache.hadoop.io.IntWritable ;  import   org.apache.hadoop.io.Text ;  import   org.apache.hadoop.mapreduce.Job ;  import   org.apache.hadoop.mapreduce.lib.input.FileInputFormat ;  import   org.apache.hadoop.mapreduce.lib.output.FileOutputFormat ;  public   class   WordCount   { \n     public   static   void   main ( String []   args )   throws   Exception   { \n         Configuration   conf   =   new   Configuration (); \n         Job   job   =   Job . getInstance ( conf ,   word count ); \n         job . setJarByClass ( WordCount . class ); \n         job . setMapperClass ( TokenizerMapper . class ); \n         job . setCombinerClass ( IntSumReducer . class ); \n         job . setReducerClass ( IntSumReducer . class ); \n         job . setOutputKeyClass ( Text . class ); \n         job . setOutputValueClass ( IntWritable . class ); \n         FileInputFormat . addInputPath ( job ,   new   Path ( args [ 0 ])); \n         FileOutputFormat . setOutputPath ( job ,   new   Path ( args [ 1 ])); \n         System . exit ( job . waitForCompletion ( true )   ?   0   :   1 ); \n     }  }", 
            "title": "Wordcount"
        }, 
        {
            "location": "/tp1/#tester-map-reduce-en-local", 
            "text": "Dans votre projet sur IntelliJ:   Cr\u00e9er un r\u00e9pertoire  input  sous le r\u00e9pertoire  resources  de votre projet.  Cr\u00e9er un fichier de test:  file.txt  dans lequel vous ins\u00e8rerez les deux lignes:    Hello Wordcount!\n  Hello Hadoop!  Cr\u00e9er une configuration de type  Application  ( Run- Edit Configurations...- +- Application ).  D\u00e9finir comme  Main Class : tn.insat.tp1.WordCount, et comme  Program Arguments :  src/main/resources/input/file.txt src/main/resources/output  Lancer le programme. Un r\u00e9pertoire  output  sera cr\u00e9\u00e9 dans le r\u00e9pertoire  resources , contenant notamment un fichier  part-r-00000 , dont le contenu devrait \u00eatre le suivant:   Hadoop! 1\nHello   2\nWordcount!  1", 
            "title": "Tester Map Reduce en local"
        }, 
        {
            "location": "/tp1/#lancer-map-reduce-sur-le-cluster", 
            "text": "Dans votre projet IntelliJ:   Cr\u00e9er une configuration Maven avec la ligne de commande:  package install  Lancer la configuration. Un fichier  wordcount-1.jar  sera cr\u00e9\u00e9 dans le r\u00e9pertoire  target  du projet.   Copier le fichier jar cr\u00e9\u00e9 dans le contenaire master. Pour cela:   Ouvrir le terminal sur le r\u00e9pertoire du projet. Cela peut \u00eatre fait avec IntelliJ en ouvrant la vue  Terminal  situ\u00e9e en bas \u00e0 gauche de la fen\u00eatre principale.   Taper la commande suivante:    docker cp target/wordcount-1.jar hadoop-master:/root/wordcount-1.jar     Revenir au shell du contenaire master, et lancer le job map reduce avec cette commande:      hadoop jar wordcount-1.jar tn.insat.tp1.WordCount input output  Le Job sera lanc\u00e9 sur le fichier  purchases.txt  que vous aviez pr\u00e9alablement charg\u00e9 dans le r\u00e9pertoire  input  de HDFS. Une fois le Job termin\u00e9, un r\u00e9pertoire  output  sera cr\u00e9\u00e9. Si tout se passe bien, vous obtiendrez un affichage ressemblant au suivant:   En affichant les derni\u00e8res lignes du fichier g\u00e9n\u00e9r\u00e9  output/part-r-00000 , avec  hadoop fs -tail output/part-r-00000 , vous obtiendrez l'affichage suivant:   Il vous est possible de monitorer vos Jobs Map Reduce, en allant \u00e0 la page:  http://localhost:8088 . Vous trouverez votre Job dans la liste des applications comme suit:   Il est \u00e9galement possible de voir le comportement des noeuds esclaves, en allant \u00e0 l'adresse:  http://localhost:8041  pour  slave1 , et  http://localhost:8042  pour  slave2 . Vous obtiendrez ce qui suit:    Application  \u00c9crire un Job Map Reduce permettant, \u00e0 partir du fichier purchases initial, de de\u0301terminer le total des ventes par magasin. La structure du fichier purchases est de la forme suivante:    date   temps   magasin   produit   cout   paiement \nVeiller \u00e0 toujours tester votre code en local avant de lancer un job sur le cluster!", 
            "title": "Lancer Map Reduce sur le cluster"
        }, 
        {
            "location": "/tp1/#homework", 
            "text": "Pour la s\u00e9ance prochaine, l'objectif est d'utiliser un cluster AWS-EMR (Elastic Map Reduce) de Amazon pour ex\u00e9cuter un Job Map Reduce de votre choix sur un vrai cluster distribu\u00e9. Pour cela, utiliser les comptes  RosettaHub  qui vous ont \u00e9t\u00e9 fournis.", 
            "title": "Homework"
        }, 
        {
            "location": "/tp2/", 
            "text": "TP2 - Traitement en Streaming avec Flink\n\n\n\n\nT\u00e9l\u00e9charger PDF\n\n\n\n\nObjectifs du TP\n\n\nCr\u00e9ation d'un processus m\u00e9tier (Business Process) en utilisant Camunda.\n\n\nOutils et Versions\n\n\n\n\nCamunda\n Version: 7.7.0\n\n\nJava\n Version 1.8.0_121 (7+ needed).\n\n\nIntelliJ IDEA\n Version Ultimate 2016.1 (ou tout autre IDE de votre choix)\n\n\nCamunda Modeler\n Version 1.10.0", 
            "title": "TP2"
        }, 
        {
            "location": "/tp2/#tp2-traitement-en-streaming-avec-flink", 
            "text": "", 
            "title": "TP2 - Traitement en Streaming avec Flink"
        }, 
        {
            "location": "/tp2/#telecharger-pdf", 
            "text": "", 
            "title": "T\u00e9l\u00e9charger PDF"
        }, 
        {
            "location": "/tp2/#objectifs-du-tp", 
            "text": "Cr\u00e9ation d'un processus m\u00e9tier (Business Process) en utilisant Camunda.", 
            "title": "Objectifs du TP"
        }, 
        {
            "location": "/tp2/#outils-et-versions", 
            "text": "Camunda  Version: 7.7.0  Java  Version 1.8.0_121 (7+ needed).  IntelliJ IDEA  Version Ultimate 2016.1 (ou tout autre IDE de votre choix)  Camunda Modeler  Version 1.10.0", 
            "title": "Outils et Versions"
        }, 
        {
            "location": "/tp3/", 
            "text": "TP3 - La Collecte de donn\u00e9es avec le Bus Kafka\n\n\n\n\nT\u00e9l\u00e9charger PDF\n\n\n\n\nObjectifs du TP\n\n\n\n\nRoutage, m\u00e9diation et transformation avec Talend ESB.\n\n\nGestion du failover et r\u00e9partition de charges, monitoring et authentification avec Talend ESB.\n\n\n\n\nOutils et Versions\n\n\n\n\nTalend Open Studio for ESB\n Version: 6.3.0\n\n\nJava\n Version 1.8.0_121", 
            "title": "TP3"
        }, 
        {
            "location": "/tp3/#tp3-la-collecte-de-donnees-avec-le-bus-kafka", 
            "text": "", 
            "title": "TP3 - La Collecte de donn\u00e9es avec le Bus Kafka"
        }, 
        {
            "location": "/tp3/#telecharger-pdf", 
            "text": "", 
            "title": "T\u00e9l\u00e9charger PDF"
        }, 
        {
            "location": "/tp3/#objectifs-du-tp", 
            "text": "Routage, m\u00e9diation et transformation avec Talend ESB.  Gestion du failover et r\u00e9partition de charges, monitoring et authentification avec Talend ESB.", 
            "title": "Objectifs du TP"
        }, 
        {
            "location": "/tp3/#outils-et-versions", 
            "text": "Talend Open Studio for ESB  Version: 6.3.0  Java  Version 1.8.0_121", 
            "title": "Outils et Versions"
        }, 
        {
            "location": "/tp4/", 
            "text": "TP4 - TBD", 
            "title": "TP4"
        }, 
        {
            "location": "/tp4/#tp4-tbd", 
            "text": "", 
            "title": "TP4 - TBD"
        }, 
        {
            "location": "/tp5/", 
            "text": "TP5 - Mise en place d'une architecture Lambda", 
            "title": "TP5"
        }, 
        {
            "location": "/tp5/#tp5-mise-en-place-dune-architecture-lambda", 
            "text": "", 
            "title": "TP5 - Mise en place d'une architecture Lambda"
        }
    ]
}