{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"GL4 - INSAT \u00b6 Cours Disponible ici: http://liliasfaxi.wixsite.com/liliasfaxi/big-data Google Classroom : https://classroom.google.com/c/NjI3ODUwNjEyNzYz Repo Github : https://github.com/INSATunisia/TP-BigData \u00b6 Overview \u00b6 Vous trouverez sur mon site officiel le cours Big Data associ\u00e9 \u00e0 ces TPs. L'objectif de ce cours est d'initier les \u00e9tudiants aux architectures Big Data: Les notions, technologies, patrons et bonnes pratiques. Le cours comportera les chapitres suivants: Introduction au Big Data Hadoop et Map Reduce Traitement de donn\u00e9es (Batch, Streaming, Temps R\u00e9el, etc.) Bases de donn\u00e9es NoSQL Putting it all together Ce cours comporte quatre s\u00e9ances de travaux pratiques: TP1: Hadoop et Map Reduce TP2: Apache Spark TP3: Apache Kafka TP4: HBase Lilia SFAXI Ce(tte) \u0153uvre est mise \u00e0 disposition selon les termes de la Licence Creative Commons Attribution - Pas d\u2019Utilisation Commerciale - Partage dans les M\u00eames Conditions 4.0 International .","title":"Travaux Pratiques Big Data"},{"location":"#gl4-insat","text":"Cours Disponible ici: http://liliasfaxi.wixsite.com/liliasfaxi/big-data Google Classroom : https://classroom.google.com/c/NjI3ODUwNjEyNzYz Repo Github : https://github.com/INSATunisia/TP-BigData","title":"GL4 - INSAT"},{"location":"#_1","text":"","title":""},{"location":"#overview","text":"Vous trouverez sur mon site officiel le cours Big Data associ\u00e9 \u00e0 ces TPs. L'objectif de ce cours est d'initier les \u00e9tudiants aux architectures Big Data: Les notions, technologies, patrons et bonnes pratiques. Le cours comportera les chapitres suivants: Introduction au Big Data Hadoop et Map Reduce Traitement de donn\u00e9es (Batch, Streaming, Temps R\u00e9el, etc.) Bases de donn\u00e9es NoSQL Putting it all together Ce cours comporte quatre s\u00e9ances de travaux pratiques: TP1: Hadoop et Map Reduce TP2: Apache Spark TP3: Apache Kafka TP4: HBase Lilia SFAXI Ce(tte) \u0153uvre est mise \u00e0 disposition selon les termes de la Licence Creative Commons Attribution - Pas d\u2019Utilisation Commerciale - Partage dans les M\u00eames Conditions 4.0 International .","title":"Overview"},{"location":"tp1/","text":"T\u00e9l\u00e9charger PDF \u00b6 Objectifs du TP \u00b6 Initiation au framework hadoop et au patron MapReduce, utilisation de docker pour lancer un cluster hadoop de 3 noeuds. Outils et Versions \u00b6 Apache Hadoop Version: 3.3.6. Docker Version latest Visual Studio Code Version 1.85.1 (ou tout autre IDE de votre choix) Java Version 1.8. Unix-like ou Unix-based Systems (Divers Linux et MacOS) Hadoop \u00b6 Pr\u00e9sentation \u00b6 Apache Hadoop est un framework open-source pour stocker et traiter les donne\u0301es volumineuses sur un cluster. Il est utilise\u0301 par un grand nombre de contributeurs et utilisateurs. Il a une licence Apache 2.0. Hadoop et Docker \u00b6 Pour d\u00e9ployer le framework Hadoop, nous allons utiliser des contenaires Docker . L'utilisation des contenaires va garantir la consistance entre les environnements de d\u00e9veloppement et permettra de r\u00e9duire consid\u00e9rablement la complexit\u00e9 de configuration des machines (dans le cas d'un acc\u00e8s natif) ainsi que la lourdeur d'ex\u00e9cution (si on opte pour l'utilisation d'une machine virtuelle). Nous avons pour le d\u00e9ploiement des ressources de ce TP suivi les instructions pr\u00e9sent\u00e9es ici . Installation \u00b6 Nous allons utiliser tout au long de ces TP trois contenaires repr\u00e9sentant respectivement un noeud ma\u00eetre (Namenode) et deux noeuds workers (Datanodes). Vous devez pour cela avoir install\u00e9 docker sur votre machine, et l'avoir correctement configur\u00e9. Ouvrir la ligne de commande, et taper les instructions suivantes: T\u00e9l\u00e9charger l'image docker upload\u00e9e sur dockerhub: docker pull liliasfaxi/hadoop-cluster:latest Cr\u00e9er les trois contenaires \u00e0 partir de l'image t\u00e9l\u00e9charg\u00e9e. Pour cela: 2.1. Cr\u00e9er un r\u00e9seau qui permettra de relier les trois contenaires: docker network create --driver = bridge hadoop 2.2. Cr\u00e9er et lancer les trois contenaires (les instructions -p permettent de faire un mapping entre les ports de la machine h\u00f4te et ceux du contenaire): docker run -itd --net = hadoop -p 9870 :9870 -p 8088 :8088 -p 7077 :7077 -p 16010 :16010 --name hadoop-master --hostname hadoop-master liliasfaxi/hadoop-cluster:latest docker run -itd -p 8040 :8042 --net = hadoop --name hadoop-worker1 --hostname hadoop-worker1 liliasfaxi/hadoop-cluster:latest docker run -itd -p 8041 :8042 --net = hadoop --name hadoop-worker2 --hostname hadoop-worker2 liliasfaxi/hadoop-cluster:latest 2.3. V\u00e9rifier que les trois contenaires tournent bien en lan\u00e7ant la commande docker ps . Un r\u00e9sultat semblable au suivant devra s'afficher: Entrer dans le contenaire master pour commencer \u00e0 l'utiliser. docker exec -it hadoop-master bash Le r\u00e9sultat de cette ex\u00e9cution sera le suivant: root@hadoop-master:~# Vous vous retrouverez dans le shell du namenode, et vous pourrez ainsi manipuler le cluster \u00e0 votre guise. La premi\u00e8re chose \u00e0 faire, une fois dans le contenaire, est de lancer hadoop et yarn. Un script est fourni pour cela, appel\u00e9 start-hadoop.sh . Lancer ce script. ./start-hadoop.sh Le r\u00e9sultat devra ressembler \u00e0 ce qui suit: Premiers pas avec Hadoop \u00b6 Toutes les commandes interagissant avec le syste\u0300me HDFS commencent par hdfs dfs . Ensuite, les options rajoute\u0301es sont tre\u0300s largement inspire\u0301es des commandes Unix standard. Cre\u0301er un re\u0301pertoire dans HDFS, appele\u0301 input . Pour cela, taper: hdfs dfs \u2013mkdir -p input En cas d'erreur: No such file or directory Si pour une raison ou une autre, vous n'arrivez pas \u00e0 cr\u00e9er le r\u00e9pertoire input , avec un message ressemblant \u00e0 ceci: ls: `.': No such file or directory , veiller \u00e0 construire l'arborescence de l'utilisateur principal (root), comme suit: hdfs dfs -mkdir -p /user/root Nous allons utiliser le fichier purchases.txt comme entr\u00e9e pour le traitement MapReduce. Commencer par d\u00e9compresser le fichier sur votre machine, puis par le charger dans le contenaire hadoop-master avec la commande suivante: docker cp purchases.txt hadoop-master:/root/purchases.txt \u00c0 partir du contenaire master, charger le fichier purchases dans le r\u00e9pertoire input (de HDFS) que vous avez cr\u00e9\u00e9: hdfs dfs \u2013put purchases.txt input Pour afficher le contenu du re\u0301pertoire input , la commande est: hdfs dfs \u2013ls input Pour afficher les derni\u00e8res lignes du fichier purchases: hdfs dfs -tail input/purchases.txt Le r\u00e9sultat suivant va donc s'afficher: Nous pr\u00e9sentons dans le tableau suivant les commandes les plus utilis\u00e9es pour manipuler les fichiers dans HDFS: Instruction Fonctionnalit\u00e9 hdfs dfs \u2013ls Afficher le contenu du re\u0301pertoire racine hdfs dfs \u2013put file.txt Upload un fichier dans hadoop (a\u0300 partir du re\u0301pertoire courant de votre disque local) hdfs dfs \u2013get file.txt Download un fichier a\u0300 partir de hadoop sur votre disque local hdfs dfs \u2013tail file.txt Lire les dernie\u0300res lignes du fichier hdfs dfs \u2013cat file.txt Affiche tout le contenu du fichier hdfs dfs \u2013mv file.txt newfile.txt Renommer (ou d\u00e9placer) le fichier hdfs dfs \u2013rm newfile.txt Supprimer le fichier hdfs dfs \u2013mkdir myinput Cre\u0301er un re\u0301pertoire Interfaces web pour Hadoop \u00b6 Hadoop offre plusieurs interfaces web pour pouvoir observer le comportement de ses diff\u00e9rentes composantes. Il est possible d'afficher ces pages directement sur notre machine h\u00f4te, et ce gr\u00e2ce \u00e0 l'utilisation de l'option -p de la commande docker run . En effet, cette option permet de publier un port du contenaire sur la machine h\u00f4te. Pour pouvoir publier tous les ports expos\u00e9s, vous pouvez lancer votre contenaire en utilisant l'option -P . En regardant la commande docker run utilis\u00e9e plus haut, vous verrez que deux ports de la machine ma\u00eetre ont \u00e9t\u00e9 expos\u00e9s: Le port 9870 : qui permet d'afficher les informations de votre namenode. Le port 8088 : qui permet d'afficher les informations du resource manager de Yarn et visualiser le comportement des diff\u00e9rents jobs. Une fois votre cluster lanc\u00e9 et hadoop d\u00e9marr\u00e9 et pr\u00eat \u00e0 l'emploi, vous pouvez, sur votre navigateur pr\u00e9f\u00e9r\u00e9 de votre machine h\u00f4te, aller \u00e0 : http://localhost:9870 . Vous obtiendrez le r\u00e9sultat suivant: Vous pouvez \u00e9galement visualiser l'avancement et les r\u00e9sultats de vos Jobs (Map Reduce ou autre) en allant \u00e0 l'adresse: http://localhost:8088 . Map Reduce \u00b6 Pr\u00e9sentation \u00b6 Un Job Map-Reduce se compose principalement de deux types de programmes: Mappers : permettent d\u2019extraire les donne\u0301es ne\u0301cessaires sous forme de clef/valeur, pour pouvoir ensuite les trier selon la clef Reducers : prennent un ensemble de donne\u0301es trie\u0301es selon leur clef, et effectuent le traitement ne\u0301cessaire sur ces donne\u0301es (somme, moyenne, total...) Wordcount \u00b6 Nous allons tester un programme MapReduce gr\u00e2ce \u00e0 un exemple tr\u00e8s simple, le WordCount , l'\u00e9quivalent du HelloWorld pour les applications de traitement de donn\u00e9es. Le Wordcount permet de calculer le nombre de mots dans un fichier donn\u00e9, en d\u00e9composant le calcul en deux \u00e9tapes: L'\u00e9tape de Mapping , qui permet de d\u00e9couper le texte en mots et de d\u00e9livrer en sortie un flux textuel, o\u00f9 chaque ligne contient le mot trouv\u00e9, suivi de la valeur 1 (pour dire que le mot a \u00e9t\u00e9 trouv\u00e9 une fois) L'\u00e9tape de Reducing , qui permet de faire la somme des 1 pour chaque mot, pour trouver le nombre total d'occurrences de ce mot dans le texte. Commen\u00e7ons par cr\u00e9er un projet Maven dans VSCode. Nous utiliserons dans notre cas JDK 1.8 . Version de JDK Ceci n'est pas une suggestion: l'utilisation d'une autre version que 1.8 provoquera des erreurs sans fin. Hadoop est compil\u00e9 avec cette version de Java, connue pour sa stabilit\u00e9. Pour cr\u00e9er un projet Maven dans VSCode: Prenez soin d'avoir les extensions Maven for Java et Extension Pack for Java activ\u00e9es. Cr\u00e9er un nouveau r\u00e9pertoire dans lequel vous inclurez votre code. Faites un clic-droit dans la fen\u00eatre Explorer et choisir Create Maven Project . Choisir No Archetype D\u00e9finir les valeurs suivantes pour votre projet: GroupId : hadoop.mapreduce ArtifactId : wordcount Version : 1 Ouvrir le fichier pom.xml automatiquement cr\u00e9\u00e9, et ajouter les d\u00e9pendances suivantes pour Hadoop, HDFS et Map Reduce: <dependencies> <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common --> <dependency> <groupId> org.apache.hadoop </groupId> <artifactId> hadoop-common </artifactId> <version> 3.3.6 </version> </dependency> <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-mapreduce-client-core --> <dependency> <groupId> org.apache.hadoop </groupId> <artifactId> hadoop-mapreduce-client-core </artifactId> <version> 3.3.6 </version> </dependency> <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-hdfs --> <dependency> <groupId> org.apache.hadoop </groupId> <artifactId> hadoop-hdfs </artifactId> <version> 3.3.6 </version> </dependency> <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-mapreduce-client-common --> <dependency> <groupId> org.apache.hadoop </groupId> <artifactId> hadoop-mapreduce-client-common </artifactId> <version> 3.3.6 </version> </dependency> <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-mapreduce-client-jobclient --> <dependency> <groupId> org.apache.hadoop </groupId> <artifactId> hadoop-mapreduce-client-jobclient </artifactId> <version> 3.3.6 </version> </dependency> </dependencies> Cr\u00e9er un package tp1 sous le r\u00e9pertoire src/main/java/hadoop/mapreduce Cr\u00e9er la classe TokenizerMapper , contenant ce code: package hadoop.mapreduce.tp1 ; import org.apache.hadoop.io.IntWritable ; import org.apache.hadoop.io.Text ; import org.apache.hadoop.mapreduce.Mapper ; import java.io.IOException ; import java.util.StringTokenizer ; public class TokenizerMapper extends Mapper < Object , Text , Text , IntWritable > { private final static IntWritable one = new IntWritable ( 1 ); private Text word = new Text (); public void map ( Object key , Text value , Mapper . Context context ) throws IOException , InterruptedException { StringTokenizer itr = new StringTokenizer ( value . toString ()); while ( itr . hasMoreTokens ()) { word . set ( itr . nextToken ()); context . write ( word , one ); } } } Cr\u00e9er la classe IntSumReducer : package hadoop.mapreduce.tp1 ; import org.apache.hadoop.io.IntWritable ; import org.apache.hadoop.io.Text ; import org.apache.hadoop.mapreduce.Reducer ; import java.io.IOException ; public class IntSumReducer extends Reducer < Text , IntWritable , Text , IntWritable > { private IntWritable result = new IntWritable (); public void reduce ( Text key , Iterable < IntWritable > values , Context context ) throws IOException , InterruptedException { int sum = 0 ; for ( IntWritable val : values ) { System . out . println ( \"value: \" + val . get ()); sum += val . get (); } System . out . println ( \"--> Sum = \" + sum ); result . set ( sum ); context . write ( key , result ); } } Enfin, cr\u00e9er la classe WordCount : package hadoop.mapreduce.tp1 ; import org.apache.hadoop.conf.Configuration ; import org.apache.hadoop.fs.Path ; import org.apache.hadoop.io.IntWritable ; import org.apache.hadoop.io.Text ; import org.apache.hadoop.mapreduce.Job ; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat ; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat ; public class WordCount { public static void main ( String [] args ) throws Exception { Configuration conf = new Configuration (); Job job = Job . getInstance ( conf , \"word count\" ); job . setJarByClass ( WordCount . class ); job . setMapperClass ( TokenizerMapper . class ); job . setCombinerClass ( IntSumReducer . class ); job . setReducerClass ( IntSumReducer . class ); job . setOutputKeyClass ( Text . class ); job . setOutputValueClass ( IntWritable . class ); FileInputFormat . addInputPath ( job , new Path ( args [ 0 ] )); FileOutputFormat . setOutputPath ( job , new Path ( args [ 1 ] )); System . exit ( job . waitForCompletion ( true ) ? 0 : 1 ); } } Tester Map Reduce en local \u00b6 Dans votre projet sur VSCode: Cr\u00e9er un r\u00e9pertoire input sous le r\u00e9pertoire resources de votre projet. Cr\u00e9er un fichier de test: file.txt dans lequel vous ins\u00e8rerez les deux lignes: Hello Wordcount! Hello Hadoop! Nous allons maintenant d\u00e9finir des arguments \u00e0 la m\u00e9thode Main: le fichier en entr\u00e9e sur lequel Map reduce va travailler, et le r\u00e9pertoire en sortie dans lequel le r\u00e9sultat sera stock\u00e9. Pour cela: Ouvrir le fichier launch.json de votre projet (Aller \u00e0 la fen\u00eatre Run and Debug , puis cliquer sur create a launch.json file ). Ajouter la ligne suivante dans la configuration WordCount , dont la classe principale est hadoop.mapreduce.tp1.WordCount : \"args\" : [ \"wordcount/src/main/resources/input/file.txt\" , \"wordcount/src/main/resources/output\" ] Arguments Il est \u00e0 noter que, dans mon cas, le fichier launch.json a \u00e9t\u00e9 cr\u00e9\u00e9 sous le r\u00e9pertoire TP1 , c'est pour cette raison que le chemin des fichiers commence par \"wordcount\". Si vous cr\u00e9ez la configuration directement sous le r\u00e9pertoire wordcount , il faudra commencer le chemin par src . S\u00e9lectionner ensuite, dans la liste des configurations du projet, WordCount comme configuration par d\u00e9faut: Lancer le programme. Un r\u00e9pertoire output sera cr\u00e9\u00e9 dans le r\u00e9pertoire resources , contenant notamment un fichier part-r-00000 , dont le contenu devrait \u00eatre le suivant: Hadoop! 1 Hello 2 Wordcount! 1 Lancer Map Reduce sur le cluster \u00b6 Dans votre projet VSCode: Pour pouvoir encapsuler toutes les d\u00e9pendances du projet dans le fichier JAR \u00e0 exporter, ajouter le plugin suivant dans le fichier pom.xml de votre projet: <build> <plugins> <plugin> <groupId> org.apache.maven.plugins </groupId> <artifactId> maven-assembly-plugin </artifactId> <version> 3.6.0 </version> <!-- Use latest version --> <configuration> <archive> <manifest> <mainClass> hadoop.mapreduce.tp1.WordCount </mainClass> </manifest> </archive> <descriptorRefs> <descriptorRef> jar-with-dependencies </descriptorRef> </descriptorRefs> </configuration> <executions> <execution> <id> make-assembly </id> <!-- this is used for inheritance merges --> <phase> package </phase> <!-- bind to the packaging phase --> <goals> <goal> single </goal> </goals> </execution> </executions> </plugin> </plugins> </build> Aller dans l'Explorer, sous Maven, puis ouvrir le Lifecycle du projet wordcount. Cliquer sur package pour compiler et packager le projet dans un fichier JAR. Un fichier wordcount-1.0-SNAPSHOT-jar-with-dependencies.jar sera cr\u00e9\u00e9 sous le r\u00e9pertoire target du projet. Copier le fichier jar cr\u00e9\u00e9 dans le contenaire master. Pour cela: Ouvrir le terminal sur le r\u00e9pertoire du projet wordcount . Cela peut \u00eatre fait avec VSCode en allant au menu Terminal -> New Terminal . Taper la commande suivante: docker cp target/wordcount-1.0-SNAPSHOT-jar-with-dependencies.jar hadoop-master:/root/wordcount.jar Revenir au shell du contenaire master, et lancer le job map reduce avec cette commande: hadoop jar wordcount.jar input output Le Job sera lanc\u00e9 sur le fichier purchases.txt que vous aviez pr\u00e9alablement charg\u00e9 dans le r\u00e9pertoire input de HDFS. Une fois le Job termin\u00e9, un r\u00e9pertoire output sera cr\u00e9\u00e9. Si tout se passe bien, vous obtiendrez un affichage ressemblant au suivant: En affichant les derni\u00e8res lignes du fichier g\u00e9n\u00e9r\u00e9 output/part-r-00000 , avec hdfs dfs -tail output/part-r-00000 , vous obtiendrez l'affichage suivant: Il vous est possible de monitorer vos Jobs Map Reduce, en allant \u00e0 la page: http://localhost:8088 . Vous trouverez votre Job dans la liste des applications comme suit: Il est \u00e9galement possible de voir le comportement des noeuds workers, en allant \u00e0 l'adresse: http://localhost:8041 pour worker1 , et http://localhost:8042 pour worker2 . Vous obtiendrez ce qui suit: Application \u00c9crire un Job Map Reduce permettant, \u00e0 partir du fichier purchases initial, de de\u0301terminer le total des ventes par magasin. Il est \u00e0 noter que la structure du fichier purchases est de la forme suivante: date temps magasin produit cout paiement Veiller \u00e0 toujours tester votre code en local avant de lancer un job sur le cluster! Homework \u00b6 Vous allez, pour ce cours, r\u00e9aliser un projet en trin\u00f4me ou quadrin\u00f4me, qui consiste en la construction d'une architecture Big Data supportant le streaming, le batch processing, et le dashboarding temps r\u00e9el. Pour la s\u00e9ance prochaine, vous allez commencer par mettre les premi\u00e8res pierres \u00e0 l'\u00e9difice: Choisir la source de donn\u00e9es sur laquelle vous allez travailler. Je vous invite \u00e0 consulter les datasets offerts par Kaggle par exemple, ou chercher une source de streaming tel que Twitter. R\u00e9fl\u00e9chir \u00e0 l'architecture cible. La pipeline devrait int\u00e9grer des traitements en batch, des traitements en streaming et une visualisation.","title":"TP1 - Le traitement Batch avec Hadoop HDFS et Map Reduce"},{"location":"tp1/#telecharger-pdf","text":"","title":"T\u00e9l\u00e9charger PDF"},{"location":"tp1/#objectifs-du-tp","text":"Initiation au framework hadoop et au patron MapReduce, utilisation de docker pour lancer un cluster hadoop de 3 noeuds.","title":"Objectifs du TP"},{"location":"tp1/#outils-et-versions","text":"Apache Hadoop Version: 3.3.6. Docker Version latest Visual Studio Code Version 1.85.1 (ou tout autre IDE de votre choix) Java Version 1.8. Unix-like ou Unix-based Systems (Divers Linux et MacOS)","title":"Outils et Versions"},{"location":"tp1/#hadoop","text":"","title":"Hadoop"},{"location":"tp1/#presentation","text":"Apache Hadoop est un framework open-source pour stocker et traiter les donne\u0301es volumineuses sur un cluster. Il est utilise\u0301 par un grand nombre de contributeurs et utilisateurs. Il a une licence Apache 2.0.","title":"Pr\u00e9sentation"},{"location":"tp1/#hadoop-et-docker","text":"Pour d\u00e9ployer le framework Hadoop, nous allons utiliser des contenaires Docker . L'utilisation des contenaires va garantir la consistance entre les environnements de d\u00e9veloppement et permettra de r\u00e9duire consid\u00e9rablement la complexit\u00e9 de configuration des machines (dans le cas d'un acc\u00e8s natif) ainsi que la lourdeur d'ex\u00e9cution (si on opte pour l'utilisation d'une machine virtuelle). Nous avons pour le d\u00e9ploiement des ressources de ce TP suivi les instructions pr\u00e9sent\u00e9es ici .","title":"Hadoop et Docker"},{"location":"tp1/#installation","text":"Nous allons utiliser tout au long de ces TP trois contenaires repr\u00e9sentant respectivement un noeud ma\u00eetre (Namenode) et deux noeuds workers (Datanodes). Vous devez pour cela avoir install\u00e9 docker sur votre machine, et l'avoir correctement configur\u00e9. Ouvrir la ligne de commande, et taper les instructions suivantes: T\u00e9l\u00e9charger l'image docker upload\u00e9e sur dockerhub: docker pull liliasfaxi/hadoop-cluster:latest Cr\u00e9er les trois contenaires \u00e0 partir de l'image t\u00e9l\u00e9charg\u00e9e. Pour cela: 2.1. Cr\u00e9er un r\u00e9seau qui permettra de relier les trois contenaires: docker network create --driver = bridge hadoop 2.2. Cr\u00e9er et lancer les trois contenaires (les instructions -p permettent de faire un mapping entre les ports de la machine h\u00f4te et ceux du contenaire): docker run -itd --net = hadoop -p 9870 :9870 -p 8088 :8088 -p 7077 :7077 -p 16010 :16010 --name hadoop-master --hostname hadoop-master liliasfaxi/hadoop-cluster:latest docker run -itd -p 8040 :8042 --net = hadoop --name hadoop-worker1 --hostname hadoop-worker1 liliasfaxi/hadoop-cluster:latest docker run -itd -p 8041 :8042 --net = hadoop --name hadoop-worker2 --hostname hadoop-worker2 liliasfaxi/hadoop-cluster:latest 2.3. V\u00e9rifier que les trois contenaires tournent bien en lan\u00e7ant la commande docker ps . Un r\u00e9sultat semblable au suivant devra s'afficher: Entrer dans le contenaire master pour commencer \u00e0 l'utiliser. docker exec -it hadoop-master bash Le r\u00e9sultat de cette ex\u00e9cution sera le suivant: root@hadoop-master:~# Vous vous retrouverez dans le shell du namenode, et vous pourrez ainsi manipuler le cluster \u00e0 votre guise. La premi\u00e8re chose \u00e0 faire, une fois dans le contenaire, est de lancer hadoop et yarn. Un script est fourni pour cela, appel\u00e9 start-hadoop.sh . Lancer ce script. ./start-hadoop.sh Le r\u00e9sultat devra ressembler \u00e0 ce qui suit:","title":"Installation"},{"location":"tp1/#premiers-pas-avec-hadoop","text":"Toutes les commandes interagissant avec le syste\u0300me HDFS commencent par hdfs dfs . Ensuite, les options rajoute\u0301es sont tre\u0300s largement inspire\u0301es des commandes Unix standard. Cre\u0301er un re\u0301pertoire dans HDFS, appele\u0301 input . Pour cela, taper: hdfs dfs \u2013mkdir -p input En cas d'erreur: No such file or directory Si pour une raison ou une autre, vous n'arrivez pas \u00e0 cr\u00e9er le r\u00e9pertoire input , avec un message ressemblant \u00e0 ceci: ls: `.': No such file or directory , veiller \u00e0 construire l'arborescence de l'utilisateur principal (root), comme suit: hdfs dfs -mkdir -p /user/root Nous allons utiliser le fichier purchases.txt comme entr\u00e9e pour le traitement MapReduce. Commencer par d\u00e9compresser le fichier sur votre machine, puis par le charger dans le contenaire hadoop-master avec la commande suivante: docker cp purchases.txt hadoop-master:/root/purchases.txt \u00c0 partir du contenaire master, charger le fichier purchases dans le r\u00e9pertoire input (de HDFS) que vous avez cr\u00e9\u00e9: hdfs dfs \u2013put purchases.txt input Pour afficher le contenu du re\u0301pertoire input , la commande est: hdfs dfs \u2013ls input Pour afficher les derni\u00e8res lignes du fichier purchases: hdfs dfs -tail input/purchases.txt Le r\u00e9sultat suivant va donc s'afficher: Nous pr\u00e9sentons dans le tableau suivant les commandes les plus utilis\u00e9es pour manipuler les fichiers dans HDFS: Instruction Fonctionnalit\u00e9 hdfs dfs \u2013ls Afficher le contenu du re\u0301pertoire racine hdfs dfs \u2013put file.txt Upload un fichier dans hadoop (a\u0300 partir du re\u0301pertoire courant de votre disque local) hdfs dfs \u2013get file.txt Download un fichier a\u0300 partir de hadoop sur votre disque local hdfs dfs \u2013tail file.txt Lire les dernie\u0300res lignes du fichier hdfs dfs \u2013cat file.txt Affiche tout le contenu du fichier hdfs dfs \u2013mv file.txt newfile.txt Renommer (ou d\u00e9placer) le fichier hdfs dfs \u2013rm newfile.txt Supprimer le fichier hdfs dfs \u2013mkdir myinput Cre\u0301er un re\u0301pertoire","title":"Premiers pas avec Hadoop"},{"location":"tp1/#interfaces-web-pour-hadoop","text":"Hadoop offre plusieurs interfaces web pour pouvoir observer le comportement de ses diff\u00e9rentes composantes. Il est possible d'afficher ces pages directement sur notre machine h\u00f4te, et ce gr\u00e2ce \u00e0 l'utilisation de l'option -p de la commande docker run . En effet, cette option permet de publier un port du contenaire sur la machine h\u00f4te. Pour pouvoir publier tous les ports expos\u00e9s, vous pouvez lancer votre contenaire en utilisant l'option -P . En regardant la commande docker run utilis\u00e9e plus haut, vous verrez que deux ports de la machine ma\u00eetre ont \u00e9t\u00e9 expos\u00e9s: Le port 9870 : qui permet d'afficher les informations de votre namenode. Le port 8088 : qui permet d'afficher les informations du resource manager de Yarn et visualiser le comportement des diff\u00e9rents jobs. Une fois votre cluster lanc\u00e9 et hadoop d\u00e9marr\u00e9 et pr\u00eat \u00e0 l'emploi, vous pouvez, sur votre navigateur pr\u00e9f\u00e9r\u00e9 de votre machine h\u00f4te, aller \u00e0 : http://localhost:9870 . Vous obtiendrez le r\u00e9sultat suivant: Vous pouvez \u00e9galement visualiser l'avancement et les r\u00e9sultats de vos Jobs (Map Reduce ou autre) en allant \u00e0 l'adresse: http://localhost:8088 .","title":"Interfaces web pour Hadoop"},{"location":"tp1/#map-reduce","text":"","title":"Map Reduce"},{"location":"tp1/#presentation_1","text":"Un Job Map-Reduce se compose principalement de deux types de programmes: Mappers : permettent d\u2019extraire les donne\u0301es ne\u0301cessaires sous forme de clef/valeur, pour pouvoir ensuite les trier selon la clef Reducers : prennent un ensemble de donne\u0301es trie\u0301es selon leur clef, et effectuent le traitement ne\u0301cessaire sur ces donne\u0301es (somme, moyenne, total...)","title":"Pr\u00e9sentation"},{"location":"tp1/#wordcount","text":"Nous allons tester un programme MapReduce gr\u00e2ce \u00e0 un exemple tr\u00e8s simple, le WordCount , l'\u00e9quivalent du HelloWorld pour les applications de traitement de donn\u00e9es. Le Wordcount permet de calculer le nombre de mots dans un fichier donn\u00e9, en d\u00e9composant le calcul en deux \u00e9tapes: L'\u00e9tape de Mapping , qui permet de d\u00e9couper le texte en mots et de d\u00e9livrer en sortie un flux textuel, o\u00f9 chaque ligne contient le mot trouv\u00e9, suivi de la valeur 1 (pour dire que le mot a \u00e9t\u00e9 trouv\u00e9 une fois) L'\u00e9tape de Reducing , qui permet de faire la somme des 1 pour chaque mot, pour trouver le nombre total d'occurrences de ce mot dans le texte. Commen\u00e7ons par cr\u00e9er un projet Maven dans VSCode. Nous utiliserons dans notre cas JDK 1.8 . Version de JDK Ceci n'est pas une suggestion: l'utilisation d'une autre version que 1.8 provoquera des erreurs sans fin. Hadoop est compil\u00e9 avec cette version de Java, connue pour sa stabilit\u00e9. Pour cr\u00e9er un projet Maven dans VSCode: Prenez soin d'avoir les extensions Maven for Java et Extension Pack for Java activ\u00e9es. Cr\u00e9er un nouveau r\u00e9pertoire dans lequel vous inclurez votre code. Faites un clic-droit dans la fen\u00eatre Explorer et choisir Create Maven Project . Choisir No Archetype D\u00e9finir les valeurs suivantes pour votre projet: GroupId : hadoop.mapreduce ArtifactId : wordcount Version : 1 Ouvrir le fichier pom.xml automatiquement cr\u00e9\u00e9, et ajouter les d\u00e9pendances suivantes pour Hadoop, HDFS et Map Reduce: <dependencies> <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common --> <dependency> <groupId> org.apache.hadoop </groupId> <artifactId> hadoop-common </artifactId> <version> 3.3.6 </version> </dependency> <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-mapreduce-client-core --> <dependency> <groupId> org.apache.hadoop </groupId> <artifactId> hadoop-mapreduce-client-core </artifactId> <version> 3.3.6 </version> </dependency> <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-hdfs --> <dependency> <groupId> org.apache.hadoop </groupId> <artifactId> hadoop-hdfs </artifactId> <version> 3.3.6 </version> </dependency> <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-mapreduce-client-common --> <dependency> <groupId> org.apache.hadoop </groupId> <artifactId> hadoop-mapreduce-client-common </artifactId> <version> 3.3.6 </version> </dependency> <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-mapreduce-client-jobclient --> <dependency> <groupId> org.apache.hadoop </groupId> <artifactId> hadoop-mapreduce-client-jobclient </artifactId> <version> 3.3.6 </version> </dependency> </dependencies> Cr\u00e9er un package tp1 sous le r\u00e9pertoire src/main/java/hadoop/mapreduce Cr\u00e9er la classe TokenizerMapper , contenant ce code: package hadoop.mapreduce.tp1 ; import org.apache.hadoop.io.IntWritable ; import org.apache.hadoop.io.Text ; import org.apache.hadoop.mapreduce.Mapper ; import java.io.IOException ; import java.util.StringTokenizer ; public class TokenizerMapper extends Mapper < Object , Text , Text , IntWritable > { private final static IntWritable one = new IntWritable ( 1 ); private Text word = new Text (); public void map ( Object key , Text value , Mapper . Context context ) throws IOException , InterruptedException { StringTokenizer itr = new StringTokenizer ( value . toString ()); while ( itr . hasMoreTokens ()) { word . set ( itr . nextToken ()); context . write ( word , one ); } } } Cr\u00e9er la classe IntSumReducer : package hadoop.mapreduce.tp1 ; import org.apache.hadoop.io.IntWritable ; import org.apache.hadoop.io.Text ; import org.apache.hadoop.mapreduce.Reducer ; import java.io.IOException ; public class IntSumReducer extends Reducer < Text , IntWritable , Text , IntWritable > { private IntWritable result = new IntWritable (); public void reduce ( Text key , Iterable < IntWritable > values , Context context ) throws IOException , InterruptedException { int sum = 0 ; for ( IntWritable val : values ) { System . out . println ( \"value: \" + val . get ()); sum += val . get (); } System . out . println ( \"--> Sum = \" + sum ); result . set ( sum ); context . write ( key , result ); } } Enfin, cr\u00e9er la classe WordCount : package hadoop.mapreduce.tp1 ; import org.apache.hadoop.conf.Configuration ; import org.apache.hadoop.fs.Path ; import org.apache.hadoop.io.IntWritable ; import org.apache.hadoop.io.Text ; import org.apache.hadoop.mapreduce.Job ; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat ; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat ; public class WordCount { public static void main ( String [] args ) throws Exception { Configuration conf = new Configuration (); Job job = Job . getInstance ( conf , \"word count\" ); job . setJarByClass ( WordCount . class ); job . setMapperClass ( TokenizerMapper . class ); job . setCombinerClass ( IntSumReducer . class ); job . setReducerClass ( IntSumReducer . class ); job . setOutputKeyClass ( Text . class ); job . setOutputValueClass ( IntWritable . class ); FileInputFormat . addInputPath ( job , new Path ( args [ 0 ] )); FileOutputFormat . setOutputPath ( job , new Path ( args [ 1 ] )); System . exit ( job . waitForCompletion ( true ) ? 0 : 1 ); } }","title":"Wordcount"},{"location":"tp1/#tester-map-reduce-en-local","text":"Dans votre projet sur VSCode: Cr\u00e9er un r\u00e9pertoire input sous le r\u00e9pertoire resources de votre projet. Cr\u00e9er un fichier de test: file.txt dans lequel vous ins\u00e8rerez les deux lignes: Hello Wordcount! Hello Hadoop! Nous allons maintenant d\u00e9finir des arguments \u00e0 la m\u00e9thode Main: le fichier en entr\u00e9e sur lequel Map reduce va travailler, et le r\u00e9pertoire en sortie dans lequel le r\u00e9sultat sera stock\u00e9. Pour cela: Ouvrir le fichier launch.json de votre projet (Aller \u00e0 la fen\u00eatre Run and Debug , puis cliquer sur create a launch.json file ). Ajouter la ligne suivante dans la configuration WordCount , dont la classe principale est hadoop.mapreduce.tp1.WordCount : \"args\" : [ \"wordcount/src/main/resources/input/file.txt\" , \"wordcount/src/main/resources/output\" ] Arguments Il est \u00e0 noter que, dans mon cas, le fichier launch.json a \u00e9t\u00e9 cr\u00e9\u00e9 sous le r\u00e9pertoire TP1 , c'est pour cette raison que le chemin des fichiers commence par \"wordcount\". Si vous cr\u00e9ez la configuration directement sous le r\u00e9pertoire wordcount , il faudra commencer le chemin par src . S\u00e9lectionner ensuite, dans la liste des configurations du projet, WordCount comme configuration par d\u00e9faut: Lancer le programme. Un r\u00e9pertoire output sera cr\u00e9\u00e9 dans le r\u00e9pertoire resources , contenant notamment un fichier part-r-00000 , dont le contenu devrait \u00eatre le suivant: Hadoop! 1 Hello 2 Wordcount! 1","title":"Tester Map Reduce en local"},{"location":"tp1/#lancer-map-reduce-sur-le-cluster","text":"Dans votre projet VSCode: Pour pouvoir encapsuler toutes les d\u00e9pendances du projet dans le fichier JAR \u00e0 exporter, ajouter le plugin suivant dans le fichier pom.xml de votre projet: <build> <plugins> <plugin> <groupId> org.apache.maven.plugins </groupId> <artifactId> maven-assembly-plugin </artifactId> <version> 3.6.0 </version> <!-- Use latest version --> <configuration> <archive> <manifest> <mainClass> hadoop.mapreduce.tp1.WordCount </mainClass> </manifest> </archive> <descriptorRefs> <descriptorRef> jar-with-dependencies </descriptorRef> </descriptorRefs> </configuration> <executions> <execution> <id> make-assembly </id> <!-- this is used for inheritance merges --> <phase> package </phase> <!-- bind to the packaging phase --> <goals> <goal> single </goal> </goals> </execution> </executions> </plugin> </plugins> </build> Aller dans l'Explorer, sous Maven, puis ouvrir le Lifecycle du projet wordcount. Cliquer sur package pour compiler et packager le projet dans un fichier JAR. Un fichier wordcount-1.0-SNAPSHOT-jar-with-dependencies.jar sera cr\u00e9\u00e9 sous le r\u00e9pertoire target du projet. Copier le fichier jar cr\u00e9\u00e9 dans le contenaire master. Pour cela: Ouvrir le terminal sur le r\u00e9pertoire du projet wordcount . Cela peut \u00eatre fait avec VSCode en allant au menu Terminal -> New Terminal . Taper la commande suivante: docker cp target/wordcount-1.0-SNAPSHOT-jar-with-dependencies.jar hadoop-master:/root/wordcount.jar Revenir au shell du contenaire master, et lancer le job map reduce avec cette commande: hadoop jar wordcount.jar input output Le Job sera lanc\u00e9 sur le fichier purchases.txt que vous aviez pr\u00e9alablement charg\u00e9 dans le r\u00e9pertoire input de HDFS. Une fois le Job termin\u00e9, un r\u00e9pertoire output sera cr\u00e9\u00e9. Si tout se passe bien, vous obtiendrez un affichage ressemblant au suivant: En affichant les derni\u00e8res lignes du fichier g\u00e9n\u00e9r\u00e9 output/part-r-00000 , avec hdfs dfs -tail output/part-r-00000 , vous obtiendrez l'affichage suivant: Il vous est possible de monitorer vos Jobs Map Reduce, en allant \u00e0 la page: http://localhost:8088 . Vous trouverez votre Job dans la liste des applications comme suit: Il est \u00e9galement possible de voir le comportement des noeuds workers, en allant \u00e0 l'adresse: http://localhost:8041 pour worker1 , et http://localhost:8042 pour worker2 . Vous obtiendrez ce qui suit: Application \u00c9crire un Job Map Reduce permettant, \u00e0 partir du fichier purchases initial, de de\u0301terminer le total des ventes par magasin. Il est \u00e0 noter que la structure du fichier purchases est de la forme suivante: date temps magasin produit cout paiement Veiller \u00e0 toujours tester votre code en local avant de lancer un job sur le cluster!","title":"Lancer Map Reduce sur le cluster"},{"location":"tp1/#homework","text":"Vous allez, pour ce cours, r\u00e9aliser un projet en trin\u00f4me ou quadrin\u00f4me, qui consiste en la construction d'une architecture Big Data supportant le streaming, le batch processing, et le dashboarding temps r\u00e9el. Pour la s\u00e9ance prochaine, vous allez commencer par mettre les premi\u00e8res pierres \u00e0 l'\u00e9difice: Choisir la source de donn\u00e9es sur laquelle vous allez travailler. Je vous invite \u00e0 consulter les datasets offerts par Kaggle par exemple, ou chercher une source de streaming tel que Twitter. R\u00e9fl\u00e9chir \u00e0 l'architecture cible. La pipeline devrait int\u00e9grer des traitements en batch, des traitements en streaming et une visualisation.","title":"Homework"},{"location":"tp2/","text":"T\u00e9l\u00e9charger PDF \u00b6 Objectifs du TP \u00b6 Utilisation de Spark pour r\u00e9aliser des traitements par lot et des traitements en streaming. Outils et Versions \u00b6 Apache Hadoop Version: 3.3.6 Apache Spark Version: 3.5.0 Docker Version latest Visual Studio Code Version 1.85.1 (ou tout autre IDE de votre choix) Java Version 1.8. Unix-like ou Unix-based Systems (Divers Linux et MacOS) Spark \u00b6 Pr\u00e9sentation \u00b6 Spark est un syst\u00e8me de traitement rapide et parall\u00e8le. Il fournit des APIs de haut niveau en Java, Scala, Python et R, et un moteur optimis\u00e9 qui supporte l'ex\u00e9cution des graphes. Il supporte \u00e9galement un ensemble d'outils de haut niveau tels que Spark SQL pour le support du traitement de donn\u00e9es structur\u00e9es, MLlib pour l'apprentissage des donn\u00e9es, GraphX pour le traitement des graphes, et Spark Streaming pour le traitment des donn\u00e9es en streaming. Spark et Hadoop \u00b6 Spark peut s'ex\u00e9cuter sur plusieurs plateformes: Hadoop, Mesos, en standalone ou sur le cloud. Il peut \u00e9galement acc\u00e9der \u00e0 diverses sources de donn\u00e9es, comme HDFS, Cassandra, HBase et S3. Dans ce TP, nous allons ex\u00e9cuter Spark sur Hadoop YARN. YARN s'occupera ainsi de la gestion des ressources pour le d\u00e9clenchement et l'ex\u00e9cution des Jobs Spark. Installation \u00b6 Nous avons proc\u00e9d\u00e9 \u00e0 l'installation de Spark sur le cluster Hadoop utilis\u00e9 dans le TP1 . Suivre les \u00e9tapes d\u00e9crites dans la partie Installation du TP1 pour t\u00e9l\u00e9charger l'image et ex\u00e9cuter les trois contenaires. Si cela est d\u00e9j\u00e0 fait, il suffit de lancer vos machines gr\u00e2ce aux commandes suivantes: docker start hadoop-master hadoop-worker1 hadoop-worker2 puis d'entrer dans le contenaire master: docker exec -it hadoop-master bash Lancer ensuite les d\u00e9mons yarn et hdfs: ./start-hadoop.sh Vous pourrez v\u00e9rifier que tous les d\u00e9mons sont lanc\u00e9s en tapant: jps . Un r\u00e9sultat semblable au suivant pourra \u00eatre visible: 880 Jps 257 NameNode 613 ResourceManager 456 SecondaryNameNode La m\u00eame op\u00e9ration sur les noeuds workers (auxquels vous acc\u00e9dez \u00e0 partir de votre machine h\u00f4te de la m\u00eame fa\u00e7on que le noeud ma\u00eetre, c'est \u00e0 dire en tapant par exemple docker exec -it hadoop-worker1 bash ) devrait donner: 176 NodeManager 65 DataNode 311 Jps Test de Spark avec Spark-Shell \u00b6 Dans le but de tester l'ex\u00e9cution de spark, commencer par cr\u00e9er un fichier file1.txt dans votre noeud master, contenant le texte suivant: Hello Spark Wordcount! Hello Hadoop Also :) Charger ensuite ce fichier dans HDFS: hdfs dfs -put file1.txt Erreur possible Si le message suivant s'affiche: put: `.': No such file or directory , c'est parce que l'arborescence du r\u00e9pertoire principal n'est pas cr\u00e9\u00e9e dans HDFS. Pour le faire, il suffit d'ex\u00e9cuter la commande suivante avant la commande de chargement : hadoop fs mkdir -p . Pour v\u00e9rifier que spark est bien install\u00e9, taper la commande suivante: spark-shell Vous devriez avoir un r\u00e9sultat semblable au suivant: Vous pourrez tester spark avec un code scala simple comme suit (\u00e0 ex\u00e9cuter ligne par ligne): 1 2 3 4 val lines = sc . textFile ( \"file1.txt\" ) val words = lines . flatMap ( _ . split ( \"\\\\s+\" )) val wc = words . map ( w => ( w , 1 )). reduceByKey ( _ + _ ) wc . saveAsTextFile ( \"file1.count\" ) Ce code vient de (1) charger le fichier file1.txt de HDFS, (2) s\u00e9parer les mots selon les caract\u00e8res d'espacement, (3) appliquer un map sur les mots obtenus qui produit le couple ( <mot> , 1), puis un reduce qui permet de faire la somme des 1 des mots identiques. Pour afficher le r\u00e9sultat, sortir de spark-shell en cliquant sur Ctrl-C . T\u00e9l\u00e9charger ensuite le r\u00e9pertoire file1.count cr\u00e9\u00e9 dans HDFS comme suit: hdfs dfs -get file1.count Le contenu des deux fichiers part-00000 et part-00001 ressemble \u00e0 ce qui suit: L'API de Spark \u00b6 A un haut niveau d'abstraction, chaque application Spark consiste en un programme driver qui ex\u00e9cute la fonction main de l'utilisateur et lance plusieurs op\u00e9rations parall\u00e8les sur le cluster. L'abstraction principale fournie par Spark est un RDD ( Resilient Distributed Dataset ), qui repr\u00e9sente une collection d'\u00e9l\u00e9ments partitionn\u00e9s \u00e0 travers les noeuds du cluster, et sur lesquelles on peut op\u00e9rer en parall\u00e8le. Les RDDs sont cr\u00e9\u00e9s \u00e0 partir d'un fichier dans HDFS par exemple, puis le transforment. Les utilisateurs peuvent demander \u00e0 Spark de sauvegarder un RDD en m\u00e9moire, lui permettant ainsi d'\u00eatre r\u00e9utilis\u00e9 efficacement \u00e0 travers plusieurs op\u00e9rations parall\u00e8les. Les RDDs supportent deux types d'op\u00e9rations: les transformations , qui permettent de cr\u00e9er un nouveau Dataset \u00e0 partir d'un Dataset existant les actions , qui retournent une valeur au programme driver apr\u00e8s avoir ex\u00e9cut\u00e9 un calcul sur le Dataset. Par exemple, un map est une transformation qui passe chaque \u00e9l\u00e9ment du dataset via une fonction, et retourne un nouvel RDD repr\u00e9sentant les r\u00e9sultats. Un reduce est une action qui agr\u00e8ge tous les \u00e9l\u00e9ments du RDD en utilisant une certaine fonction et retourne le r\u00e9sultat final au programme. Toutes les transformations dans Spark sont lazy , car elles ne calculent pas le r\u00e9sultat imm\u00e9diatement. Elles se souviennent des transformations appliqu\u00e9es \u00e0 un dataset de base (par ex. un fichier). Les transformations ne sont calcul\u00e9es que quand une action n\u00e9cessite qu'un r\u00e9sultat soit retourn\u00e9 au programme principal. Cela permet \u00e0 Spark de s'ex\u00e9cuter plus efficacement. Exemple \u00b6 L'exemple que nous allons pr\u00e9senter ici par \u00e9tapes permet de relever les mots les plus fr\u00e9quents dans un fichier. Pour cela, le code suivant est utilis\u00e9: //Etape 1 - Cr\u00e9er un RDD \u00e0 partir d'un fichier texte de Hadoop val docs = sc . textFile ( \"file1.txt\" ) //Etape 2 - Convertir les lignes en minuscule val lower = docs . map ( line => line . toLowerCase ) //Etape 3 - S\u00e9parer les lignes en mots val words = lower . flatMap ( line => line . split ( \"\\\\s+\" )) //Etape 4 - produire les tuples (mot, 1) val counts = words . map ( word => ( word , 1 )) //Etape 5 - Compter tous les mots val freq = counts . reduceByKey ( _ + _ ) //Etape 6 - Inverser les tuples (transformation avec swap) freq . map ( _ . swap ) //Etape 6 - Inverser les tuples (action de s\u00e9lection des 3 premiers) val top = freq . map ( _ . swap ). top ( 3 ) Spark Batch en Java \u00b6 Pr\u00e9paration de l'environnement et Code \u00b6 Nous allons dans cette partie cr\u00e9er un projet Spark Batch en Java (un simple WordCount), le charger sur le cluster et lancer le job. Cr\u00e9er un projet Maven avec VSCode, en utilisant la config suivante: <groupId> spark.batch </groupId> <artifactId> wordcount-spark </artifactId> Rajouter dans le fichier pom les d\u00e9pendances n\u00e9cessaires, et indiquer la version du compilateur Java: <properties> <maven.compiler.source> 1.8 </maven.compiler.source> <maven.compiler.target> 1.8 </maven.compiler.target> </properties> <dependencies> <dependency> <groupId> org.apache.spark </groupId> <artifactId> spark-core_2.13 </artifactId> <version> 3.5.0 </version> </dependency> <dependency> <groupId> org.slf4j </groupId> <artifactId> slf4j-reload4j </artifactId> <version> 2.1.0-alpha1 </version> <scope> test </scope> </dependency> </dependencies> Sous le r\u00e9pertoire java, cr\u00e9er un package que vous appellerez spark.batch.tp21 , et dedans, une classe appel\u00e9e WordCountTask . \u00c9crire le code suivant dans WordCountTask.java : import org.apache.spark.SparkConf ; import org.apache.spark.api.java.JavaPairRDD ; import org.apache.spark.api.java.JavaRDD ; import org.apache.spark.api.java.JavaSparkContext ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import scala.Tuple2 ; import java.util.Arrays ; import com.google.common.base.Preconditions ; public class WordCountTask { private static final Logger LOGGER = LoggerFactory . getLogger ( WordCountTask . class ); public static void main ( String [] args ) { Preconditions . checkArgument ( args . length > 1 , \"Please provide the path of input file and output dir as parameters.\" ); new WordCountTask (). run ( args [ 0 ] , args [ 1 ] ); } public void run ( String inputFilePath , String outputDir ) { String master = \"local[*]\" ; SparkConf conf = new SparkConf () . setAppName ( WordCountTask . class . getName ()) . setMaster ( master ); JavaSparkContext sc = new JavaSparkContext ( conf ); JavaRDD < String > textFile = sc . textFile ( inputFilePath ); JavaPairRDD < String , Integer > counts = textFile . flatMap ( s -> Arrays . asList ( s . split ( \" \" )). iterator ()) . mapToPair ( word -> new Tuple2 <> ( word , 1 )) . reduceByKey (( a , b ) -> a + b ); counts . saveAsTextFile ( outputDir ); } } La premi\u00e8re chose \u00e0 faire dans un programme Spark est de cr\u00e9er un objet JavaSparkContext , qui indique \u00e0 Spark comment acc\u00e9der \u00e0 un cluster. Pour cr\u00e9er ce contexte, vous aurez besoin de construire un objet SparkConf qui contient toutes les informations sur l'application. appName est le nom de l'application master est une URL d'un cluster Spark, Mesos ou YARN, ou bien une cha\u00eene sp\u00e9ciale local pour lancer le job en mode local. Warning Nous avons indiqu\u00e9 ici que notre master est local pour les besoins du test, mais plus tard, en le packageant pour le cluster, nous allons enlever cette indication. Il est en effet d\u00e9conseill\u00e9 de la hard-coder dans le programme, il faudrait plut\u00f4t l'indiquer comme option de commande \u00e0 chaque fois que nous lan\u00e7ons le job. Le reste du code de l'application est la version en Java de l'exemple en scala que nous avions fait avec spark-shell. Test du code en local \u00b6 Pour tester le code sur votre machine, proc\u00e9der aux \u00e9tapes suivantes: Ins\u00e9rer un fichier texte de votre choix (par exemple le fameux loremipsum.txt ) dans le r\u00e9pertoire src/main/resources. Lancer le programme en utilisant les arguments suivants: Arg1 : le chemin du fichier loremipsum.txt Arg2 : le chemin d'un r\u00e9pertoire out sous resources (vous ne devez pas le cr\u00e9er) Cliquer sur OK, et lancer la configuration. Si tout se passe bien, un r\u00e9pertoire out sera cr\u00e9\u00e9 sous resources , qui contient (entre autres) deux fichiers: part-00000, part-00001. Lancement du code sur le cluster \u00b6 Pour ex\u00e9cuter le code sur le cluster, modifier comme indiqu\u00e9 les lignes en jaune dans ce qui suit: public class WordCountTask { private static final Logger LOGGER = LoggerFactory . getLogger ( WordCountTask . class ); public static void main ( String [] args ) { checkArgument ( args . length > 1 , \"Please provide the path of input file and output dir as parameters.\" ); new WordCountTask (). run ( args [ 0 ] , args [ 1 ] ); } public void run ( String inputFilePath , String outputDir ) { SparkConf conf = new SparkConf () . setAppName ( WordCountTask . class . getName ()); JavaSparkContext sc = new JavaSparkContext ( conf ); JavaRDD < String > textFile = sc . textFile ( inputFilePath ); JavaPairRDD < String , Integer > counts = textFile . flatMap ( s -> Arrays . asList ( s . split ( \"\\t\" )). iterator ()) . mapToPair ( word -> new Tuple2 <> ( word , 1 )) . reduceByKey (( a , b ) -> a + b ); counts . saveAsTextFile ( outputDir ); } } Lancer ensuite une configuration de type Maven, avec la commande package . Un fichier intitul\u00e9 wordcount-spark-1.0-SNAPSHOT.jar sera cr\u00e9\u00e9 sous le r\u00e9pertoire target. Nous allons maintenant copier ce fichier dans docker. Pour cela, naviguer vers le r\u00e9pertoire du projet avec votre terminal (ou plus simplement utiliser le terminal dans VSCode), et taper la commande suivante: docker cp target/wordcount-spark-1.0-SNAPSHOT.jar hadoop-master:/root/wordcount-spark.jar Revenir \u00e0 votre contenaire master, et lancer un job Spark en utilisant ce fichier jar g\u00e9n\u00e9r\u00e9, avec la commande spark-submit , un script utilis\u00e9 pour lancer des applications spark sur un cluster. spark-submit --class spark.batch.tp21.WordCountTask --master local wordcount-spark.jar input/purchases.txt out-spark Nous allons lancer le job en mode local, pour commencer. Le fichier en entr\u00e9e est le fichier purchases.txt (que vous d\u00e9j\u00e0 charg\u00e9 dans HDFS dans le TP pr\u00e9c\u00e9dent), et le r\u00e9sultat sera stock\u00e9 dans un nouveau r\u00e9pertoire out-spark . Attention V\u00e9rifiez bien que le fichier purchases existe dans le r\u00e9pertoire input de HDFS (et que le r\u00e9pertoire out-spark n'existe pas)! Si ce n'est pas le cas, vous pouvez le charger avec les commandes suivantes: hdfs dfs -mkdir -p input hdfs dfs -put purchases.txt input Si tout se passe bien, vous devriez trouver, dans le r\u00e9pertoire out-spark , deux fichiers part-00000 et part-00001, qui ressemblent \u00e0 ce qui suit: Nous allons maintenant tester le comportement de spark-submit si on l'ex\u00e9cute en mode cluster sur YARN. Pour cela, ex\u00e9cuter le code suivant: spark-submit --class spark.batch.tp21.WordCountTask --master yarn --deploy-mode cluster wordcount-spark.jar input/purchases.txt out-spark2 En lan\u00e7ant le job sur Yarn, deux modes de d\u00e9ploiement sont possibles: Mode cluster : o\u00f9 tout le job s'ex\u00e9cute dans le cluster, c'est \u00e0 dire les Spark Executors (qui ex\u00e9cutent les vraies t\u00e2ches) et le Spark Driver (qui ordonnance les Executors). Ce dernier sera encapsul\u00e9 dans un YARN Application Master. Mode client : o\u00f9 Spark Driver s'ex\u00e9cute sur la machine cliente (tel que votre propre ordinateur portable). Si votre machine s'\u00e9teint, le job s'arr\u00eate. Ce mode est appropri\u00e9 pour les jobs interactifs. Si tout se passe bien, vous devriez obtenir un r\u00e9pertoire out-spark2 dans HDFS avec les fichiers usuels. En cas d'erreur: consulter les logs! En cas d'erreur ou d'interruption du job sur Yarn, vous pourrez consulter les fichiers logs pour chercher le message d'erreur (le message affich\u00e9 sur la console n'est pas assez explicite). Pour cela, sur votre navigateur, aller \u00e0 l'adresse: http://localhost:8041/logs/userlogs et suivez toujours les derniers liens jusqu'\u00e0 stderr . Spark Streaming \u00b6 Spark est connu pour supporter \u00e9galement le traitement des donn\u00e9es en streaming. Les donn\u00e9es peuvent \u00eatre lues \u00e0 partir de plusieurs sources tel que Kafka, Flume, Kinesis ou des sockets TCP, et peuvent \u00eatre trait\u00e9es en utilisant des algorithmes complexes. Ensuite, les donn\u00e9es trait\u00e9es peuvent \u00eatre stock\u00e9es sur des syst\u00e8mes de fichiers, des bases de donn\u00e9es ou des dashboards. Il est m\u00eame possible de r\u00e9aliser des algorithmes de machine learning et de traitement de graphes sur les flux de donn\u00e9es. En interne, il fonctionne comme suit: Spark Streaming re\u00e7oit des donn\u00e9es en streaming et les divise en micro-batches, qui sont ensuite calcul\u00e9s par le moteur de spark pour g\u00e9n\u00e9rer le flux final de r\u00e9sultats. Environnement et Code \u00b6 Nous allons commencer par tester le streaming en local, comme d'habitude. Pour cela: Commencer par cr\u00e9er un nouveau projet Maven, avec le fichier pom suivant: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <project xmlns= \"http://maven.apache.org/POM/4.0.0\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation= \"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\" > <modelVersion> 4.0.0 </modelVersion> <groupId> spark.streaming </groupId> <artifactId> stream </artifactId> <version> 1.0-SNAPSHOT </version> <properties> <maven.compiler.source> 1.8 </maven.compiler.source> <maven.compiler.target> 1.8 </maven.compiler.target> </properties> <dependencies> <dependency> <groupId> org.apache.spark </groupId> <artifactId> spark-core_2.13 </artifactId> <version> 3.5.0 </version> </dependency> <dependency> <groupId> org.apache.spark </groupId> <artifactId> spark-streaming_2.13 </artifactId> <version> 3.5.0 </version> </dependency> </dependencies> </project> Cr\u00e9er une classe spark.streaming.tp22.Stream avec le code suivant: import org.apache.spark.sql.Dataset ; import org.apache.spark.sql.Encoders ; import org.apache.spark.sql.SparkSession ; import org.apache.spark.sql.streaming.StreamingQuery ; import org.apache.spark.sql.streaming.StreamingQueryException ; import org.apache.spark.sql.streaming.Trigger ; import java.util.concurrent.TimeoutException ; import java.util.Arrays ; public class Stream { public static void main ( String [] args ) throws StreamingQueryException , TimeoutException { SparkSession spark = SparkSession . builder () . appName ( \"NetworkWordCount\" ) . master ( \"local[*]\" ) . getOrCreate (); // Create DataFrame representing the stream of input lines from connection to localhost:9999 Dataset < String > lines = spark . readStream () . format ( \"socket\" ) . option ( \"host\" , \"localhost\" ) . option ( \"port\" , 9999 ) . load () . as ( Encoders . STRING ()); // Split the lines into words Dataset < String > words = lines . flatMap ( ( String x ) -> Arrays . asList ( x . split ( \" \" )). iterator (), Encoders . STRING ()); // Generate running word count Dataset < org . apache . spark . sql . Row > wordCounts = words . groupBy ( \"value\" ). count (); // Start running the query that prints the running counts to the console StreamingQuery query = wordCounts . writeStream () . outputMode ( \"complete\" ) . format ( \"console\" ) . trigger ( Trigger . ProcessingTime ( \"1 second\" )) . start (); query . awaitTermination (); } } Ce code permet de calculer le nombre de mots dans un stream de donn\u00e9es (provenant du port localhost:9999) chaque seconde. Dans sa version actuelle, Spark encourage l'utilisation de Structured Streaming ,une API de haut niveau qui fournit un traitement plus efficace, et qui est construite au dessus de Spark SQL, en int\u00e9grant les structures DataFrame et Dataset. Trigger Interval Dans Spark Structured Streaming, le concept de microbatch est utilis\u00e9 pour traiter les donn\u00e9es en continu par petits lots incr\u00e9mentaux. La dur\u00e9e de chaque micro-lot est configurable et d\u00e9termine la fr\u00e9quence de traitement des donn\u00e9es en continu. Cette dur\u00e9e est appel\u00e9e \"intervalle de d\u00e9clenchement\". Si vous ne sp\u00e9cifiez pas explicitement d'intervalle de d\u00e9clenchement, le trigger par d\u00e9faut est ProcessingTime(0) , qui est aussi connu comme le mode de traitement par micro-lots. Ce param\u00e8tre par d\u00e9faut signifie que Spark essaiera de traiter les donn\u00e9es aussi rapidement que possible, sans d\u00e9lai fixe entre les micro-lots. Test du code en Local \u00b6 Le stream ici sera diffus\u00e9 par une petite commande utilitaire qui se trouve dans la majorit\u00e9 des syst\u00e8mes Unix-like. Ouvrir un terminal, et taper la commande suivante pour cr\u00e9er le stream: nc -lk 9999 Ex\u00e9cuter votre classe Stream . L'application est en \u00e9coute sur localhost:9999. Commencer \u00e0 \u00e9crire des messages sur la console de votre terminal (l\u00e0 o\u00f9 vous avez lanc\u00e9 la commande nc) A chaque fois que vous entrez quelque chose sur le terminal, l'application Stream l'intercepte, et l'affichage sur l'\u00e9cran de la console change, comme suit: Lancement du code sur le cluster \u00b6 Pour lancer le code pr\u00e9c\u00e9dent sur le cluster, il faudra d'abord faire une petite modification: changer la valeur localhost par l'IP de votre machine hote (celle que vous utilisez pour lancer la commande nc ). G\u00e9n\u00e9rer le fichier jar. Copier le fichier jar sur le contenaire master. Assurez-vous que la commande nc tourne bien sur votre machine, en attente de messages. Sur votre noeud master, lancer la commande suivante: spark-submit --class spark.streaming.tp22.Stream --master local stream.jar Cette fois, \u00e9norm\u00e9ment de texte est g\u00e9n\u00e9r\u00e9 en continu sur la console. Comme nous avons d\u00e9fini dans l'application console comme sortie, le r\u00e9sultat du traitement s'affichera au milieu de tout ce texte. Une fois que vous aurez saisi le texte \u00e0 tester, arr\u00eater l'application (avec Ctrl-C), et chercher dans le texte la cha\u00eene \" Batch: \". Vous trouverez normalement un r\u00e9sultat semblable au suivant: Homework \u00b6 Vous allez maintenant appliquer des traitements sur votre projet selon votre besoin. Vos contraintes ici est d'avoir les deux types de traitement: Batch et Streaming. Vous pouvez utiliser Map Reduce ou Spark pour le traitement en Batch.","title":"TP2 - Traitement par Lot et Streaming avec Spark"},{"location":"tp2/#telecharger-pdf","text":"","title":"T\u00e9l\u00e9charger PDF"},{"location":"tp2/#objectifs-du-tp","text":"Utilisation de Spark pour r\u00e9aliser des traitements par lot et des traitements en streaming.","title":"Objectifs du TP"},{"location":"tp2/#outils-et-versions","text":"Apache Hadoop Version: 3.3.6 Apache Spark Version: 3.5.0 Docker Version latest Visual Studio Code Version 1.85.1 (ou tout autre IDE de votre choix) Java Version 1.8. Unix-like ou Unix-based Systems (Divers Linux et MacOS)","title":"Outils et Versions"},{"location":"tp2/#spark","text":"","title":"Spark"},{"location":"tp2/#presentation","text":"Spark est un syst\u00e8me de traitement rapide et parall\u00e8le. Il fournit des APIs de haut niveau en Java, Scala, Python et R, et un moteur optimis\u00e9 qui supporte l'ex\u00e9cution des graphes. Il supporte \u00e9galement un ensemble d'outils de haut niveau tels que Spark SQL pour le support du traitement de donn\u00e9es structur\u00e9es, MLlib pour l'apprentissage des donn\u00e9es, GraphX pour le traitement des graphes, et Spark Streaming pour le traitment des donn\u00e9es en streaming.","title":"Pr\u00e9sentation"},{"location":"tp2/#spark-et-hadoop","text":"Spark peut s'ex\u00e9cuter sur plusieurs plateformes: Hadoop, Mesos, en standalone ou sur le cloud. Il peut \u00e9galement acc\u00e9der \u00e0 diverses sources de donn\u00e9es, comme HDFS, Cassandra, HBase et S3. Dans ce TP, nous allons ex\u00e9cuter Spark sur Hadoop YARN. YARN s'occupera ainsi de la gestion des ressources pour le d\u00e9clenchement et l'ex\u00e9cution des Jobs Spark.","title":"Spark et Hadoop"},{"location":"tp2/#installation","text":"Nous avons proc\u00e9d\u00e9 \u00e0 l'installation de Spark sur le cluster Hadoop utilis\u00e9 dans le TP1 . Suivre les \u00e9tapes d\u00e9crites dans la partie Installation du TP1 pour t\u00e9l\u00e9charger l'image et ex\u00e9cuter les trois contenaires. Si cela est d\u00e9j\u00e0 fait, il suffit de lancer vos machines gr\u00e2ce aux commandes suivantes: docker start hadoop-master hadoop-worker1 hadoop-worker2 puis d'entrer dans le contenaire master: docker exec -it hadoop-master bash Lancer ensuite les d\u00e9mons yarn et hdfs: ./start-hadoop.sh Vous pourrez v\u00e9rifier que tous les d\u00e9mons sont lanc\u00e9s en tapant: jps . Un r\u00e9sultat semblable au suivant pourra \u00eatre visible: 880 Jps 257 NameNode 613 ResourceManager 456 SecondaryNameNode La m\u00eame op\u00e9ration sur les noeuds workers (auxquels vous acc\u00e9dez \u00e0 partir de votre machine h\u00f4te de la m\u00eame fa\u00e7on que le noeud ma\u00eetre, c'est \u00e0 dire en tapant par exemple docker exec -it hadoop-worker1 bash ) devrait donner: 176 NodeManager 65 DataNode 311 Jps","title":"Installation"},{"location":"tp2/#test-de-spark-avec-spark-shell","text":"Dans le but de tester l'ex\u00e9cution de spark, commencer par cr\u00e9er un fichier file1.txt dans votre noeud master, contenant le texte suivant: Hello Spark Wordcount! Hello Hadoop Also :) Charger ensuite ce fichier dans HDFS: hdfs dfs -put file1.txt Erreur possible Si le message suivant s'affiche: put: `.': No such file or directory , c'est parce que l'arborescence du r\u00e9pertoire principal n'est pas cr\u00e9\u00e9e dans HDFS. Pour le faire, il suffit d'ex\u00e9cuter la commande suivante avant la commande de chargement : hadoop fs mkdir -p . Pour v\u00e9rifier que spark est bien install\u00e9, taper la commande suivante: spark-shell Vous devriez avoir un r\u00e9sultat semblable au suivant: Vous pourrez tester spark avec un code scala simple comme suit (\u00e0 ex\u00e9cuter ligne par ligne): 1 2 3 4 val lines = sc . textFile ( \"file1.txt\" ) val words = lines . flatMap ( _ . split ( \"\\\\s+\" )) val wc = words . map ( w => ( w , 1 )). reduceByKey ( _ + _ ) wc . saveAsTextFile ( \"file1.count\" ) Ce code vient de (1) charger le fichier file1.txt de HDFS, (2) s\u00e9parer les mots selon les caract\u00e8res d'espacement, (3) appliquer un map sur les mots obtenus qui produit le couple ( <mot> , 1), puis un reduce qui permet de faire la somme des 1 des mots identiques. Pour afficher le r\u00e9sultat, sortir de spark-shell en cliquant sur Ctrl-C . T\u00e9l\u00e9charger ensuite le r\u00e9pertoire file1.count cr\u00e9\u00e9 dans HDFS comme suit: hdfs dfs -get file1.count Le contenu des deux fichiers part-00000 et part-00001 ressemble \u00e0 ce qui suit:","title":"Test de Spark avec Spark-Shell"},{"location":"tp2/#lapi-de-spark","text":"A un haut niveau d'abstraction, chaque application Spark consiste en un programme driver qui ex\u00e9cute la fonction main de l'utilisateur et lance plusieurs op\u00e9rations parall\u00e8les sur le cluster. L'abstraction principale fournie par Spark est un RDD ( Resilient Distributed Dataset ), qui repr\u00e9sente une collection d'\u00e9l\u00e9ments partitionn\u00e9s \u00e0 travers les noeuds du cluster, et sur lesquelles on peut op\u00e9rer en parall\u00e8le. Les RDDs sont cr\u00e9\u00e9s \u00e0 partir d'un fichier dans HDFS par exemple, puis le transforment. Les utilisateurs peuvent demander \u00e0 Spark de sauvegarder un RDD en m\u00e9moire, lui permettant ainsi d'\u00eatre r\u00e9utilis\u00e9 efficacement \u00e0 travers plusieurs op\u00e9rations parall\u00e8les. Les RDDs supportent deux types d'op\u00e9rations: les transformations , qui permettent de cr\u00e9er un nouveau Dataset \u00e0 partir d'un Dataset existant les actions , qui retournent une valeur au programme driver apr\u00e8s avoir ex\u00e9cut\u00e9 un calcul sur le Dataset. Par exemple, un map est une transformation qui passe chaque \u00e9l\u00e9ment du dataset via une fonction, et retourne un nouvel RDD repr\u00e9sentant les r\u00e9sultats. Un reduce est une action qui agr\u00e8ge tous les \u00e9l\u00e9ments du RDD en utilisant une certaine fonction et retourne le r\u00e9sultat final au programme. Toutes les transformations dans Spark sont lazy , car elles ne calculent pas le r\u00e9sultat imm\u00e9diatement. Elles se souviennent des transformations appliqu\u00e9es \u00e0 un dataset de base (par ex. un fichier). Les transformations ne sont calcul\u00e9es que quand une action n\u00e9cessite qu'un r\u00e9sultat soit retourn\u00e9 au programme principal. Cela permet \u00e0 Spark de s'ex\u00e9cuter plus efficacement.","title":"L'API de Spark"},{"location":"tp2/#exemple","text":"L'exemple que nous allons pr\u00e9senter ici par \u00e9tapes permet de relever les mots les plus fr\u00e9quents dans un fichier. Pour cela, le code suivant est utilis\u00e9: //Etape 1 - Cr\u00e9er un RDD \u00e0 partir d'un fichier texte de Hadoop val docs = sc . textFile ( \"file1.txt\" ) //Etape 2 - Convertir les lignes en minuscule val lower = docs . map ( line => line . toLowerCase ) //Etape 3 - S\u00e9parer les lignes en mots val words = lower . flatMap ( line => line . split ( \"\\\\s+\" )) //Etape 4 - produire les tuples (mot, 1) val counts = words . map ( word => ( word , 1 )) //Etape 5 - Compter tous les mots val freq = counts . reduceByKey ( _ + _ ) //Etape 6 - Inverser les tuples (transformation avec swap) freq . map ( _ . swap ) //Etape 6 - Inverser les tuples (action de s\u00e9lection des 3 premiers) val top = freq . map ( _ . swap ). top ( 3 )","title":"Exemple"},{"location":"tp2/#spark-batch-en-java","text":"","title":"Spark Batch en Java"},{"location":"tp2/#preparation-de-lenvironnement-et-code","text":"Nous allons dans cette partie cr\u00e9er un projet Spark Batch en Java (un simple WordCount), le charger sur le cluster et lancer le job. Cr\u00e9er un projet Maven avec VSCode, en utilisant la config suivante: <groupId> spark.batch </groupId> <artifactId> wordcount-spark </artifactId> Rajouter dans le fichier pom les d\u00e9pendances n\u00e9cessaires, et indiquer la version du compilateur Java: <properties> <maven.compiler.source> 1.8 </maven.compiler.source> <maven.compiler.target> 1.8 </maven.compiler.target> </properties> <dependencies> <dependency> <groupId> org.apache.spark </groupId> <artifactId> spark-core_2.13 </artifactId> <version> 3.5.0 </version> </dependency> <dependency> <groupId> org.slf4j </groupId> <artifactId> slf4j-reload4j </artifactId> <version> 2.1.0-alpha1 </version> <scope> test </scope> </dependency> </dependencies> Sous le r\u00e9pertoire java, cr\u00e9er un package que vous appellerez spark.batch.tp21 , et dedans, une classe appel\u00e9e WordCountTask . \u00c9crire le code suivant dans WordCountTask.java : import org.apache.spark.SparkConf ; import org.apache.spark.api.java.JavaPairRDD ; import org.apache.spark.api.java.JavaRDD ; import org.apache.spark.api.java.JavaSparkContext ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import scala.Tuple2 ; import java.util.Arrays ; import com.google.common.base.Preconditions ; public class WordCountTask { private static final Logger LOGGER = LoggerFactory . getLogger ( WordCountTask . class ); public static void main ( String [] args ) { Preconditions . checkArgument ( args . length > 1 , \"Please provide the path of input file and output dir as parameters.\" ); new WordCountTask (). run ( args [ 0 ] , args [ 1 ] ); } public void run ( String inputFilePath , String outputDir ) { String master = \"local[*]\" ; SparkConf conf = new SparkConf () . setAppName ( WordCountTask . class . getName ()) . setMaster ( master ); JavaSparkContext sc = new JavaSparkContext ( conf ); JavaRDD < String > textFile = sc . textFile ( inputFilePath ); JavaPairRDD < String , Integer > counts = textFile . flatMap ( s -> Arrays . asList ( s . split ( \" \" )). iterator ()) . mapToPair ( word -> new Tuple2 <> ( word , 1 )) . reduceByKey (( a , b ) -> a + b ); counts . saveAsTextFile ( outputDir ); } } La premi\u00e8re chose \u00e0 faire dans un programme Spark est de cr\u00e9er un objet JavaSparkContext , qui indique \u00e0 Spark comment acc\u00e9der \u00e0 un cluster. Pour cr\u00e9er ce contexte, vous aurez besoin de construire un objet SparkConf qui contient toutes les informations sur l'application. appName est le nom de l'application master est une URL d'un cluster Spark, Mesos ou YARN, ou bien une cha\u00eene sp\u00e9ciale local pour lancer le job en mode local. Warning Nous avons indiqu\u00e9 ici que notre master est local pour les besoins du test, mais plus tard, en le packageant pour le cluster, nous allons enlever cette indication. Il est en effet d\u00e9conseill\u00e9 de la hard-coder dans le programme, il faudrait plut\u00f4t l'indiquer comme option de commande \u00e0 chaque fois que nous lan\u00e7ons le job. Le reste du code de l'application est la version en Java de l'exemple en scala que nous avions fait avec spark-shell.","title":"Pr\u00e9paration de l'environnement et Code"},{"location":"tp2/#test-du-code-en-local","text":"Pour tester le code sur votre machine, proc\u00e9der aux \u00e9tapes suivantes: Ins\u00e9rer un fichier texte de votre choix (par exemple le fameux loremipsum.txt ) dans le r\u00e9pertoire src/main/resources. Lancer le programme en utilisant les arguments suivants: Arg1 : le chemin du fichier loremipsum.txt Arg2 : le chemin d'un r\u00e9pertoire out sous resources (vous ne devez pas le cr\u00e9er) Cliquer sur OK, et lancer la configuration. Si tout se passe bien, un r\u00e9pertoire out sera cr\u00e9\u00e9 sous resources , qui contient (entre autres) deux fichiers: part-00000, part-00001.","title":"Test du code en local"},{"location":"tp2/#lancement-du-code-sur-le-cluster","text":"Pour ex\u00e9cuter le code sur le cluster, modifier comme indiqu\u00e9 les lignes en jaune dans ce qui suit: public class WordCountTask { private static final Logger LOGGER = LoggerFactory . getLogger ( WordCountTask . class ); public static void main ( String [] args ) { checkArgument ( args . length > 1 , \"Please provide the path of input file and output dir as parameters.\" ); new WordCountTask (). run ( args [ 0 ] , args [ 1 ] ); } public void run ( String inputFilePath , String outputDir ) { SparkConf conf = new SparkConf () . setAppName ( WordCountTask . class . getName ()); JavaSparkContext sc = new JavaSparkContext ( conf ); JavaRDD < String > textFile = sc . textFile ( inputFilePath ); JavaPairRDD < String , Integer > counts = textFile . flatMap ( s -> Arrays . asList ( s . split ( \"\\t\" )). iterator ()) . mapToPair ( word -> new Tuple2 <> ( word , 1 )) . reduceByKey (( a , b ) -> a + b ); counts . saveAsTextFile ( outputDir ); } } Lancer ensuite une configuration de type Maven, avec la commande package . Un fichier intitul\u00e9 wordcount-spark-1.0-SNAPSHOT.jar sera cr\u00e9\u00e9 sous le r\u00e9pertoire target. Nous allons maintenant copier ce fichier dans docker. Pour cela, naviguer vers le r\u00e9pertoire du projet avec votre terminal (ou plus simplement utiliser le terminal dans VSCode), et taper la commande suivante: docker cp target/wordcount-spark-1.0-SNAPSHOT.jar hadoop-master:/root/wordcount-spark.jar Revenir \u00e0 votre contenaire master, et lancer un job Spark en utilisant ce fichier jar g\u00e9n\u00e9r\u00e9, avec la commande spark-submit , un script utilis\u00e9 pour lancer des applications spark sur un cluster. spark-submit --class spark.batch.tp21.WordCountTask --master local wordcount-spark.jar input/purchases.txt out-spark Nous allons lancer le job en mode local, pour commencer. Le fichier en entr\u00e9e est le fichier purchases.txt (que vous d\u00e9j\u00e0 charg\u00e9 dans HDFS dans le TP pr\u00e9c\u00e9dent), et le r\u00e9sultat sera stock\u00e9 dans un nouveau r\u00e9pertoire out-spark . Attention V\u00e9rifiez bien que le fichier purchases existe dans le r\u00e9pertoire input de HDFS (et que le r\u00e9pertoire out-spark n'existe pas)! Si ce n'est pas le cas, vous pouvez le charger avec les commandes suivantes: hdfs dfs -mkdir -p input hdfs dfs -put purchases.txt input Si tout se passe bien, vous devriez trouver, dans le r\u00e9pertoire out-spark , deux fichiers part-00000 et part-00001, qui ressemblent \u00e0 ce qui suit: Nous allons maintenant tester le comportement de spark-submit si on l'ex\u00e9cute en mode cluster sur YARN. Pour cela, ex\u00e9cuter le code suivant: spark-submit --class spark.batch.tp21.WordCountTask --master yarn --deploy-mode cluster wordcount-spark.jar input/purchases.txt out-spark2 En lan\u00e7ant le job sur Yarn, deux modes de d\u00e9ploiement sont possibles: Mode cluster : o\u00f9 tout le job s'ex\u00e9cute dans le cluster, c'est \u00e0 dire les Spark Executors (qui ex\u00e9cutent les vraies t\u00e2ches) et le Spark Driver (qui ordonnance les Executors). Ce dernier sera encapsul\u00e9 dans un YARN Application Master. Mode client : o\u00f9 Spark Driver s'ex\u00e9cute sur la machine cliente (tel que votre propre ordinateur portable). Si votre machine s'\u00e9teint, le job s'arr\u00eate. Ce mode est appropri\u00e9 pour les jobs interactifs. Si tout se passe bien, vous devriez obtenir un r\u00e9pertoire out-spark2 dans HDFS avec les fichiers usuels. En cas d'erreur: consulter les logs! En cas d'erreur ou d'interruption du job sur Yarn, vous pourrez consulter les fichiers logs pour chercher le message d'erreur (le message affich\u00e9 sur la console n'est pas assez explicite). Pour cela, sur votre navigateur, aller \u00e0 l'adresse: http://localhost:8041/logs/userlogs et suivez toujours les derniers liens jusqu'\u00e0 stderr .","title":"Lancement du code sur le cluster"},{"location":"tp2/#spark-streaming","text":"Spark est connu pour supporter \u00e9galement le traitement des donn\u00e9es en streaming. Les donn\u00e9es peuvent \u00eatre lues \u00e0 partir de plusieurs sources tel que Kafka, Flume, Kinesis ou des sockets TCP, et peuvent \u00eatre trait\u00e9es en utilisant des algorithmes complexes. Ensuite, les donn\u00e9es trait\u00e9es peuvent \u00eatre stock\u00e9es sur des syst\u00e8mes de fichiers, des bases de donn\u00e9es ou des dashboards. Il est m\u00eame possible de r\u00e9aliser des algorithmes de machine learning et de traitement de graphes sur les flux de donn\u00e9es. En interne, il fonctionne comme suit: Spark Streaming re\u00e7oit des donn\u00e9es en streaming et les divise en micro-batches, qui sont ensuite calcul\u00e9s par le moteur de spark pour g\u00e9n\u00e9rer le flux final de r\u00e9sultats.","title":"Spark Streaming"},{"location":"tp2/#environnement-et-code","text":"Nous allons commencer par tester le streaming en local, comme d'habitude. Pour cela: Commencer par cr\u00e9er un nouveau projet Maven, avec le fichier pom suivant: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <project xmlns= \"http://maven.apache.org/POM/4.0.0\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation= \"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\" > <modelVersion> 4.0.0 </modelVersion> <groupId> spark.streaming </groupId> <artifactId> stream </artifactId> <version> 1.0-SNAPSHOT </version> <properties> <maven.compiler.source> 1.8 </maven.compiler.source> <maven.compiler.target> 1.8 </maven.compiler.target> </properties> <dependencies> <dependency> <groupId> org.apache.spark </groupId> <artifactId> spark-core_2.13 </artifactId> <version> 3.5.0 </version> </dependency> <dependency> <groupId> org.apache.spark </groupId> <artifactId> spark-streaming_2.13 </artifactId> <version> 3.5.0 </version> </dependency> </dependencies> </project> Cr\u00e9er une classe spark.streaming.tp22.Stream avec le code suivant: import org.apache.spark.sql.Dataset ; import org.apache.spark.sql.Encoders ; import org.apache.spark.sql.SparkSession ; import org.apache.spark.sql.streaming.StreamingQuery ; import org.apache.spark.sql.streaming.StreamingQueryException ; import org.apache.spark.sql.streaming.Trigger ; import java.util.concurrent.TimeoutException ; import java.util.Arrays ; public class Stream { public static void main ( String [] args ) throws StreamingQueryException , TimeoutException { SparkSession spark = SparkSession . builder () . appName ( \"NetworkWordCount\" ) . master ( \"local[*]\" ) . getOrCreate (); // Create DataFrame representing the stream of input lines from connection to localhost:9999 Dataset < String > lines = spark . readStream () . format ( \"socket\" ) . option ( \"host\" , \"localhost\" ) . option ( \"port\" , 9999 ) . load () . as ( Encoders . STRING ()); // Split the lines into words Dataset < String > words = lines . flatMap ( ( String x ) -> Arrays . asList ( x . split ( \" \" )). iterator (), Encoders . STRING ()); // Generate running word count Dataset < org . apache . spark . sql . Row > wordCounts = words . groupBy ( \"value\" ). count (); // Start running the query that prints the running counts to the console StreamingQuery query = wordCounts . writeStream () . outputMode ( \"complete\" ) . format ( \"console\" ) . trigger ( Trigger . ProcessingTime ( \"1 second\" )) . start (); query . awaitTermination (); } } Ce code permet de calculer le nombre de mots dans un stream de donn\u00e9es (provenant du port localhost:9999) chaque seconde. Dans sa version actuelle, Spark encourage l'utilisation de Structured Streaming ,une API de haut niveau qui fournit un traitement plus efficace, et qui est construite au dessus de Spark SQL, en int\u00e9grant les structures DataFrame et Dataset. Trigger Interval Dans Spark Structured Streaming, le concept de microbatch est utilis\u00e9 pour traiter les donn\u00e9es en continu par petits lots incr\u00e9mentaux. La dur\u00e9e de chaque micro-lot est configurable et d\u00e9termine la fr\u00e9quence de traitement des donn\u00e9es en continu. Cette dur\u00e9e est appel\u00e9e \"intervalle de d\u00e9clenchement\". Si vous ne sp\u00e9cifiez pas explicitement d'intervalle de d\u00e9clenchement, le trigger par d\u00e9faut est ProcessingTime(0) , qui est aussi connu comme le mode de traitement par micro-lots. Ce param\u00e8tre par d\u00e9faut signifie que Spark essaiera de traiter les donn\u00e9es aussi rapidement que possible, sans d\u00e9lai fixe entre les micro-lots.","title":"Environnement et Code"},{"location":"tp2/#test-du-code-en-local_1","text":"Le stream ici sera diffus\u00e9 par une petite commande utilitaire qui se trouve dans la majorit\u00e9 des syst\u00e8mes Unix-like. Ouvrir un terminal, et taper la commande suivante pour cr\u00e9er le stream: nc -lk 9999 Ex\u00e9cuter votre classe Stream . L'application est en \u00e9coute sur localhost:9999. Commencer \u00e0 \u00e9crire des messages sur la console de votre terminal (l\u00e0 o\u00f9 vous avez lanc\u00e9 la commande nc) A chaque fois que vous entrez quelque chose sur le terminal, l'application Stream l'intercepte, et l'affichage sur l'\u00e9cran de la console change, comme suit:","title":"Test du code en Local"},{"location":"tp2/#lancement-du-code-sur-le-cluster_1","text":"Pour lancer le code pr\u00e9c\u00e9dent sur le cluster, il faudra d'abord faire une petite modification: changer la valeur localhost par l'IP de votre machine hote (celle que vous utilisez pour lancer la commande nc ). G\u00e9n\u00e9rer le fichier jar. Copier le fichier jar sur le contenaire master. Assurez-vous que la commande nc tourne bien sur votre machine, en attente de messages. Sur votre noeud master, lancer la commande suivante: spark-submit --class spark.streaming.tp22.Stream --master local stream.jar Cette fois, \u00e9norm\u00e9ment de texte est g\u00e9n\u00e9r\u00e9 en continu sur la console. Comme nous avons d\u00e9fini dans l'application console comme sortie, le r\u00e9sultat du traitement s'affichera au milieu de tout ce texte. Une fois que vous aurez saisi le texte \u00e0 tester, arr\u00eater l'application (avec Ctrl-C), et chercher dans le texte la cha\u00eene \" Batch: \". Vous trouverez normalement un r\u00e9sultat semblable au suivant:","title":"Lancement du code sur le cluster"},{"location":"tp2/#homework","text":"Vous allez maintenant appliquer des traitements sur votre projet selon votre besoin. Vos contraintes ici est d'avoir les deux types de traitement: Batch et Streaming. Vous pouvez utiliser Map Reduce ou Spark pour le traitement en Batch.","title":"Homework"},{"location":"tp3/","text":"T\u00e9l\u00e9charger PDF \u00b6 Objectifs du TP \u00b6 Utilisation de Kafka pour une collecte de donn\u00e9es distribu\u00e9e, et int\u00e9gration avec Spark. Outils et Versions \u00b6 Apache Kafka Version 2.13-3.6.1 Apache Hadoop Version: 3.3.6 Apache Spark Version: 3.5.0 Docker Version latest Visual Studio Code Version 1.85.1 (ou tout autre IDE de votre choix) Java Version 1.8. Unix-like ou Unix-based Systems (Divers Linux et MacOS) Kafka \u00b6 Qu'est-ce qu'un syst\u00e8me de messaging? \u00b6 Un syst\u00e8me de messaging ( Messaging System ) est responsable du transfert de donn\u00e9es d'une application \u00e0 une autre, de mani\u00e8re \u00e0 ce que les applications puissent se concentrer sur les donn\u00e9es sans s'inqui\u00e9ter de la mani\u00e8re de les partager ou de les collecter. Le messaging distribu\u00e9 est bas\u00e9 sur le principe de file de message fiable. Les messages sont stock\u00e9s de mani\u00e8re asynchrone dans des files d'attente entre les applications clientes et le syst\u00e8me de messaging. Deux types de patrons de messaging existent: Les syst\u00e8mes \" point \u00e0 point \" et les syst\u00e8mes \" publish-subscribe \". 1. Syst\u00e8mes de messaging Point \u00e0 Point \u00b6 Dans un syst\u00e8me point \u00e0 point, les messages sont stock\u00e9s dans une file. un ou plusieurs consommateurs peuvent consommer les message dans la file, mais un message ne peut \u00eatre consomm\u00e9 que par un seul consommateur \u00e0 la fois. Une fois le consommateur lit le message, ce dernier dispara\u00eet de la file. 2. Syst\u00e8mes de messaging Publish/Subscribe \u00b6 Dans un syst\u00e8me publish-subscribe, les messages sont stock\u00e9s dans un \" topic \". Contrairement \u00e0 un syst\u00e8me point \u00e0 point, les consommateurs peuvent souscrire \u00e0 un ou plusieurs topics et consommer tous les messages de ce topic. Pr\u00e9sentation de Kafka \u00b6 Apache Kafka est une plateforme de streaming qui b\u00e9n\u00e9ficie de trois fonctionnalit\u00e9s: Elle vous permet de publier et souscrire \u00e0 un flux d'enregistrements. Elle ressemble ainsi \u00e0 une file demessage ou un syst\u00e8me de messaging d'entreprise. Elle permet de stocker des flux d'enregistrements d'une fa\u00e7on tol\u00e9rante aux pannes. Elle vous permet de traiter (au besoin) les enregistrements au fur et \u00e0 mesure qu'ils arrivent. Les principaux avantages de Kafka sont: La fiabliti\u00e9 : Kafka est distribu\u00e9, partitionn\u00e9, r\u00e9pliqu\u00e9 et tol\u00e9rent aux fautes. La scalabilit\u00e9 : Kafka se met \u00e0 l'\u00e9chelle facilement et sans temps d'arr\u00eat. La durabilit\u00e9 : Kafka utilise un commit log distribu\u00e9, ce qui permet de stocker les messages sur le disque le plus vite possible. La performance : Kafka a un d\u00e9bit \u00e9lev\u00e9 pour la publication et l'abonnement. Architecture de Kafka \u00b6 Pour comprendre le fonctionnement de Kafka, il faut d'abord se familiariser avec le vocabulaire suivant: Topic : Un flux de messages appartenant \u00e0 une cat\u00e9gorie particuli\u00e8re. Les donn\u00e9es sont stock\u00e9es dans des topics. Partitions : Chaque topic est divis\u00e9 en partitions. Pour chaque topic, Kafka conserve un minimum d'une partition. Chaque partition contient des messages dans une s\u00e9quence ordonn\u00e9e immuable. Une partition est impl\u00e9ment\u00e9e comme un ensemble de s\u00e8gments de tailles \u00e9gales. Offset : Les enregistrements d'une partition ont chacun un identifiant s\u00e9quentiel appel\u00e9 offset , qui permet de l'identifier de mani\u00e8re unique dans la partition. R\u00e9pliques : Les r\u00e9pliques sont des backups d'une partition. Elles ne sont jamais lues ni modifi\u00e9es par les acteurs externes, elles servent uniquement \u00e0 pr\u00e9venir la perte de donn\u00e9es. Brokers : Les brokers (ou courtiers) sont de simples syst\u00e8mes responsables de maintenir les donn\u00e9es publi\u00e9es. Chaque courtier peut avoir z\u00e9ro ou plusieurs partitions par topic. Si un topic admet N partitions et N courtiers, chaque courtier va avoir une seule partition. Si le nombre de courtiers est plus grand que celui des partitions, certains n'auront aucune partition de ce topic. Cluster : Un syst\u00e8me Kafka ayant plus qu'un seul Broker est appel\u00e9 cluster Kafka . L'ajout de nouveau brokers est fait de mani\u00e8re transparente sans temps d'arr\u00eat. Producers : Les producteurs sont les \u00e9diteurs de messages \u00e0 un ou plusieurs topics Kafka. Ils envoient des donn\u00e9es aux courtiers Kafka. Chaque fois qu'un producteur publie un message \u00e0 un courtier, ce dernier rattache le message au dernier s\u00e8gment, ajout\u00e9 ainsi \u00e0 une partition. Un producteur peut \u00e9galement envoyer un message \u00e0 une partition particuli\u00e8re. Consumers : Les consommateurs lisent les donn\u00e9es \u00e0 partir des brokers. Ils souscrivent \u00e0 un ou plusieurs topics, et consomment les messages publi\u00e9s en extrayant les donn\u00e9es \u00e0 partir des brokers. Leaders : Le leader est le noeud responsable de toutes les lectures et \u00e9critures d'une partition donn\u00e9e. Chaque partition a un serveur jouant le r\u00f4le de leader. Follower : C'est un noeud qui suit les instructions du leader. Si le leader tombe en panne, l'un des followers deviendra automatiquement le nouveau leader. La figure suivante montre un exemple de flux entre les diff\u00e9rentes parties d'un syst\u00e8me Kafka: Dans cet exemple, un topic est configur\u00e9 en trois partitions. En supposant que, si le facteur de r\u00e9plication du topic est de 3, alors Kafka va cr\u00e9er trois r\u00e9pliques identiques de chaque partition et les placer dans le cluster pour les rendre disponibles pour toutes les op\u00e9rations. L'identifiant de la r\u00e9plique est le m\u00eame que l'identifiant du serveur qui l'h\u00e9berge. Pour \u00e9quilibrer la charge dans le cluster, chaque broker stocke une ou plusieurs de ces partitions. Plusieurs producteurs et consommateurs peuvent publier et extraire les messages au m\u00eame moment. Kafka et Zookeeper \u00b6 Zookeeper est un service centralis\u00e9 permettant de maintenir l'information de configuration, de nommage, de synchronisation et de services de groupe. Ces services sont utilis\u00e9s par les applications distribu\u00e9es en g\u00e9n\u00e9ral, et par Kafka en particulier. Pour \u00e9viter la complexit\u00e9 et difficult\u00e9 de leur impl\u00e9mentation manuelle, Zookeeper est utilis\u00e9. Un cluster Kafka consiste typiquement en plusieurs courtiers (Brokers) pour maintenir la r\u00e9partition de charge. Ces courtiers sont stateless, c'est pour cela qu'ils utilisent Zookeeper pour maintenir l'\u00e9tat du cluster. Un courtier peut g\u00e9rer des centaines de milliers de lectures et \u00e9critures par seconde, et chaque courtier peut g\u00e9rer des t\u00e9ra-octets de messages sans impact sur la performance. Zookeeper est utilis\u00e9 pour g\u00e9rer et coordonner les courtiers Kafka. Il permet de notifier les producteurs et consommateurs de messages de la pr\u00e9sence de tout nouveau courtier, ou de l'\u00e9chec d'un courtier dans le cluster. Il est \u00e0 noter que les nouvelles versions de Kafka abandonnent petit \u00e0 petit Zookeeper pour une gestion interne des m\u00e9tadonn\u00e9es, gr\u00e2ce au protocole de consensus appel\u00e9 KRaft (Kafka Raft). Installation \u00b6 Kafka a \u00e9t\u00e9 install\u00e9 sur le m\u00eame cluster que les deux TP pr\u00e9c\u00e9dents. Suivre les \u00e9tapes d\u00e9crites dans la partie Installation du TP1 pour t\u00e9l\u00e9charger l'image et ex\u00e9cuter les trois contenaires. Si cela est d\u00e9j\u00e0 fait, il suffit de lancer vos machines gr\u00e2ce aux commandes suivantes: docker start hadoop-master hadoop-worker1 hadoop-worker2 puis d'entrer dans le contenaire master: docker exec -it hadoop-master bash Lancer ensuite les d\u00e9mons yarn et hdfs: ./start-hadoop.sh Lancer Kafka et Zookeeper en tapant : ./start-kafka-zookeeper.sh Les deux d\u00e9mons Kafka et Zookeeper seront lanc\u00e9s. Vous pourrez v\u00e9rifier cela en tapant jps pour voir quels processus Java sont en ex\u00e9cution, vous devriez trouver les processus suivants (en plus des processus Hadoop usuels): 2756 Kafka 53 QuorumPeerMain Premi\u00e8re utilisation de Kafka \u00b6 Cr\u00e9ation d'un topic \u00b6 Pour g\u00e9rer les topics, Kafka fournit une commande appel\u00e9e kafka-topics.sh . Dans un nouveau terminal, taper la commande suivante pour cr\u00e9er un nouveau topic appel\u00e9 \"Hello-Kafka\". kafka-topics.sh --create --topic Hello-Kafka --replication-factor 1 --partitions 1 --bootstrap-server localhost:9092 Attention Cette commande fonctionne car nous avions rajout\u00e9 /usr/local/kafka/bin \u00e0 la variable d'environnement PATH. Si ce n'\u00e9tait pas le cas, on aurait du appeler /usr/local/kafka/bin/kafka-topics.sh Pour afficher la liste des topics existants, il faudra utiliser: kafka-topics.sh --list --bootstrap-server localhost:9092 Le r\u00e9sultat devrait \u00eatre : Hello-Kafka Exemple Producteur Consommateur \u00b6 Kafka fournit un exemple de producteur standard que vous pouvez directement utiliser. Il suffit de taper: kafka-console-producer.sh --broker-list localhost:9092 --topic Hello-Kafka Tout ce que vous taperez dor\u00e9navant sur la console sera envoy\u00e9 \u00e0 Kafka. L'option --broker-list permet de d\u00e9finir la liste des courtiers auxquels vous enverrez le message. Pour l'instant, vous n'en disposez que d'un, et il est d\u00e9ploy\u00e9 \u00e0 l'adresse localhost:9092. Pour lancer le consommateur standard, utiliser: kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic Hello-Kafka --from-beginning Le r\u00e9sultat devra ressembler au suivant: Configuration de plusieurs brokers \u00b6 Dans ce qui pr\u00e9c\u00e8de, nous avons configur\u00e9 Kafka pour lancer un seul broker. Pour cr\u00e9er plusieurs brokers, il suffit de dupliquer le fichier $KAFKA_HOME/config/server.properties autant de fois que n\u00e9cessaire. Dans notre cas, nous allons cr\u00e9er deux autre fichiers: server-one.properties et server-two.properties , puis nous modifions les param\u00e8tres suivants comme suit: ### config/server-one.properties broker.id = 1 listeners = PLAINTEXT://localhost:9093 log.dirs = /tmp/kafka-logs-1 ### config/server-two.properties broker.id = 2 listeners = PLAINTEXT://localhost:9094 log.dirs = /tmp/kafka-logs-2 Pour d\u00e9marrer les diff\u00e9rents brokers, il suffit d'appeler kafka-server-start.sh avec les nouveaux fichiers de configuration. kafka-server-start.sh $KAFKA_HOME /config/server.properties & kafka-server-start.sh $KAFKA_HOME /config/server-one.properties & kafka-server-start.sh $KAFKA_HOME /config/server-two.properties & Lancer jps pour voir les trois serveurs s'ex\u00e9cuter. Cr\u00e9ation d'une application personnalis\u00e9e \u00b6 Nous allons dans cette partie cr\u00e9er une application pour publier et consommer des messages de Kafka. Pour cela, nous allons utiliser KafkaProducer API et KafkaConsumer API. Producteur \u00b6 Pour cr\u00e9er un producteur Kafka, cr\u00e9er un fichier dans un r\u00e9pertoire de votre choix dans le contenaire master, intitul\u00e9 SimpleProducer.java . Son code est le suivant: import java.util.Properties ; import org.apache.kafka.clients.producer.Producer ; import org.apache.kafka.clients.producer.KafkaProducer ; import org.apache.kafka.clients.producer.ProducerRecord ; public class SimpleProducer { public static void main ( String [] args ) throws Exception { // Verifier que le topic est donne en argument if ( args . length == 0 ){ System . out . println ( \"Entrer le nom du topic\" ); return ; } // Assigner topicName a une variable String topicName = args [ 0 ] . toString (); // Creer une instance de proprietes pour acceder aux configurations du producteur Properties props = new Properties (); // Assigner l'identifiant du serveur kafka props . put ( \"bootstrap.servers\" , \"localhost:9092\" ); // Definir un acquittement pour les requetes du producteur props . put ( \"acks\" , \"all\" ); // Si la requete echoue, le producteur peut reessayer automatiquemt props . put ( \"retries\" , 0 ); // Specifier la taille du buffer size dans la config props . put ( \"batch.size\" , 16384 ); // buffer.memory controle le montant total de memoire disponible au producteur pour le buffering props . put ( \"buffer.memory\" , 33554432 ); props . put ( \"key.serializer\" , \"org.apache.kafka.common.serialization.StringSerializer\" ); props . put ( \"value.serializer\" , \"org.apache.kafka.common.serialization.StringSerializer\" ); Producer < String , String > producer = new KafkaProducer < String , String > ( props ); for ( int i = 0 ; i < 10 ; i ++ ) producer . send ( new ProducerRecord < String , String > ( topicName , Integer . toString ( i ), Integer . toString ( i ))); System . out . println ( \"Message envoye avec succes\" ); producer . close (); } } ProducerRecord est une paire clef/valeur envoy\u00e9e au cluster Kafka. Son constructeur peut prendre 4, 3 ou 2 param\u00e8tres, selon le besoin. Les signatures autoris\u00e9es sont comme suit: public ProducerRecord ( string topic , int partition , k key , v value ){...} public ProducerRecord ( string topic , k key , v value ){...} public ProducerRecord ( string topic , v value ){...} Pour compiler ce code, taper dans la console (en vous positionnant dans le r\u00e9pertoire qui contient le fichier SimpleProducer.java): javac -cp \" $KAFKA_HOME /libs/*\" :. SimpleProducer.java Lancer ensuite le producer en tapant: java -cp \" $KAFKA_HOME /libs/*\" :. SimpleProducer Hello-Kafka Pour voir le r\u00e9sultat saisi dans Kafka, il est possible d'utiliser le consommateur pr\u00e9d\u00e9fini de Kafka, \u00e0 condition d'utiliser le m\u00eame topic: kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic Hello-Kafka --from-beginning Le r\u00e9sultat devrait contenir les donn\u00e9es suivantes : 0 1 2 3 4 5 6 7 8 9 Consommateur \u00b6 Pour cr\u00e9er un consommateur, proc\u00e9der de m\u00eame. Cr\u00e9er un fichier SimpleConsumer.java , avec le code suivant: import java.util.Properties ; import java.util.Arrays ; import java.time.Duration ; import org.apache.kafka.clients.consumer.KafkaConsumer ; import org.apache.kafka.clients.consumer.ConsumerRecords ; import org.apache.kafka.clients.consumer.ConsumerRecord ; public class SimpleConsumer { public static void main ( String [] args ) throws Exception { if ( args . length == 0 ){ System . out . println ( \"Entrer le nom du topic\" ); return ; } String topicName = args [ 0 ] . toString (); Properties props = new Properties (); props . put ( \"bootstrap.servers\" , \"localhost:9092\" ); props . put ( \"group.id\" , \"test\" ); props . put ( \"enable.auto.commit\" , \"true\" ); props . put ( \"auto.commit.interval.ms\" , \"1000\" ); props . put ( \"session.timeout.ms\" , \"30000\" ); props . put ( \"key.deserializer\" , \"org.apache.kafka.common.serialization.StringDeserializer\" ); props . put ( \"value.deserializer\" , \"org.apache.kafka.common.serialization.StringDeserializer\" ); KafkaConsumer < String , String > consumer = new KafkaConsumer < String , String > ( props ); // Kafka Consumer va souscrire a la liste de topics ici consumer . subscribe ( Arrays . asList ( topicName )); // Afficher le nom du topic System . out . println ( \"Souscris au topic \" + topicName ); int i = 0 ; while ( true ) { ConsumerRecords < String , String > records = consumer . poll ( Duration . ofMillis ( 100 )); for ( ConsumerRecord < String , String > record : records ) // Afficher l'offset, clef et valeur des enregistrements du consommateur System . out . printf ( \"offset = %d, key = %s, value = %s\\n\" , record . offset (), record . key (), record . value ()); } } } Compiler le consommateur avec: javac -cp \" $KAFKA_HOME /libs/*\" :. SimpleConsumer.java Puis l'ex\u00e9cuter: java -cp \" $KAFKA_HOME /libs/*\" :. SimpleConsumer Hello-Kafka Le consommateur est maintenant \u00e0 l'\u00e9coute du serveur de messagerie. Ouvrir un nouveau terminal et relancer le producteur que vous aviez d\u00e9velopp\u00e9 tout \u00e0 l'heure. Le r\u00e9sultat dans le consommateur devrait ressembler \u00e0 ceci. offset = 32, key = 0, value = 0 offset = 33, key = 1, value = 1 offset = 34, key = 2, value = 2 offset = 35, key = 3, value = 3 offset = 36, key = 4, value = 4 offset = 37, key = 5, value = 5 offset = 38, key = 6, value = 6 offset = 39, key = 7, value = 7 offset = 40, key = 8, value = 8 offset = 41, key = 9, value = 9 Int\u00e9gration de Kafka avec Spark \u00b6 Utilit\u00e9 \u00b6 Kafka repr\u00e9sente une plateforme potentielle pour le messaging et l'int\u00e9gration de Spark streaming. Kafka agit comme \u00e9tant le hub central pour les flux de donn\u00e9es en temps r\u00e9el, qui sont ensuite trait\u00e9s avec des algorithmes complexes par Spark Streaming. Une fois les donn\u00e9es trait\u00e9es, Spark Streaming peut publier les r\u00e9sultats dans un autre topic Kafka ou les stocker dans HDFS, d'autres bases de donn\u00e9es ou des dashboards. R\u00e9alisation \u00b6 Pour faire cela, nous allons r\u00e9aliser un exemple simple, o\u00f9 Spark Streaming consomme des donn\u00e9es de Kafka pour r\u00e9aliser l'\u00e9ternel wordcount. Dans votre machine locale, ouvrir votre IDE pr\u00e9f\u00e9r\u00e9 et cr\u00e9er un nouveau projet Maven, avec les propri\u00e9t\u00e9s suivantes: groupId : spark.kafka artifactId : stream-kafka-spark version : 1 Une fois le projet cr\u00e9\u00e9, modifier le fichier pom.xml pour qu'il ressemble \u00e0 ce qui suit: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <project xmlns= \"http://maven.apache.org/POM/4.0.0\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation= \"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\" > <modelVersion> 4.0.0 </modelVersion> <groupId> spark.kafka </groupId> <artifactId> stream-kafka-spark </artifactId> <version> 1 </version> <dependencies> <dependency> <groupId> org.apache.spark </groupId> <artifactId> spark-core_2.12 </artifactId> <version> 3.5.0 </version> </dependency> <dependency> <groupId> org.apache.spark </groupId> <artifactId> spark-streaming_2.12 </artifactId> <version> 3.5.0 </version> <scope> provided </scope> </dependency> <dependency> <groupId> org.apache.spark </groupId> <artifactId> spark-sql-kafka-0-10_2.12 </artifactId> <version> 3.5.0 </version> </dependency> <dependency> <groupId> org.apache.spark </groupId> <artifactId> spark-sql_2.12 </artifactId> <version> 3.5.0 </version> <scope> provided </scope> </dependency> <dependency> <groupId> org.apache.kafka </groupId> <artifactId> kafka-clients </artifactId> <version> 3.6.1 </version> </dependency> </dependencies> <build> <sourceDirectory> src/main/java </sourceDirectory> <testSourceDirectory> src/test/java </testSourceDirectory> <plugins> <plugin> <groupId> org.apache.maven.plugins </groupId> <artifactId> maven-compiler-plugin </artifactId> <configuration> <source> 1.8 </source> <target> 1.8 </target> </configuration> </plugin> <plugin> <artifactId> maven-assembly-plugin </artifactId> <configuration> <archive> <manifest> <mainClass> spark.kafka.SparkKafkaWordCount </mainClass> </manifest> </archive> <descriptorRefs> <descriptorRef> jar-with-dependencies </descriptorRef> </descriptorRefs> </configuration> </plugin> </plugins> </build> </project> Le plugin maven-assembly-plugin est utile pour pouvoir cr\u00e9er un jar contenant toutes les d\u00e9pendances du projet. Cr\u00e9er ensuite une classe SparkKafkaWordCount sous le package spark.kafka. Le code de cette classe sera comme suit: package spark.kafka ; import java.util.Arrays ; import org.apache.spark.api.java.function.FlatMapFunction ; import org.apache.spark.api.java.function.MapFunction ; import org.apache.spark.sql.Dataset ; import org.apache.spark.sql.Encoders ; import org.apache.spark.sql.Row ; import org.apache.spark.sql.SparkSession ; import scala.Tuple2 ; public class SparkKafkaWordCount { public static void main ( String [] args ) throws Exception { if ( args . length < 3 ) { System . err . println ( \"Usage: SparkKafkaWordCount <bootstrap-servers> <subscribe-topics> <group-id>\" ); System . exit ( 1 ); } String bootstrapServers = args [ 0 ] ; String topics = args [ 1 ] ; String groupId = args [ 2 ] ; SparkSession spark = SparkSession . builder () . appName ( \"SparkKafkaWordCount\" ) . getOrCreate (); // Create DataFrame representing the stream of input lines from Kafka Dataset < Row > df = spark . readStream () . format ( \"kafka\" ) . option ( \"kafka.bootstrap.servers\" , bootstrapServers ) . option ( \"subscribe\" , topics ) . option ( \"kafka.group.id\" , groupId ) . load (); df . selectExpr ( \"CAST(key AS STRING)\" , \"CAST(value AS STRING)\" ) . as ( Encoders . tuple ( Encoders . STRING (), Encoders . STRING ())) . flatMap (( FlatMapFunction < Tuple2 < String , String > , String > ) value -> Arrays . asList ( value . _2 . split ( \" \" )). iterator (), Encoders . STRING ()) . groupByKey (( MapFunction < String , String > ) value -> value , Encoders . STRING ()) . count () . writeStream () . outputMode ( \"complete\" ) . format ( \"console\" ) . start () . awaitTermination (); } } Ce code est un exemple d'application Spark utilisant l'int\u00e9gration de Spark avec Kafka pour effectuer un comptage de mots en temps r\u00e9el (WordCount) \u00e0 partir des messages Kafka. Voici ce que fait chaque partie du code : Initialisation de SparkSession : Le code commence par cr\u00e9er une instance de SparkSession, n\u00e9cessaire pour ex\u00e9cuter toute application Spark. Param\u00e8tres d'entr\u00e9e : Le programme attend trois arguments - l'adresse des serveurs Kafka ( bootstrapServers ), les topics auxquels s'abonner ( topics ) et l'identifiant du groupe consommateur Kafka ( groupId ). Lecture des donn\u00e9es de Kafka : Le DataFrame df est cr\u00e9\u00e9 en lisant le flux de donn\u00e9es \u00e0 partir de Kafka en utilisant les param\u00e8tres fournis. Les donn\u00e9es lues incluent key et value pour chaque message Kafka. Traitement des donn\u00e9es : Le code transforme les donn\u00e9es en castant key et value en cha\u00eenes de caract\u00e8res. Ensuite, il utilise la fonction flatMap pour diviser chaque valeur (chaque ligne de texte des messages Kafka) en mots individuels. Il utilise groupByKey pour regrouper les mots identiques. Comptage et \u00e9criture : Il compte le nombre d'occurrences de chaque mot ( count ) et \u00e9crit le r\u00e9sultat du comptage en continu \u00e0 la console ( writeStream.format(\"console\") ). Ex\u00e9cution : L'application d\u00e9marre le traitement du flux avec start () et attend que le traitement soit termin\u00e9 avec awaitTermination (). Nous allons ensuite r\u00e9er une configuration Maven pour lancer la commande: mvn clean compile assembly:single Dans le r\u00e9pertoire target, un fichier stream-kafka-spark-1-jar-with-dependencies.jar est cr\u00e9\u00e9. Copier ce fichier dans le contenaire master, en utilisant le terminal comme suit: docker cp target/stream-kafka-spark-1-jar-with-dependencies.jar hadoop-master:/root Revenir \u00e0 votre contenaire master, et lancer la commande spark-submit pour lancer l'\u00e9couteur de streaming spark. spark-submit --class spark.kafka.SparkKafkaWordCount --master local stream-kafka-spark-1-jar-with-dependencies.jar localhost:9092 Hello-Kafka mySparkConsumerGroup >> out Les trois options \u00e0 la fin de la commande sont requises par la classe SparkKafkaWordCount et repr\u00e9sentent respectivement l'adresse du broker, le nom du topic et l'identifiant du groupe de consommateurs Kafka (ensemble de consommateurs (processus ou threads) qui s'abonnent aux m\u00eames topics et qui travaillent ensemble pour consommer les donn\u00e9es). Remarque >>out est utilis\u00e9 pour stocker les r\u00e9sultats produits par spark streaming dans un fichier appel\u00e9 out . Dans un autre terminal, lancer le producteur pr\u00e9d\u00e9fini de Kafka pour tester la r\u00e9action du consommateur spark streaming: kafka-console-producer.sh --broker-list localhost:9092 --topic Hello-Kafka Ecrire du texte dans la fen\u00eatre du producteur. Ensuite, arr\u00eater le flux de spark-submit, et observer le contenu du fichier out. Il devra ressembler \u00e0 ce qui suit: Homework \u00b6 Pour votre projet, vous allez utiliser Kafka pour g\u00e9rer les flux entrants et les envoyer \u00e0 Spark. Ces m\u00eames donn\u00e9es (ou une partie de ces donn\u00e9es) peuvent \u00e9galement \u00eatre stock\u00e9es dans HDFS pour un traitement par lot ult\u00e9rieur. R\u00e9aliser les liaisons n\u00e9cessaires entre Kafka et Spark, puis Kafka et HDFS.","title":"TP3 - La Collecte de Donn\u00e9es avec le Bus Kafka"},{"location":"tp3/#telecharger-pdf","text":"","title":"T\u00e9l\u00e9charger PDF"},{"location":"tp3/#objectifs-du-tp","text":"Utilisation de Kafka pour une collecte de donn\u00e9es distribu\u00e9e, et int\u00e9gration avec Spark.","title":"Objectifs du TP"},{"location":"tp3/#outils-et-versions","text":"Apache Kafka Version 2.13-3.6.1 Apache Hadoop Version: 3.3.6 Apache Spark Version: 3.5.0 Docker Version latest Visual Studio Code Version 1.85.1 (ou tout autre IDE de votre choix) Java Version 1.8. Unix-like ou Unix-based Systems (Divers Linux et MacOS)","title":"Outils et Versions"},{"location":"tp3/#kafka","text":"","title":"Kafka"},{"location":"tp3/#quest-ce-quun-systeme-de-messaging","text":"Un syst\u00e8me de messaging ( Messaging System ) est responsable du transfert de donn\u00e9es d'une application \u00e0 une autre, de mani\u00e8re \u00e0 ce que les applications puissent se concentrer sur les donn\u00e9es sans s'inqui\u00e9ter de la mani\u00e8re de les partager ou de les collecter. Le messaging distribu\u00e9 est bas\u00e9 sur le principe de file de message fiable. Les messages sont stock\u00e9s de mani\u00e8re asynchrone dans des files d'attente entre les applications clientes et le syst\u00e8me de messaging. Deux types de patrons de messaging existent: Les syst\u00e8mes \" point \u00e0 point \" et les syst\u00e8mes \" publish-subscribe \".","title":"Qu'est-ce qu'un syst\u00e8me de messaging?"},{"location":"tp3/#1-systemes-de-messaging-point-a-point","text":"Dans un syst\u00e8me point \u00e0 point, les messages sont stock\u00e9s dans une file. un ou plusieurs consommateurs peuvent consommer les message dans la file, mais un message ne peut \u00eatre consomm\u00e9 que par un seul consommateur \u00e0 la fois. Une fois le consommateur lit le message, ce dernier dispara\u00eet de la file.","title":"1. Syst\u00e8mes de messaging Point \u00e0 Point"},{"location":"tp3/#2-systemes-de-messaging-publishsubscribe","text":"Dans un syst\u00e8me publish-subscribe, les messages sont stock\u00e9s dans un \" topic \". Contrairement \u00e0 un syst\u00e8me point \u00e0 point, les consommateurs peuvent souscrire \u00e0 un ou plusieurs topics et consommer tous les messages de ce topic.","title":"2. Syst\u00e8mes de messaging Publish/Subscribe"},{"location":"tp3/#presentation-de-kafka","text":"Apache Kafka est une plateforme de streaming qui b\u00e9n\u00e9ficie de trois fonctionnalit\u00e9s: Elle vous permet de publier et souscrire \u00e0 un flux d'enregistrements. Elle ressemble ainsi \u00e0 une file demessage ou un syst\u00e8me de messaging d'entreprise. Elle permet de stocker des flux d'enregistrements d'une fa\u00e7on tol\u00e9rante aux pannes. Elle vous permet de traiter (au besoin) les enregistrements au fur et \u00e0 mesure qu'ils arrivent. Les principaux avantages de Kafka sont: La fiabliti\u00e9 : Kafka est distribu\u00e9, partitionn\u00e9, r\u00e9pliqu\u00e9 et tol\u00e9rent aux fautes. La scalabilit\u00e9 : Kafka se met \u00e0 l'\u00e9chelle facilement et sans temps d'arr\u00eat. La durabilit\u00e9 : Kafka utilise un commit log distribu\u00e9, ce qui permet de stocker les messages sur le disque le plus vite possible. La performance : Kafka a un d\u00e9bit \u00e9lev\u00e9 pour la publication et l'abonnement.","title":"Pr\u00e9sentation de Kafka"},{"location":"tp3/#architecture-de-kafka","text":"Pour comprendre le fonctionnement de Kafka, il faut d'abord se familiariser avec le vocabulaire suivant: Topic : Un flux de messages appartenant \u00e0 une cat\u00e9gorie particuli\u00e8re. Les donn\u00e9es sont stock\u00e9es dans des topics. Partitions : Chaque topic est divis\u00e9 en partitions. Pour chaque topic, Kafka conserve un minimum d'une partition. Chaque partition contient des messages dans une s\u00e9quence ordonn\u00e9e immuable. Une partition est impl\u00e9ment\u00e9e comme un ensemble de s\u00e8gments de tailles \u00e9gales. Offset : Les enregistrements d'une partition ont chacun un identifiant s\u00e9quentiel appel\u00e9 offset , qui permet de l'identifier de mani\u00e8re unique dans la partition. R\u00e9pliques : Les r\u00e9pliques sont des backups d'une partition. Elles ne sont jamais lues ni modifi\u00e9es par les acteurs externes, elles servent uniquement \u00e0 pr\u00e9venir la perte de donn\u00e9es. Brokers : Les brokers (ou courtiers) sont de simples syst\u00e8mes responsables de maintenir les donn\u00e9es publi\u00e9es. Chaque courtier peut avoir z\u00e9ro ou plusieurs partitions par topic. Si un topic admet N partitions et N courtiers, chaque courtier va avoir une seule partition. Si le nombre de courtiers est plus grand que celui des partitions, certains n'auront aucune partition de ce topic. Cluster : Un syst\u00e8me Kafka ayant plus qu'un seul Broker est appel\u00e9 cluster Kafka . L'ajout de nouveau brokers est fait de mani\u00e8re transparente sans temps d'arr\u00eat. Producers : Les producteurs sont les \u00e9diteurs de messages \u00e0 un ou plusieurs topics Kafka. Ils envoient des donn\u00e9es aux courtiers Kafka. Chaque fois qu'un producteur publie un message \u00e0 un courtier, ce dernier rattache le message au dernier s\u00e8gment, ajout\u00e9 ainsi \u00e0 une partition. Un producteur peut \u00e9galement envoyer un message \u00e0 une partition particuli\u00e8re. Consumers : Les consommateurs lisent les donn\u00e9es \u00e0 partir des brokers. Ils souscrivent \u00e0 un ou plusieurs topics, et consomment les messages publi\u00e9s en extrayant les donn\u00e9es \u00e0 partir des brokers. Leaders : Le leader est le noeud responsable de toutes les lectures et \u00e9critures d'une partition donn\u00e9e. Chaque partition a un serveur jouant le r\u00f4le de leader. Follower : C'est un noeud qui suit les instructions du leader. Si le leader tombe en panne, l'un des followers deviendra automatiquement le nouveau leader. La figure suivante montre un exemple de flux entre les diff\u00e9rentes parties d'un syst\u00e8me Kafka: Dans cet exemple, un topic est configur\u00e9 en trois partitions. En supposant que, si le facteur de r\u00e9plication du topic est de 3, alors Kafka va cr\u00e9er trois r\u00e9pliques identiques de chaque partition et les placer dans le cluster pour les rendre disponibles pour toutes les op\u00e9rations. L'identifiant de la r\u00e9plique est le m\u00eame que l'identifiant du serveur qui l'h\u00e9berge. Pour \u00e9quilibrer la charge dans le cluster, chaque broker stocke une ou plusieurs de ces partitions. Plusieurs producteurs et consommateurs peuvent publier et extraire les messages au m\u00eame moment.","title":"Architecture de Kafka"},{"location":"tp3/#kafka-et-zookeeper","text":"Zookeeper est un service centralis\u00e9 permettant de maintenir l'information de configuration, de nommage, de synchronisation et de services de groupe. Ces services sont utilis\u00e9s par les applications distribu\u00e9es en g\u00e9n\u00e9ral, et par Kafka en particulier. Pour \u00e9viter la complexit\u00e9 et difficult\u00e9 de leur impl\u00e9mentation manuelle, Zookeeper est utilis\u00e9. Un cluster Kafka consiste typiquement en plusieurs courtiers (Brokers) pour maintenir la r\u00e9partition de charge. Ces courtiers sont stateless, c'est pour cela qu'ils utilisent Zookeeper pour maintenir l'\u00e9tat du cluster. Un courtier peut g\u00e9rer des centaines de milliers de lectures et \u00e9critures par seconde, et chaque courtier peut g\u00e9rer des t\u00e9ra-octets de messages sans impact sur la performance. Zookeeper est utilis\u00e9 pour g\u00e9rer et coordonner les courtiers Kafka. Il permet de notifier les producteurs et consommateurs de messages de la pr\u00e9sence de tout nouveau courtier, ou de l'\u00e9chec d'un courtier dans le cluster. Il est \u00e0 noter que les nouvelles versions de Kafka abandonnent petit \u00e0 petit Zookeeper pour une gestion interne des m\u00e9tadonn\u00e9es, gr\u00e2ce au protocole de consensus appel\u00e9 KRaft (Kafka Raft).","title":"Kafka et Zookeeper"},{"location":"tp3/#installation","text":"Kafka a \u00e9t\u00e9 install\u00e9 sur le m\u00eame cluster que les deux TP pr\u00e9c\u00e9dents. Suivre les \u00e9tapes d\u00e9crites dans la partie Installation du TP1 pour t\u00e9l\u00e9charger l'image et ex\u00e9cuter les trois contenaires. Si cela est d\u00e9j\u00e0 fait, il suffit de lancer vos machines gr\u00e2ce aux commandes suivantes: docker start hadoop-master hadoop-worker1 hadoop-worker2 puis d'entrer dans le contenaire master: docker exec -it hadoop-master bash Lancer ensuite les d\u00e9mons yarn et hdfs: ./start-hadoop.sh Lancer Kafka et Zookeeper en tapant : ./start-kafka-zookeeper.sh Les deux d\u00e9mons Kafka et Zookeeper seront lanc\u00e9s. Vous pourrez v\u00e9rifier cela en tapant jps pour voir quels processus Java sont en ex\u00e9cution, vous devriez trouver les processus suivants (en plus des processus Hadoop usuels): 2756 Kafka 53 QuorumPeerMain","title":"Installation"},{"location":"tp3/#premiere-utilisation-de-kafka","text":"","title":"Premi\u00e8re utilisation de Kafka"},{"location":"tp3/#creation-dun-topic","text":"Pour g\u00e9rer les topics, Kafka fournit une commande appel\u00e9e kafka-topics.sh . Dans un nouveau terminal, taper la commande suivante pour cr\u00e9er un nouveau topic appel\u00e9 \"Hello-Kafka\". kafka-topics.sh --create --topic Hello-Kafka --replication-factor 1 --partitions 1 --bootstrap-server localhost:9092 Attention Cette commande fonctionne car nous avions rajout\u00e9 /usr/local/kafka/bin \u00e0 la variable d'environnement PATH. Si ce n'\u00e9tait pas le cas, on aurait du appeler /usr/local/kafka/bin/kafka-topics.sh Pour afficher la liste des topics existants, il faudra utiliser: kafka-topics.sh --list --bootstrap-server localhost:9092 Le r\u00e9sultat devrait \u00eatre : Hello-Kafka","title":"Cr\u00e9ation d'un topic"},{"location":"tp3/#exemple-producteur-consommateur","text":"Kafka fournit un exemple de producteur standard que vous pouvez directement utiliser. Il suffit de taper: kafka-console-producer.sh --broker-list localhost:9092 --topic Hello-Kafka Tout ce que vous taperez dor\u00e9navant sur la console sera envoy\u00e9 \u00e0 Kafka. L'option --broker-list permet de d\u00e9finir la liste des courtiers auxquels vous enverrez le message. Pour l'instant, vous n'en disposez que d'un, et il est d\u00e9ploy\u00e9 \u00e0 l'adresse localhost:9092. Pour lancer le consommateur standard, utiliser: kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic Hello-Kafka --from-beginning Le r\u00e9sultat devra ressembler au suivant:","title":"Exemple Producteur Consommateur"},{"location":"tp3/#configuration-de-plusieurs-brokers","text":"Dans ce qui pr\u00e9c\u00e8de, nous avons configur\u00e9 Kafka pour lancer un seul broker. Pour cr\u00e9er plusieurs brokers, il suffit de dupliquer le fichier $KAFKA_HOME/config/server.properties autant de fois que n\u00e9cessaire. Dans notre cas, nous allons cr\u00e9er deux autre fichiers: server-one.properties et server-two.properties , puis nous modifions les param\u00e8tres suivants comme suit: ### config/server-one.properties broker.id = 1 listeners = PLAINTEXT://localhost:9093 log.dirs = /tmp/kafka-logs-1 ### config/server-two.properties broker.id = 2 listeners = PLAINTEXT://localhost:9094 log.dirs = /tmp/kafka-logs-2 Pour d\u00e9marrer les diff\u00e9rents brokers, il suffit d'appeler kafka-server-start.sh avec les nouveaux fichiers de configuration. kafka-server-start.sh $KAFKA_HOME /config/server.properties & kafka-server-start.sh $KAFKA_HOME /config/server-one.properties & kafka-server-start.sh $KAFKA_HOME /config/server-two.properties & Lancer jps pour voir les trois serveurs s'ex\u00e9cuter.","title":"Configuration de plusieurs brokers"},{"location":"tp3/#creation-dune-application-personnalisee","text":"Nous allons dans cette partie cr\u00e9er une application pour publier et consommer des messages de Kafka. Pour cela, nous allons utiliser KafkaProducer API et KafkaConsumer API.","title":"Cr\u00e9ation d'une application personnalis\u00e9e"},{"location":"tp3/#producteur","text":"Pour cr\u00e9er un producteur Kafka, cr\u00e9er un fichier dans un r\u00e9pertoire de votre choix dans le contenaire master, intitul\u00e9 SimpleProducer.java . Son code est le suivant: import java.util.Properties ; import org.apache.kafka.clients.producer.Producer ; import org.apache.kafka.clients.producer.KafkaProducer ; import org.apache.kafka.clients.producer.ProducerRecord ; public class SimpleProducer { public static void main ( String [] args ) throws Exception { // Verifier que le topic est donne en argument if ( args . length == 0 ){ System . out . println ( \"Entrer le nom du topic\" ); return ; } // Assigner topicName a une variable String topicName = args [ 0 ] . toString (); // Creer une instance de proprietes pour acceder aux configurations du producteur Properties props = new Properties (); // Assigner l'identifiant du serveur kafka props . put ( \"bootstrap.servers\" , \"localhost:9092\" ); // Definir un acquittement pour les requetes du producteur props . put ( \"acks\" , \"all\" ); // Si la requete echoue, le producteur peut reessayer automatiquemt props . put ( \"retries\" , 0 ); // Specifier la taille du buffer size dans la config props . put ( \"batch.size\" , 16384 ); // buffer.memory controle le montant total de memoire disponible au producteur pour le buffering props . put ( \"buffer.memory\" , 33554432 ); props . put ( \"key.serializer\" , \"org.apache.kafka.common.serialization.StringSerializer\" ); props . put ( \"value.serializer\" , \"org.apache.kafka.common.serialization.StringSerializer\" ); Producer < String , String > producer = new KafkaProducer < String , String > ( props ); for ( int i = 0 ; i < 10 ; i ++ ) producer . send ( new ProducerRecord < String , String > ( topicName , Integer . toString ( i ), Integer . toString ( i ))); System . out . println ( \"Message envoye avec succes\" ); producer . close (); } } ProducerRecord est une paire clef/valeur envoy\u00e9e au cluster Kafka. Son constructeur peut prendre 4, 3 ou 2 param\u00e8tres, selon le besoin. Les signatures autoris\u00e9es sont comme suit: public ProducerRecord ( string topic , int partition , k key , v value ){...} public ProducerRecord ( string topic , k key , v value ){...} public ProducerRecord ( string topic , v value ){...} Pour compiler ce code, taper dans la console (en vous positionnant dans le r\u00e9pertoire qui contient le fichier SimpleProducer.java): javac -cp \" $KAFKA_HOME /libs/*\" :. SimpleProducer.java Lancer ensuite le producer en tapant: java -cp \" $KAFKA_HOME /libs/*\" :. SimpleProducer Hello-Kafka Pour voir le r\u00e9sultat saisi dans Kafka, il est possible d'utiliser le consommateur pr\u00e9d\u00e9fini de Kafka, \u00e0 condition d'utiliser le m\u00eame topic: kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic Hello-Kafka --from-beginning Le r\u00e9sultat devrait contenir les donn\u00e9es suivantes : 0 1 2 3 4 5 6 7 8 9","title":"Producteur"},{"location":"tp3/#consommateur","text":"Pour cr\u00e9er un consommateur, proc\u00e9der de m\u00eame. Cr\u00e9er un fichier SimpleConsumer.java , avec le code suivant: import java.util.Properties ; import java.util.Arrays ; import java.time.Duration ; import org.apache.kafka.clients.consumer.KafkaConsumer ; import org.apache.kafka.clients.consumer.ConsumerRecords ; import org.apache.kafka.clients.consumer.ConsumerRecord ; public class SimpleConsumer { public static void main ( String [] args ) throws Exception { if ( args . length == 0 ){ System . out . println ( \"Entrer le nom du topic\" ); return ; } String topicName = args [ 0 ] . toString (); Properties props = new Properties (); props . put ( \"bootstrap.servers\" , \"localhost:9092\" ); props . put ( \"group.id\" , \"test\" ); props . put ( \"enable.auto.commit\" , \"true\" ); props . put ( \"auto.commit.interval.ms\" , \"1000\" ); props . put ( \"session.timeout.ms\" , \"30000\" ); props . put ( \"key.deserializer\" , \"org.apache.kafka.common.serialization.StringDeserializer\" ); props . put ( \"value.deserializer\" , \"org.apache.kafka.common.serialization.StringDeserializer\" ); KafkaConsumer < String , String > consumer = new KafkaConsumer < String , String > ( props ); // Kafka Consumer va souscrire a la liste de topics ici consumer . subscribe ( Arrays . asList ( topicName )); // Afficher le nom du topic System . out . println ( \"Souscris au topic \" + topicName ); int i = 0 ; while ( true ) { ConsumerRecords < String , String > records = consumer . poll ( Duration . ofMillis ( 100 )); for ( ConsumerRecord < String , String > record : records ) // Afficher l'offset, clef et valeur des enregistrements du consommateur System . out . printf ( \"offset = %d, key = %s, value = %s\\n\" , record . offset (), record . key (), record . value ()); } } } Compiler le consommateur avec: javac -cp \" $KAFKA_HOME /libs/*\" :. SimpleConsumer.java Puis l'ex\u00e9cuter: java -cp \" $KAFKA_HOME /libs/*\" :. SimpleConsumer Hello-Kafka Le consommateur est maintenant \u00e0 l'\u00e9coute du serveur de messagerie. Ouvrir un nouveau terminal et relancer le producteur que vous aviez d\u00e9velopp\u00e9 tout \u00e0 l'heure. Le r\u00e9sultat dans le consommateur devrait ressembler \u00e0 ceci. offset = 32, key = 0, value = 0 offset = 33, key = 1, value = 1 offset = 34, key = 2, value = 2 offset = 35, key = 3, value = 3 offset = 36, key = 4, value = 4 offset = 37, key = 5, value = 5 offset = 38, key = 6, value = 6 offset = 39, key = 7, value = 7 offset = 40, key = 8, value = 8 offset = 41, key = 9, value = 9","title":"Consommateur"},{"location":"tp3/#integration-de-kafka-avec-spark","text":"","title":"Int\u00e9gration de Kafka avec Spark"},{"location":"tp3/#utilite","text":"Kafka repr\u00e9sente une plateforme potentielle pour le messaging et l'int\u00e9gration de Spark streaming. Kafka agit comme \u00e9tant le hub central pour les flux de donn\u00e9es en temps r\u00e9el, qui sont ensuite trait\u00e9s avec des algorithmes complexes par Spark Streaming. Une fois les donn\u00e9es trait\u00e9es, Spark Streaming peut publier les r\u00e9sultats dans un autre topic Kafka ou les stocker dans HDFS, d'autres bases de donn\u00e9es ou des dashboards.","title":"Utilit\u00e9"},{"location":"tp3/#realisation","text":"Pour faire cela, nous allons r\u00e9aliser un exemple simple, o\u00f9 Spark Streaming consomme des donn\u00e9es de Kafka pour r\u00e9aliser l'\u00e9ternel wordcount. Dans votre machine locale, ouvrir votre IDE pr\u00e9f\u00e9r\u00e9 et cr\u00e9er un nouveau projet Maven, avec les propri\u00e9t\u00e9s suivantes: groupId : spark.kafka artifactId : stream-kafka-spark version : 1 Une fois le projet cr\u00e9\u00e9, modifier le fichier pom.xml pour qu'il ressemble \u00e0 ce qui suit: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <project xmlns= \"http://maven.apache.org/POM/4.0.0\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation= \"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\" > <modelVersion> 4.0.0 </modelVersion> <groupId> spark.kafka </groupId> <artifactId> stream-kafka-spark </artifactId> <version> 1 </version> <dependencies> <dependency> <groupId> org.apache.spark </groupId> <artifactId> spark-core_2.12 </artifactId> <version> 3.5.0 </version> </dependency> <dependency> <groupId> org.apache.spark </groupId> <artifactId> spark-streaming_2.12 </artifactId> <version> 3.5.0 </version> <scope> provided </scope> </dependency> <dependency> <groupId> org.apache.spark </groupId> <artifactId> spark-sql-kafka-0-10_2.12 </artifactId> <version> 3.5.0 </version> </dependency> <dependency> <groupId> org.apache.spark </groupId> <artifactId> spark-sql_2.12 </artifactId> <version> 3.5.0 </version> <scope> provided </scope> </dependency> <dependency> <groupId> org.apache.kafka </groupId> <artifactId> kafka-clients </artifactId> <version> 3.6.1 </version> </dependency> </dependencies> <build> <sourceDirectory> src/main/java </sourceDirectory> <testSourceDirectory> src/test/java </testSourceDirectory> <plugins> <plugin> <groupId> org.apache.maven.plugins </groupId> <artifactId> maven-compiler-plugin </artifactId> <configuration> <source> 1.8 </source> <target> 1.8 </target> </configuration> </plugin> <plugin> <artifactId> maven-assembly-plugin </artifactId> <configuration> <archive> <manifest> <mainClass> spark.kafka.SparkKafkaWordCount </mainClass> </manifest> </archive> <descriptorRefs> <descriptorRef> jar-with-dependencies </descriptorRef> </descriptorRefs> </configuration> </plugin> </plugins> </build> </project> Le plugin maven-assembly-plugin est utile pour pouvoir cr\u00e9er un jar contenant toutes les d\u00e9pendances du projet. Cr\u00e9er ensuite une classe SparkKafkaWordCount sous le package spark.kafka. Le code de cette classe sera comme suit: package spark.kafka ; import java.util.Arrays ; import org.apache.spark.api.java.function.FlatMapFunction ; import org.apache.spark.api.java.function.MapFunction ; import org.apache.spark.sql.Dataset ; import org.apache.spark.sql.Encoders ; import org.apache.spark.sql.Row ; import org.apache.spark.sql.SparkSession ; import scala.Tuple2 ; public class SparkKafkaWordCount { public static void main ( String [] args ) throws Exception { if ( args . length < 3 ) { System . err . println ( \"Usage: SparkKafkaWordCount <bootstrap-servers> <subscribe-topics> <group-id>\" ); System . exit ( 1 ); } String bootstrapServers = args [ 0 ] ; String topics = args [ 1 ] ; String groupId = args [ 2 ] ; SparkSession spark = SparkSession . builder () . appName ( \"SparkKafkaWordCount\" ) . getOrCreate (); // Create DataFrame representing the stream of input lines from Kafka Dataset < Row > df = spark . readStream () . format ( \"kafka\" ) . option ( \"kafka.bootstrap.servers\" , bootstrapServers ) . option ( \"subscribe\" , topics ) . option ( \"kafka.group.id\" , groupId ) . load (); df . selectExpr ( \"CAST(key AS STRING)\" , \"CAST(value AS STRING)\" ) . as ( Encoders . tuple ( Encoders . STRING (), Encoders . STRING ())) . flatMap (( FlatMapFunction < Tuple2 < String , String > , String > ) value -> Arrays . asList ( value . _2 . split ( \" \" )). iterator (), Encoders . STRING ()) . groupByKey (( MapFunction < String , String > ) value -> value , Encoders . STRING ()) . count () . writeStream () . outputMode ( \"complete\" ) . format ( \"console\" ) . start () . awaitTermination (); } } Ce code est un exemple d'application Spark utilisant l'int\u00e9gration de Spark avec Kafka pour effectuer un comptage de mots en temps r\u00e9el (WordCount) \u00e0 partir des messages Kafka. Voici ce que fait chaque partie du code : Initialisation de SparkSession : Le code commence par cr\u00e9er une instance de SparkSession, n\u00e9cessaire pour ex\u00e9cuter toute application Spark. Param\u00e8tres d'entr\u00e9e : Le programme attend trois arguments - l'adresse des serveurs Kafka ( bootstrapServers ), les topics auxquels s'abonner ( topics ) et l'identifiant du groupe consommateur Kafka ( groupId ). Lecture des donn\u00e9es de Kafka : Le DataFrame df est cr\u00e9\u00e9 en lisant le flux de donn\u00e9es \u00e0 partir de Kafka en utilisant les param\u00e8tres fournis. Les donn\u00e9es lues incluent key et value pour chaque message Kafka. Traitement des donn\u00e9es : Le code transforme les donn\u00e9es en castant key et value en cha\u00eenes de caract\u00e8res. Ensuite, il utilise la fonction flatMap pour diviser chaque valeur (chaque ligne de texte des messages Kafka) en mots individuels. Il utilise groupByKey pour regrouper les mots identiques. Comptage et \u00e9criture : Il compte le nombre d'occurrences de chaque mot ( count ) et \u00e9crit le r\u00e9sultat du comptage en continu \u00e0 la console ( writeStream.format(\"console\") ). Ex\u00e9cution : L'application d\u00e9marre le traitement du flux avec start () et attend que le traitement soit termin\u00e9 avec awaitTermination (). Nous allons ensuite r\u00e9er une configuration Maven pour lancer la commande: mvn clean compile assembly:single Dans le r\u00e9pertoire target, un fichier stream-kafka-spark-1-jar-with-dependencies.jar est cr\u00e9\u00e9. Copier ce fichier dans le contenaire master, en utilisant le terminal comme suit: docker cp target/stream-kafka-spark-1-jar-with-dependencies.jar hadoop-master:/root Revenir \u00e0 votre contenaire master, et lancer la commande spark-submit pour lancer l'\u00e9couteur de streaming spark. spark-submit --class spark.kafka.SparkKafkaWordCount --master local stream-kafka-spark-1-jar-with-dependencies.jar localhost:9092 Hello-Kafka mySparkConsumerGroup >> out Les trois options \u00e0 la fin de la commande sont requises par la classe SparkKafkaWordCount et repr\u00e9sentent respectivement l'adresse du broker, le nom du topic et l'identifiant du groupe de consommateurs Kafka (ensemble de consommateurs (processus ou threads) qui s'abonnent aux m\u00eames topics et qui travaillent ensemble pour consommer les donn\u00e9es). Remarque >>out est utilis\u00e9 pour stocker les r\u00e9sultats produits par spark streaming dans un fichier appel\u00e9 out . Dans un autre terminal, lancer le producteur pr\u00e9d\u00e9fini de Kafka pour tester la r\u00e9action du consommateur spark streaming: kafka-console-producer.sh --broker-list localhost:9092 --topic Hello-Kafka Ecrire du texte dans la fen\u00eatre du producteur. Ensuite, arr\u00eater le flux de spark-submit, et observer le contenu du fichier out. Il devra ressembler \u00e0 ce qui suit:","title":"R\u00e9alisation"},{"location":"tp3/#homework","text":"Pour votre projet, vous allez utiliser Kafka pour g\u00e9rer les flux entrants et les envoyer \u00e0 Spark. Ces m\u00eames donn\u00e9es (ou une partie de ces donn\u00e9es) peuvent \u00e9galement \u00eatre stock\u00e9es dans HDFS pour un traitement par lot ult\u00e9rieur. R\u00e9aliser les liaisons n\u00e9cessaires entre Kafka et Spark, puis Kafka et HDFS.","title":"Homework"},{"location":"tp4/","text":"T\u00e9l\u00e9charger PDF \u00b6 Objectifs du TP \u00b6 Manipulation de donn\u00e9es avec HBase, et traitement co-localis\u00e9 avec Spark. Outils et Versions \u00b6 Apache HBase Version 2.5.8 Apache Hadoop Version: 3.3.6 Apache Spark Version: 3.5.0 Docker Version latest Visual Studio Code Version 1.85.1 (ou tout autre IDE de votre choix) Java Version 1.8. Unix-like ou Unix-based Systems (Divers Linux et MacOS) Apache HBase \u00b6 Pr\u00e9sentation \u00b6 HBase est un syst\u00e8me de gestion de bases de donn\u00e9es distribu\u00e9, non-relationnel et orient\u00e9 colonnes, d\u00e9velopp\u00e9 au-dessus du syst\u00e8me de fichier HDFS. Il permet un acc\u00e8s al\u00e9atoire en \u00e9criture/lecture en temps r\u00e9el \u00e0 un tr\u00e8s grand ensemble de donn\u00e9es. Mod\u00e8le de donn\u00e9es \u00b6 Le mod\u00e8le se base sur six concepts, qui sont : Table : dans HBase les donn\u00e9es sont organis\u00e9es dans des tables. Les noms des tables sont des cha\u00eenes de caract\u00e8res. Row : dans chaque table, les donn\u00e9es sont organis\u00e9es dans des lignes. Une ligne est identifi\u00e9e par une cl\u00e9 unique ( RowKey ). La Rowkey n\u2019a pas de type, elle est trait\u00e9e comme un tableau d\u2019octets. Column Family : Les donn\u00e9es au sein d\u2019une ligne sont regroup\u00e9es par column family . Chaque ligne de la table a les m\u00eames column families, qui peuvent \u00eatre peupl\u00e9es ou pas. Les column families sont d\u00e9finies \u00e0 la cr\u00e9ation de la table dans HBase. Les noms des column families sont des cha\u00eenes de caract\u00e8res. Column qualifier : L\u2019acc\u00e8s aux donn\u00e9es au sein d\u2019une column family se fait via le column qualifier ou column . Ce dernier n\u2019est pas sp\u00e9cifi\u00e9 \u00e0 la cr\u00e9ation de la table mais plut\u00f4t \u00e0 l\u2019insertion de la donn\u00e9e. Comme les rowkeys, le column qualifier n\u2019est pas typ\u00e9, il est trait\u00e9 comme un tableau d\u2019octets. Cell : La combinaison du RowKey, de la Column Family ainsi que la Column qualifier identifie d\u2019une mani\u00e8re unique une cellule. Les donn\u00e9es stock\u00e9es dans une cellule sont appel\u00e9es les valeurs de cette cellule. Les valeurs n\u2019ont pas de type, ils sont toujours consid\u00e9r\u00e9s comme tableaux d\u2019octets. Version : Les valeurs au sein d\u2019une cellule sont versionn\u00e9s. Les versions sont identifi\u00e9s par leur timestamp (de type long). Le nombre de versions est configur\u00e9 via la Column Family. Par d\u00e9faut, ce nombre est \u00e9gal \u00e0 trois. Les donn\u00e9es dans HBase sont stock\u00e9es sous forme de HFiles , par colonnes, dans HDFS. Chaque HFile se charge de stocker des donn\u00e9es correspondantes \u00e0 une column family particuli\u00e8re. Autres caract\u00e9ristiques de HBase: HBase n'a pas de sch\u00e9ma pr\u00e9d\u00e9fini, sauf qu'il faut d\u00e9finir les familles de colonnes \u00e0 la cr\u00e9ation des tables, car elles repr\u00e9sentent l'organisation physique des donn\u00e9es HBase est d\u00e9crite comme \u00e9tant un magasin de donn\u00e9es clef/valeur, o\u00f9 la clef est la combinaison ( row - column family - column - timestamp ) repr\u00e9sente la clef, et la cell repr\u00e9sente la valeur. Architecture \u00b6 Physiquement, HBase est compos\u00e9 de trois types de serveurs de type Master/Worker. Region Servers : permettent de fournir les donn\u00e9es pour lectures et \u00e9critures. Pour acc\u00e9der aux donn\u00e9es, les clients communiquent avec les RegionServers directement. HBase HMaster : g\u00e8re l'affectation des r\u00e9gions, les op\u00e9rations de cr\u00e9ation et suppression de tables. Zookeeper : permet de maintenir le cluster en \u00e9tat. Le DataNode de Hadoop permet de stocker les donn\u00e9es que le Region Server g\u00e8re. Toutes les donn\u00e9es de HBase sont stock\u00e9es dans des fichiers HDFS. Les RegionServers sont colocalis\u00e9s avec les DataNodes. Le NameNode permet de maintenir les m\u00e9tadonn\u00e9es sur tous les blocs physiques qui forment les fichiers. Les tables HBase sont divis\u00e9es horizontalement, par row en plusieurs Regions . Une region contient toutes les lignes de la table comprises entre deux clefs donn\u00e9es. Les regions sont affect\u00e9es \u00e0 des noeuds dans le cluster, appel\u00e9s Region Servers , qui permettent de servir les donn\u00e9es pour la lecture et l'\u00e9criture. Un region server peut servir jusqu'\u00e0 1000 r\u00e9gions. Le HBase Master est responsable de coordonner les region servers en assignant les r\u00e9gions au d\u00e9marrage, les r\u00e9assignant en cas de r\u00e9cup\u00e9ration ou d'\u00e9quilibrage de charge, et en faisant le monitoring des instances des region servers dans le cluster. Il permet \u00e9galement de fournir une interface pour la cr\u00e9ation, la suppression et la modification des tables. HBase utilise Zookeeper comme service de coordination pour maintenir l'\u00e9tat du serveur dans le cluster. Zookeeper sait quels serveurs sont actifs et disponibles, et fournit une notification en cas d'\u00e9chec d'un serveur. Installation \u00b6 HBase est install\u00e9 sur le m\u00eame cluster que pr\u00e9c\u00e9demment. Suivre les \u00e9tapes d\u00e9crites dans la partie Installation du TP1 pour t\u00e9l\u00e9charger l'image et ex\u00e9cuter les trois contenaires. Si cela est d\u00e9j\u00e0 fait, il suffit de lancer vos machines gr\u00e2ce aux commandes suivantes: docker start hadoop-master hadoop-worker1 hadoop-worker2 puis d'entrer dans le contenaire master: docker exec -it hadoop-master bash Lancer ensuite les d\u00e9mons yarn et hdfs: ./start-hadoop.sh Lancer HBase en tapant : start-hbase.sh Une fois c'est fait, en tapant jps , vous devriez avoir un r\u00e9sultat ressemblant au suivant: 161 NameNode 1138 HRegionServer 499 ResourceManager 1028 HMaster 966 HQuorumPeer 1499 Jps 348 SecondaryNameNode Vous remarquerez que tous les d\u00e9mons Hadoop (NameNode, SecondaryNameNode et ResourceManager) ainsi que les d\u00e9mons HBase (HRegionServer, HMaster et HQuorumPeer (Zookeeper)) sont d\u00e9marr\u00e9s. Premi\u00e8re manipulation de HBase \u00b6 HBase Shell \u00b6 Pour manipuler votre base de donn\u00e9es avec son shell interactif, vous devez lancer le script suivant: hbase shell Vous obtiendrez une interface ressemblant \u00e0 la suivante: Nous allons cr\u00e9er une base de donn\u00e9es qui contient les donn\u00e9es suivantes: Commen\u00e7ons par cr\u00e9er la table, ainsi que les familles de colonnes associ\u00e9es: create 'sales_ledger' , 'customer' , 'sales' V\u00e9rifier que la table est bien cr\u00e9\u00e9e: list Vous devriez obtenir le r\u00e9sultat suivant: Ins\u00e9rer les diff\u00e9rentes lignes: put 'sales_ledger' , '101' , 'customer:name' , 'John White' put 'sales_ledger' , '101' , 'customer:city' , 'Los Angeles, CA' put 'sales_ledger' , '101' , 'sales:product' , 'Chairs' put 'sales_ledger' , '101' , 'sales:amount' , '$400.00' put 'sales_ledger' , '102' , 'customer:name' , 'Jane Brown' put 'sales_ledger' , '102' , 'customer:city' , 'Atlanta, GA' put 'sales_ledger' , '102' , 'sales:product' , 'Lamps' put 'sales_ledger' , '102' , 'sales:amount' , '$200.00' put 'sales_ledger' , '103' , 'customer:name' , 'Bill Green' put 'sales_ledger' , '103' , 'customer:city' , 'Pittsburgh, PA' put 'sales_ledger' , '103' , 'sales:product' , 'Desk' put 'sales_ledger' , '103' , 'sales:amount' , '$500.00' put 'sales_ledger' , '104' , 'customer:name' , 'Jack Black' put 'sales_ledger' , '104' , 'customer:city' , 'St. Louis, MO' put 'sales_ledger' , '104' , 'sales:product' , 'Bed' put 'sales_ledger' , '104' , 'sales:amount' , '$1,600.00' Visualiser le r\u00e9sultat de l'insertion, en tapant: scan 'sales_ledger' Afficher les valeurs de la colonne product de la ligne 102 get 'sales_ledger' , '102' , { COLUMN = > 'sales:product' } Vous obtiendrez: Vous pourrez quitter le shell en tapant: exit HBase API \u00b6 HBase fournit une API en Java pour pouvoir manipuler programmatiquement les donn\u00e9es de la base. Nous allons montrer ici un exemple tr\u00e8s simple. Dans votre contenaire principal, cr\u00e9er un r\u00e9pertoire hbase-code \u00e0 l'emplacement de votre choix, puis d\u00e9placez-vous dedans. mkdir hbase-code cd hbase-code Cr\u00e9er un projet maven intitul\u00e9 hello-hbase dans ce r\u00e9pertoire (maven est d\u00e9j\u00e0 install\u00e9): mvn archetype:generate -DgroupId = tn.insat.tp4 -DartifactId = hello-hbase -DarchetypeArtifactId = maven-archetype-quickstart -DinteractiveMode = false Cr\u00e9er et ouvrir le fichier HelloHBase.java sous le r\u00e9pertoire tp4 (J'utilise vim ici, qui est d\u00e9j\u00e0 install\u00e9): cd hello-hbase vim src/main/java/tn/insat/tp4/HelloHBase.java Ins\u00e9rer le code suivant dans le fichier: package tn.insat.tp4 ; import org.apache.hadoop.conf.Configuration ; import org.apache.hadoop.hbase.HBaseConfiguration ; import org.apache.hadoop.hbase.TableName ; import org.apache.hadoop.hbase.client.* ; import org.apache.hadoop.hbase.util.Bytes ; import java.io.IOException ; public class HelloHBase { private Table table1 ; private String tableName = \"user\" ; private String family1 = \"PersonalData\" ; private String family2 = \"ProfessionalData\" ; public void createHbaseTable () throws IOException { Configuration config = HBaseConfiguration . create (); try ( Connection connection = ConnectionFactory . createConnection ( config ); Admin admin = connection . getAdmin ()) { // Create table with column families TableDescriptor tableDescriptor = TableDescriptorBuilder . newBuilder ( TableName . valueOf ( tableName )) . setColumnFamily ( ColumnFamilyDescriptorBuilder . of ( family1 )) . setColumnFamily ( ColumnFamilyDescriptorBuilder . of ( family2 )) . build (); System . out . println ( \"Connecting\" ); System . out . println ( \"Creating Table\" ); createOrOverwrite ( admin , tableDescriptor ); System . out . println ( \"Done......\" ); table1 = connection . getTable ( TableName . valueOf ( tableName )); try { System . out . println ( \"Adding user: user1\" ); byte [] row1 = Bytes . toBytes ( \"user1\" ); Put p = new Put ( row1 ); p . addColumn ( Bytes . toBytes ( family1 ), Bytes . toBytes ( \"name\" ), Bytes . toBytes ( \"ahmed\" )); p . addColumn ( Bytes . toBytes ( family1 ), Bytes . toBytes ( \"address\" ), Bytes . toBytes ( \"tunis\" )); p . addColumn ( Bytes . toBytes ( family2 ), Bytes . toBytes ( \"company\" ), Bytes . toBytes ( \"biat\" )); p . addColumn ( Bytes . toBytes ( family2 ), Bytes . toBytes ( \"salary\" ), Bytes . toBytes ( \"10000\" )); table1 . put ( p ); System . out . println ( \"Adding user: user2\" ); byte [] row2 = Bytes . toBytes ( \"user2\" ); Put p2 = new Put ( row2 ); p2 . addColumn ( Bytes . toBytes ( family1 ), Bytes . toBytes ( \"name\" ), Bytes . toBytes ( \"imen\" )); p2 . addColumn ( Bytes . toBytes ( family1 ), Bytes . toBytes ( \"tel\" ), Bytes . toBytes ( \"21212121\" )); p2 . addColumn ( Bytes . toBytes ( family2 ), Bytes . toBytes ( \"profession\" ), Bytes . toBytes ( \"educator\" )); p2 . addColumn ( Bytes . toBytes ( family2 ), Bytes . toBytes ( \"company\" ), Bytes . toBytes ( \"insat\" )); table1 . put ( p2 ); System . out . println ( \"Reading data...\" ); Get g = new Get ( row1 ); Result r = table1 . get ( g ); System . out . println ( Bytes . toString ( r . getValue ( Bytes . toBytes ( family1 ), Bytes . toBytes ( \"name\" )))); } catch ( Exception e ) { e . printStackTrace (); } } finally { if ( table1 != null ) { table1 . close (); } } } public static void createOrOverwrite ( Admin admin , TableDescriptor table ) throws IOException { if ( admin . tableExists ( table . getTableName ())) { admin . disableTable ( table . getTableName ()); admin . deleteTable ( table . getTableName ()); } admin . createTable ( table ); } public static void main ( String [] args ) throws IOException { HelloHBase admin = new HelloHBase (); admin . createHbaseTable (); } } Ce code Java utilise l'API Apache HBase pour interagir avec une base de donn\u00e9es HBase : Cr\u00e9ation ou mise \u00e0 jour d'une table : Il cr\u00e9e ou remplace une table HBase nomm\u00e9e user qui contient deux familles de colonnes, PersonalData et ProfessionalData. Insertion de donn\u00e9es : Il ajoute deux enregistrements dans la table user. Le premier enregistrement pour l'utilisateur user1 inclut des informations personnelles et professionnelles comme le nom, l'adresse, la compagnie et le salaire. Le deuxi\u00e8me enregistrement, pour l'utilisateur user2, inclut \u00e9galement des informations personnelles et professionnelles comme le nom, le t\u00e9l\u00e9phone, la profession et la compagnie. Lecture de donn\u00e9es : Apr\u00e8s l'insertion, il lit et affiche la valeur de la colonne name sous la famille de colonnes PersonalData pour l'utilisateur user1. Ce script g\u00e8re \u00e9galement la connexion et la fermeture de la table ainsi que la gestion administrative de la base de donn\u00e9es HBase, s'assurant que la table existe, la cr\u00e9ant si n\u00e9cessaire, et la recr\u00e9ant si elle existait d\u00e9j\u00e0. Ouvrir le fichier pom.xml et saisir les informations suivantes: <project xmlns= \"http://maven.apache.org/POM/4.0.0\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation= \"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\" > <modelVersion> 4.0.0 </modelVersion> <groupId> tn.insat </groupId> <artifactId> hello-hbase </artifactId> <packaging> jar </packaging> <version> 1.0-SNAPSHOT </version> <name> hello-hbase </name> <url> http://maven.apache.org </url> <dependencies> <dependency> <groupId> junit </groupId> <artifactId> junit </artifactId> <version> 3.8.1 </version> <scope> test </scope> </dependency> <dependency> <groupId> org.slf4j </groupId> <artifactId> slf4j-api </artifactId> <version> 2.0.13 </version> </dependency> <!-- HBase dependencies --> <dependency> <groupId> org.apache.hbase </groupId> <artifactId> hbase-client </artifactId> <version> 2.5.8 </version> <exclusions> <exclusion> <groupId> org.apache.hadoop </groupId> <artifactId> * </artifactId> </exclusion> </exclusions> </dependency> <dependency> <groupId> org.apache.hbase </groupId> <artifactId> hbase-common </artifactId> <version> 2.5.8 </version> <exclusions> <exclusion> <groupId> org.apache.hadoop </groupId> <artifactId> * </artifactId> </exclusion> </exclusions> </dependency> <dependency> <groupId> org.apache.hbase </groupId> <artifactId> hbase-server </artifactId> <version> 2.5.8 </version> <exclusions> <exclusion> <groupId> org.apache.hadoop </groupId> <artifactId> * </artifactId> </exclusion> </exclusions> </dependency> <!-- Hadoop dependencies --> <dependency> <groupId> org.apache.hadoop </groupId> <artifactId> hadoop-common </artifactId> <version> 3.3.6 </version> </dependency> <dependency> <groupId> org.apache.hadoop </groupId> <artifactId> hadoop-hdfs-client </artifactId> <version> 3.3.6 </version> </dependency> </dependencies> <build> <plugins> <plugin> <groupId> org.apache.maven.plugins </groupId> <artifactId> maven-compiler-plugin </artifactId> <version> 3.8.1 </version> <configuration> <source> 1.8 </source> <target> 1.8 </target> </configuration> </plugin> <plugin> <groupId> org.apache.maven.plugins </groupId> <artifactId> maven-jar-plugin </artifactId> <version> 3.2.0 </version> <configuration> <archive> <manifest> <addClasspath> true </addClasspath> <mainClass> tn.insat.tp4.HelloHBase </mainClass> </manifest> </archive> </configuration> </plugin> <plugin> <groupId> org.apache.maven.plugins </groupId> <artifactId> maven-dependency-plugin </artifactId> <version> 3.1.2 </version> <executions> <execution> <id> copy-dependencies </id> <phase> prepare-package </phase> <goals> <goal> copy-dependencies </goal> </goals> <configuration> <outputDirectory> lib </outputDirectory> </configuration> </execution> </executions> </plugin> </plugins> </build> </project> Ce fichier de configuration permet de t\u00e9l\u00e9charger toutes les d\u00e9pendances n\u00e9cessaires au projet. De plus, il d\u00e9finit les plugins suivants: Le plugin maven-compiler-plugin est configur\u00e9 pour utiliser Java 8 (source et target 1.8). Le plugin maven-jar-plugin d\u00e9finit la classe principale et ajuste le chemin de classe pour l'ex\u00e9cution. Le plugin maven-dependency-plugin est utilis\u00e9 pour copier toutes les d\u00e9pendances dans un r\u00e9pertoire lib, facilitant l'ex\u00e9cution et la distribution. Tout en restant sous le r\u00e9pertoire hbase-code , compiler et g\u00e9n\u00e9rer l'ex\u00e9cutable du projet: mvn clean package Ex\u00e9cuter ce code: java -cp \"target/hello-hbase-1.0-SNAPSHOT.jar:lib/*\" tn.insat.tp4.HelloHBase Le r\u00e9sultat devrait ressembler au suivant: Traitement de donn\u00e9es avec Spark \u00b6 Install\u00e9 sur le m\u00eame cluster que HBase, Spark peut \u00eatre utilis\u00e9 pour r\u00e9aliser des traitements complexes sur les donn\u00e9es de HBase. Pour cela, les diff\u00e9rents Executors de Spark seront co-localis\u00e9s avec les region servers, et pourront r\u00e9aliser des traitements parall\u00e8les directement l\u00e0 o\u00f9 les donn\u00e9es sont stock\u00e9es. Nous allons r\u00e9aliser un traitement simple pour montrer comment greffer spark sur HBase. Cr\u00e9er un nouveau projet Maven. mvn archetype:generate -DgroupId = tn.insat.tp4 -DartifactId = hbase-spark -DinteractiveMode = false Utiliser le fichier POM suivant: <project xmlns= \"http://maven.apache.org/POM/4.0.0\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation= \"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\" > <modelVersion> 4.0.0 </modelVersion> <groupId> tn.insat.tp4 </groupId> <artifactId> hbase-spark </artifactId> <packaging> jar </packaging> <version> 1 </version> <name> hbase-spark </name> <url> http://maven.apache.org </url> <build> <plugins> <plugin> <groupId> org.apache.maven.plugins </groupId> <artifactId> maven-compiler-plugin </artifactId> <configuration> <source> 1.8 </source> <target> 1.8 </target> </configuration> </plugin> </plugins> </build> <dependencies> <dependency> <groupId> org.apache.hbase </groupId> <artifactId> hbase </artifactId> <version> 2.5.8 </version> <type> pom </type> </dependency> <dependency> <groupId> org.apache.hbase </groupId> <artifactId> hbase-spark </artifactId> <version> 2.0.0-alpha4 </version> </dependency> <dependency> <groupId> org.apache.spark </groupId> <artifactId> spark-core_2.13 </artifactId> <version> 3.5.0 </version> </dependency> </dependencies> </project> Cr\u00e9er la classe tn.insat.tp4.HbaseSparkProcess dont le code est le suivant: package tn.insat.tp4 ; import org.apache.hadoop.conf.Configuration ; import org.apache.hadoop.hbase.HBaseConfiguration ; import org.apache.hadoop.hbase.client.Result ; import org.apache.hadoop.hbase.io.ImmutableBytesWritable ; import org.apache.hadoop.hbase.mapreduce.TableInputFormat ; import org.apache.spark.SparkConf ; import org.apache.spark.api.java.JavaPairRDD ; import org.apache.spark.api.java.JavaSparkContext ; public class HbaseSparkProcess { public void createHbaseTable () { Configuration config = HBaseConfiguration . create (); SparkConf sparkConf = new SparkConf (). setAppName ( \"SparkHBaseTest\" ). setMaster ( \"local[4]\" ); JavaSparkContext jsc = new JavaSparkContext ( sparkConf ); config . set ( TableInputFormat . INPUT_TABLE , \"sales_ledger\" ); JavaPairRDD < ImmutableBytesWritable , Result > hBaseRDD = jsc . newAPIHadoopRDD ( config , TableInputFormat . class , ImmutableBytesWritable . class , Result . class ); System . out . println ( \"nombre d'enregistrements: \" + hBaseRDD . count ()); } public static void main ( String [] args ){ HbaseSparkProcess admin = new HbaseSparkProcess (); admin . createHbaseTable (); } } Ce code permet de lire la table sales_ledger que nous avions pr\u00e9c\u00e9demment cr\u00e9\u00e9e, puis de cr\u00e9er un RDD en m\u00e9moire la repr\u00e9sentant. Un Job spark permettra de compter le nombre d'\u00e9l\u00e9ments dans la base. Faire un mvn clean package sur le projet. Un fichier hbase-spark-1.jar sera cr\u00e9\u00e9 sous le r\u00e9pertoire target du projet. Copier tous les fichiers de la biblioth\u00e8que hbase dans le r\u00e9pertoire jars de spark: cp -r $HBASE_HOME /lib/* $SPARK_HOME /jars Ex\u00e9cuter ce fichier gr\u00e2ce \u00e0 spark-submit comme suit: spark-submit --class tn.insat.tp4.HbaseSparkProcess --master yarn --deploy-mode client processing-1.jar Le r\u00e9sultat qui devra s'afficher ressemblera au suivant: Activit\u00e9 Explorer l'utilitaire d'import de donn\u00e9es ImportTsv de HBase, et donner les \u00e9tapes n\u00e9cessaires pour l'int\u00e9grer et le tester dans votre environnement. Homework \u00b6 Pour la phase finale de votre projet, int\u00e9grez une base de donn\u00e9es NOSQL. Cela peut \u00eatre HBase, ou bien toute autre base telle que MongoDB ou Cassandra. L'objectif est de stocker dans cette base le r\u00e9sultat des traitements r\u00e9alis\u00e9s pr\u00e9c\u00e9demment. Il est ensuite demand\u00e9 d'utiliser un outil de visualisation de votre choix pour afficher des courbes ou graphes.","title":"TP4 - Stockage de Donn\u00e9es dans une Base NOSQL avec HBase"},{"location":"tp4/#telecharger-pdf","text":"","title":"T\u00e9l\u00e9charger PDF"},{"location":"tp4/#objectifs-du-tp","text":"Manipulation de donn\u00e9es avec HBase, et traitement co-localis\u00e9 avec Spark.","title":"Objectifs du TP"},{"location":"tp4/#outils-et-versions","text":"Apache HBase Version 2.5.8 Apache Hadoop Version: 3.3.6 Apache Spark Version: 3.5.0 Docker Version latest Visual Studio Code Version 1.85.1 (ou tout autre IDE de votre choix) Java Version 1.8. Unix-like ou Unix-based Systems (Divers Linux et MacOS)","title":"Outils et Versions"},{"location":"tp4/#apache-hbase","text":"","title":"Apache HBase"},{"location":"tp4/#presentation","text":"HBase est un syst\u00e8me de gestion de bases de donn\u00e9es distribu\u00e9, non-relationnel et orient\u00e9 colonnes, d\u00e9velopp\u00e9 au-dessus du syst\u00e8me de fichier HDFS. Il permet un acc\u00e8s al\u00e9atoire en \u00e9criture/lecture en temps r\u00e9el \u00e0 un tr\u00e8s grand ensemble de donn\u00e9es.","title":"Pr\u00e9sentation"},{"location":"tp4/#modele-de-donnees","text":"Le mod\u00e8le se base sur six concepts, qui sont : Table : dans HBase les donn\u00e9es sont organis\u00e9es dans des tables. Les noms des tables sont des cha\u00eenes de caract\u00e8res. Row : dans chaque table, les donn\u00e9es sont organis\u00e9es dans des lignes. Une ligne est identifi\u00e9e par une cl\u00e9 unique ( RowKey ). La Rowkey n\u2019a pas de type, elle est trait\u00e9e comme un tableau d\u2019octets. Column Family : Les donn\u00e9es au sein d\u2019une ligne sont regroup\u00e9es par column family . Chaque ligne de la table a les m\u00eames column families, qui peuvent \u00eatre peupl\u00e9es ou pas. Les column families sont d\u00e9finies \u00e0 la cr\u00e9ation de la table dans HBase. Les noms des column families sont des cha\u00eenes de caract\u00e8res. Column qualifier : L\u2019acc\u00e8s aux donn\u00e9es au sein d\u2019une column family se fait via le column qualifier ou column . Ce dernier n\u2019est pas sp\u00e9cifi\u00e9 \u00e0 la cr\u00e9ation de la table mais plut\u00f4t \u00e0 l\u2019insertion de la donn\u00e9e. Comme les rowkeys, le column qualifier n\u2019est pas typ\u00e9, il est trait\u00e9 comme un tableau d\u2019octets. Cell : La combinaison du RowKey, de la Column Family ainsi que la Column qualifier identifie d\u2019une mani\u00e8re unique une cellule. Les donn\u00e9es stock\u00e9es dans une cellule sont appel\u00e9es les valeurs de cette cellule. Les valeurs n\u2019ont pas de type, ils sont toujours consid\u00e9r\u00e9s comme tableaux d\u2019octets. Version : Les valeurs au sein d\u2019une cellule sont versionn\u00e9s. Les versions sont identifi\u00e9s par leur timestamp (de type long). Le nombre de versions est configur\u00e9 via la Column Family. Par d\u00e9faut, ce nombre est \u00e9gal \u00e0 trois. Les donn\u00e9es dans HBase sont stock\u00e9es sous forme de HFiles , par colonnes, dans HDFS. Chaque HFile se charge de stocker des donn\u00e9es correspondantes \u00e0 une column family particuli\u00e8re. Autres caract\u00e9ristiques de HBase: HBase n'a pas de sch\u00e9ma pr\u00e9d\u00e9fini, sauf qu'il faut d\u00e9finir les familles de colonnes \u00e0 la cr\u00e9ation des tables, car elles repr\u00e9sentent l'organisation physique des donn\u00e9es HBase est d\u00e9crite comme \u00e9tant un magasin de donn\u00e9es clef/valeur, o\u00f9 la clef est la combinaison ( row - column family - column - timestamp ) repr\u00e9sente la clef, et la cell repr\u00e9sente la valeur.","title":"Mod\u00e8le de donn\u00e9es"},{"location":"tp4/#architecture","text":"Physiquement, HBase est compos\u00e9 de trois types de serveurs de type Master/Worker. Region Servers : permettent de fournir les donn\u00e9es pour lectures et \u00e9critures. Pour acc\u00e9der aux donn\u00e9es, les clients communiquent avec les RegionServers directement. HBase HMaster : g\u00e8re l'affectation des r\u00e9gions, les op\u00e9rations de cr\u00e9ation et suppression de tables. Zookeeper : permet de maintenir le cluster en \u00e9tat. Le DataNode de Hadoop permet de stocker les donn\u00e9es que le Region Server g\u00e8re. Toutes les donn\u00e9es de HBase sont stock\u00e9es dans des fichiers HDFS. Les RegionServers sont colocalis\u00e9s avec les DataNodes. Le NameNode permet de maintenir les m\u00e9tadonn\u00e9es sur tous les blocs physiques qui forment les fichiers. Les tables HBase sont divis\u00e9es horizontalement, par row en plusieurs Regions . Une region contient toutes les lignes de la table comprises entre deux clefs donn\u00e9es. Les regions sont affect\u00e9es \u00e0 des noeuds dans le cluster, appel\u00e9s Region Servers , qui permettent de servir les donn\u00e9es pour la lecture et l'\u00e9criture. Un region server peut servir jusqu'\u00e0 1000 r\u00e9gions. Le HBase Master est responsable de coordonner les region servers en assignant les r\u00e9gions au d\u00e9marrage, les r\u00e9assignant en cas de r\u00e9cup\u00e9ration ou d'\u00e9quilibrage de charge, et en faisant le monitoring des instances des region servers dans le cluster. Il permet \u00e9galement de fournir une interface pour la cr\u00e9ation, la suppression et la modification des tables. HBase utilise Zookeeper comme service de coordination pour maintenir l'\u00e9tat du serveur dans le cluster. Zookeeper sait quels serveurs sont actifs et disponibles, et fournit une notification en cas d'\u00e9chec d'un serveur.","title":"Architecture"},{"location":"tp4/#installation","text":"HBase est install\u00e9 sur le m\u00eame cluster que pr\u00e9c\u00e9demment. Suivre les \u00e9tapes d\u00e9crites dans la partie Installation du TP1 pour t\u00e9l\u00e9charger l'image et ex\u00e9cuter les trois contenaires. Si cela est d\u00e9j\u00e0 fait, il suffit de lancer vos machines gr\u00e2ce aux commandes suivantes: docker start hadoop-master hadoop-worker1 hadoop-worker2 puis d'entrer dans le contenaire master: docker exec -it hadoop-master bash Lancer ensuite les d\u00e9mons yarn et hdfs: ./start-hadoop.sh Lancer HBase en tapant : start-hbase.sh Une fois c'est fait, en tapant jps , vous devriez avoir un r\u00e9sultat ressemblant au suivant: 161 NameNode 1138 HRegionServer 499 ResourceManager 1028 HMaster 966 HQuorumPeer 1499 Jps 348 SecondaryNameNode Vous remarquerez que tous les d\u00e9mons Hadoop (NameNode, SecondaryNameNode et ResourceManager) ainsi que les d\u00e9mons HBase (HRegionServer, HMaster et HQuorumPeer (Zookeeper)) sont d\u00e9marr\u00e9s.","title":"Installation"},{"location":"tp4/#premiere-manipulation-de-hbase","text":"","title":"Premi\u00e8re manipulation de HBase"},{"location":"tp4/#hbase-shell","text":"Pour manipuler votre base de donn\u00e9es avec son shell interactif, vous devez lancer le script suivant: hbase shell Vous obtiendrez une interface ressemblant \u00e0 la suivante: Nous allons cr\u00e9er une base de donn\u00e9es qui contient les donn\u00e9es suivantes: Commen\u00e7ons par cr\u00e9er la table, ainsi que les familles de colonnes associ\u00e9es: create 'sales_ledger' , 'customer' , 'sales' V\u00e9rifier que la table est bien cr\u00e9\u00e9e: list Vous devriez obtenir le r\u00e9sultat suivant: Ins\u00e9rer les diff\u00e9rentes lignes: put 'sales_ledger' , '101' , 'customer:name' , 'John White' put 'sales_ledger' , '101' , 'customer:city' , 'Los Angeles, CA' put 'sales_ledger' , '101' , 'sales:product' , 'Chairs' put 'sales_ledger' , '101' , 'sales:amount' , '$400.00' put 'sales_ledger' , '102' , 'customer:name' , 'Jane Brown' put 'sales_ledger' , '102' , 'customer:city' , 'Atlanta, GA' put 'sales_ledger' , '102' , 'sales:product' , 'Lamps' put 'sales_ledger' , '102' , 'sales:amount' , '$200.00' put 'sales_ledger' , '103' , 'customer:name' , 'Bill Green' put 'sales_ledger' , '103' , 'customer:city' , 'Pittsburgh, PA' put 'sales_ledger' , '103' , 'sales:product' , 'Desk' put 'sales_ledger' , '103' , 'sales:amount' , '$500.00' put 'sales_ledger' , '104' , 'customer:name' , 'Jack Black' put 'sales_ledger' , '104' , 'customer:city' , 'St. Louis, MO' put 'sales_ledger' , '104' , 'sales:product' , 'Bed' put 'sales_ledger' , '104' , 'sales:amount' , '$1,600.00' Visualiser le r\u00e9sultat de l'insertion, en tapant: scan 'sales_ledger' Afficher les valeurs de la colonne product de la ligne 102 get 'sales_ledger' , '102' , { COLUMN = > 'sales:product' } Vous obtiendrez: Vous pourrez quitter le shell en tapant: exit","title":"HBase Shell"},{"location":"tp4/#hbase-api","text":"HBase fournit une API en Java pour pouvoir manipuler programmatiquement les donn\u00e9es de la base. Nous allons montrer ici un exemple tr\u00e8s simple. Dans votre contenaire principal, cr\u00e9er un r\u00e9pertoire hbase-code \u00e0 l'emplacement de votre choix, puis d\u00e9placez-vous dedans. mkdir hbase-code cd hbase-code Cr\u00e9er un projet maven intitul\u00e9 hello-hbase dans ce r\u00e9pertoire (maven est d\u00e9j\u00e0 install\u00e9): mvn archetype:generate -DgroupId = tn.insat.tp4 -DartifactId = hello-hbase -DarchetypeArtifactId = maven-archetype-quickstart -DinteractiveMode = false Cr\u00e9er et ouvrir le fichier HelloHBase.java sous le r\u00e9pertoire tp4 (J'utilise vim ici, qui est d\u00e9j\u00e0 install\u00e9): cd hello-hbase vim src/main/java/tn/insat/tp4/HelloHBase.java Ins\u00e9rer le code suivant dans le fichier: package tn.insat.tp4 ; import org.apache.hadoop.conf.Configuration ; import org.apache.hadoop.hbase.HBaseConfiguration ; import org.apache.hadoop.hbase.TableName ; import org.apache.hadoop.hbase.client.* ; import org.apache.hadoop.hbase.util.Bytes ; import java.io.IOException ; public class HelloHBase { private Table table1 ; private String tableName = \"user\" ; private String family1 = \"PersonalData\" ; private String family2 = \"ProfessionalData\" ; public void createHbaseTable () throws IOException { Configuration config = HBaseConfiguration . create (); try ( Connection connection = ConnectionFactory . createConnection ( config ); Admin admin = connection . getAdmin ()) { // Create table with column families TableDescriptor tableDescriptor = TableDescriptorBuilder . newBuilder ( TableName . valueOf ( tableName )) . setColumnFamily ( ColumnFamilyDescriptorBuilder . of ( family1 )) . setColumnFamily ( ColumnFamilyDescriptorBuilder . of ( family2 )) . build (); System . out . println ( \"Connecting\" ); System . out . println ( \"Creating Table\" ); createOrOverwrite ( admin , tableDescriptor ); System . out . println ( \"Done......\" ); table1 = connection . getTable ( TableName . valueOf ( tableName )); try { System . out . println ( \"Adding user: user1\" ); byte [] row1 = Bytes . toBytes ( \"user1\" ); Put p = new Put ( row1 ); p . addColumn ( Bytes . toBytes ( family1 ), Bytes . toBytes ( \"name\" ), Bytes . toBytes ( \"ahmed\" )); p . addColumn ( Bytes . toBytes ( family1 ), Bytes . toBytes ( \"address\" ), Bytes . toBytes ( \"tunis\" )); p . addColumn ( Bytes . toBytes ( family2 ), Bytes . toBytes ( \"company\" ), Bytes . toBytes ( \"biat\" )); p . addColumn ( Bytes . toBytes ( family2 ), Bytes . toBytes ( \"salary\" ), Bytes . toBytes ( \"10000\" )); table1 . put ( p ); System . out . println ( \"Adding user: user2\" ); byte [] row2 = Bytes . toBytes ( \"user2\" ); Put p2 = new Put ( row2 ); p2 . addColumn ( Bytes . toBytes ( family1 ), Bytes . toBytes ( \"name\" ), Bytes . toBytes ( \"imen\" )); p2 . addColumn ( Bytes . toBytes ( family1 ), Bytes . toBytes ( \"tel\" ), Bytes . toBytes ( \"21212121\" )); p2 . addColumn ( Bytes . toBytes ( family2 ), Bytes . toBytes ( \"profession\" ), Bytes . toBytes ( \"educator\" )); p2 . addColumn ( Bytes . toBytes ( family2 ), Bytes . toBytes ( \"company\" ), Bytes . toBytes ( \"insat\" )); table1 . put ( p2 ); System . out . println ( \"Reading data...\" ); Get g = new Get ( row1 ); Result r = table1 . get ( g ); System . out . println ( Bytes . toString ( r . getValue ( Bytes . toBytes ( family1 ), Bytes . toBytes ( \"name\" )))); } catch ( Exception e ) { e . printStackTrace (); } } finally { if ( table1 != null ) { table1 . close (); } } } public static void createOrOverwrite ( Admin admin , TableDescriptor table ) throws IOException { if ( admin . tableExists ( table . getTableName ())) { admin . disableTable ( table . getTableName ()); admin . deleteTable ( table . getTableName ()); } admin . createTable ( table ); } public static void main ( String [] args ) throws IOException { HelloHBase admin = new HelloHBase (); admin . createHbaseTable (); } } Ce code Java utilise l'API Apache HBase pour interagir avec une base de donn\u00e9es HBase : Cr\u00e9ation ou mise \u00e0 jour d'une table : Il cr\u00e9e ou remplace une table HBase nomm\u00e9e user qui contient deux familles de colonnes, PersonalData et ProfessionalData. Insertion de donn\u00e9es : Il ajoute deux enregistrements dans la table user. Le premier enregistrement pour l'utilisateur user1 inclut des informations personnelles et professionnelles comme le nom, l'adresse, la compagnie et le salaire. Le deuxi\u00e8me enregistrement, pour l'utilisateur user2, inclut \u00e9galement des informations personnelles et professionnelles comme le nom, le t\u00e9l\u00e9phone, la profession et la compagnie. Lecture de donn\u00e9es : Apr\u00e8s l'insertion, il lit et affiche la valeur de la colonne name sous la famille de colonnes PersonalData pour l'utilisateur user1. Ce script g\u00e8re \u00e9galement la connexion et la fermeture de la table ainsi que la gestion administrative de la base de donn\u00e9es HBase, s'assurant que la table existe, la cr\u00e9ant si n\u00e9cessaire, et la recr\u00e9ant si elle existait d\u00e9j\u00e0. Ouvrir le fichier pom.xml et saisir les informations suivantes: <project xmlns= \"http://maven.apache.org/POM/4.0.0\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation= \"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\" > <modelVersion> 4.0.0 </modelVersion> <groupId> tn.insat </groupId> <artifactId> hello-hbase </artifactId> <packaging> jar </packaging> <version> 1.0-SNAPSHOT </version> <name> hello-hbase </name> <url> http://maven.apache.org </url> <dependencies> <dependency> <groupId> junit </groupId> <artifactId> junit </artifactId> <version> 3.8.1 </version> <scope> test </scope> </dependency> <dependency> <groupId> org.slf4j </groupId> <artifactId> slf4j-api </artifactId> <version> 2.0.13 </version> </dependency> <!-- HBase dependencies --> <dependency> <groupId> org.apache.hbase </groupId> <artifactId> hbase-client </artifactId> <version> 2.5.8 </version> <exclusions> <exclusion> <groupId> org.apache.hadoop </groupId> <artifactId> * </artifactId> </exclusion> </exclusions> </dependency> <dependency> <groupId> org.apache.hbase </groupId> <artifactId> hbase-common </artifactId> <version> 2.5.8 </version> <exclusions> <exclusion> <groupId> org.apache.hadoop </groupId> <artifactId> * </artifactId> </exclusion> </exclusions> </dependency> <dependency> <groupId> org.apache.hbase </groupId> <artifactId> hbase-server </artifactId> <version> 2.5.8 </version> <exclusions> <exclusion> <groupId> org.apache.hadoop </groupId> <artifactId> * </artifactId> </exclusion> </exclusions> </dependency> <!-- Hadoop dependencies --> <dependency> <groupId> org.apache.hadoop </groupId> <artifactId> hadoop-common </artifactId> <version> 3.3.6 </version> </dependency> <dependency> <groupId> org.apache.hadoop </groupId> <artifactId> hadoop-hdfs-client </artifactId> <version> 3.3.6 </version> </dependency> </dependencies> <build> <plugins> <plugin> <groupId> org.apache.maven.plugins </groupId> <artifactId> maven-compiler-plugin </artifactId> <version> 3.8.1 </version> <configuration> <source> 1.8 </source> <target> 1.8 </target> </configuration> </plugin> <plugin> <groupId> org.apache.maven.plugins </groupId> <artifactId> maven-jar-plugin </artifactId> <version> 3.2.0 </version> <configuration> <archive> <manifest> <addClasspath> true </addClasspath> <mainClass> tn.insat.tp4.HelloHBase </mainClass> </manifest> </archive> </configuration> </plugin> <plugin> <groupId> org.apache.maven.plugins </groupId> <artifactId> maven-dependency-plugin </artifactId> <version> 3.1.2 </version> <executions> <execution> <id> copy-dependencies </id> <phase> prepare-package </phase> <goals> <goal> copy-dependencies </goal> </goals> <configuration> <outputDirectory> lib </outputDirectory> </configuration> </execution> </executions> </plugin> </plugins> </build> </project> Ce fichier de configuration permet de t\u00e9l\u00e9charger toutes les d\u00e9pendances n\u00e9cessaires au projet. De plus, il d\u00e9finit les plugins suivants: Le plugin maven-compiler-plugin est configur\u00e9 pour utiliser Java 8 (source et target 1.8). Le plugin maven-jar-plugin d\u00e9finit la classe principale et ajuste le chemin de classe pour l'ex\u00e9cution. Le plugin maven-dependency-plugin est utilis\u00e9 pour copier toutes les d\u00e9pendances dans un r\u00e9pertoire lib, facilitant l'ex\u00e9cution et la distribution. Tout en restant sous le r\u00e9pertoire hbase-code , compiler et g\u00e9n\u00e9rer l'ex\u00e9cutable du projet: mvn clean package Ex\u00e9cuter ce code: java -cp \"target/hello-hbase-1.0-SNAPSHOT.jar:lib/*\" tn.insat.tp4.HelloHBase Le r\u00e9sultat devrait ressembler au suivant:","title":"HBase API"},{"location":"tp4/#traitement-de-donnees-avec-spark","text":"Install\u00e9 sur le m\u00eame cluster que HBase, Spark peut \u00eatre utilis\u00e9 pour r\u00e9aliser des traitements complexes sur les donn\u00e9es de HBase. Pour cela, les diff\u00e9rents Executors de Spark seront co-localis\u00e9s avec les region servers, et pourront r\u00e9aliser des traitements parall\u00e8les directement l\u00e0 o\u00f9 les donn\u00e9es sont stock\u00e9es. Nous allons r\u00e9aliser un traitement simple pour montrer comment greffer spark sur HBase. Cr\u00e9er un nouveau projet Maven. mvn archetype:generate -DgroupId = tn.insat.tp4 -DartifactId = hbase-spark -DinteractiveMode = false Utiliser le fichier POM suivant: <project xmlns= \"http://maven.apache.org/POM/4.0.0\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation= \"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\" > <modelVersion> 4.0.0 </modelVersion> <groupId> tn.insat.tp4 </groupId> <artifactId> hbase-spark </artifactId> <packaging> jar </packaging> <version> 1 </version> <name> hbase-spark </name> <url> http://maven.apache.org </url> <build> <plugins> <plugin> <groupId> org.apache.maven.plugins </groupId> <artifactId> maven-compiler-plugin </artifactId> <configuration> <source> 1.8 </source> <target> 1.8 </target> </configuration> </plugin> </plugins> </build> <dependencies> <dependency> <groupId> org.apache.hbase </groupId> <artifactId> hbase </artifactId> <version> 2.5.8 </version> <type> pom </type> </dependency> <dependency> <groupId> org.apache.hbase </groupId> <artifactId> hbase-spark </artifactId> <version> 2.0.0-alpha4 </version> </dependency> <dependency> <groupId> org.apache.spark </groupId> <artifactId> spark-core_2.13 </artifactId> <version> 3.5.0 </version> </dependency> </dependencies> </project> Cr\u00e9er la classe tn.insat.tp4.HbaseSparkProcess dont le code est le suivant: package tn.insat.tp4 ; import org.apache.hadoop.conf.Configuration ; import org.apache.hadoop.hbase.HBaseConfiguration ; import org.apache.hadoop.hbase.client.Result ; import org.apache.hadoop.hbase.io.ImmutableBytesWritable ; import org.apache.hadoop.hbase.mapreduce.TableInputFormat ; import org.apache.spark.SparkConf ; import org.apache.spark.api.java.JavaPairRDD ; import org.apache.spark.api.java.JavaSparkContext ; public class HbaseSparkProcess { public void createHbaseTable () { Configuration config = HBaseConfiguration . create (); SparkConf sparkConf = new SparkConf (). setAppName ( \"SparkHBaseTest\" ). setMaster ( \"local[4]\" ); JavaSparkContext jsc = new JavaSparkContext ( sparkConf ); config . set ( TableInputFormat . INPUT_TABLE , \"sales_ledger\" ); JavaPairRDD < ImmutableBytesWritable , Result > hBaseRDD = jsc . newAPIHadoopRDD ( config , TableInputFormat . class , ImmutableBytesWritable . class , Result . class ); System . out . println ( \"nombre d'enregistrements: \" + hBaseRDD . count ()); } public static void main ( String [] args ){ HbaseSparkProcess admin = new HbaseSparkProcess (); admin . createHbaseTable (); } } Ce code permet de lire la table sales_ledger que nous avions pr\u00e9c\u00e9demment cr\u00e9\u00e9e, puis de cr\u00e9er un RDD en m\u00e9moire la repr\u00e9sentant. Un Job spark permettra de compter le nombre d'\u00e9l\u00e9ments dans la base. Faire un mvn clean package sur le projet. Un fichier hbase-spark-1.jar sera cr\u00e9\u00e9 sous le r\u00e9pertoire target du projet. Copier tous les fichiers de la biblioth\u00e8que hbase dans le r\u00e9pertoire jars de spark: cp -r $HBASE_HOME /lib/* $SPARK_HOME /jars Ex\u00e9cuter ce fichier gr\u00e2ce \u00e0 spark-submit comme suit: spark-submit --class tn.insat.tp4.HbaseSparkProcess --master yarn --deploy-mode client processing-1.jar Le r\u00e9sultat qui devra s'afficher ressemblera au suivant: Activit\u00e9 Explorer l'utilitaire d'import de donn\u00e9es ImportTsv de HBase, et donner les \u00e9tapes n\u00e9cessaires pour l'int\u00e9grer et le tester dans votre environnement.","title":"Traitement de donn\u00e9es avec Spark"},{"location":"tp4/#homework","text":"Pour la phase finale de votre projet, int\u00e9grez une base de donn\u00e9es NOSQL. Cela peut \u00eatre HBase, ou bien toute autre base telle que MongoDB ou Cassandra. L'objectif est de stocker dans cette base le r\u00e9sultat des traitements r\u00e9alis\u00e9s pr\u00e9c\u00e9demment. Il est ensuite demand\u00e9 d'utiliser un outil de visualisation de votre choix pour afficher des courbes ou graphes.","title":"Homework"}]}