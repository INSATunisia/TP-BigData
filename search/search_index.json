{
    "docs": [
        {
            "location": "/", 
            "text": "Travaux Pratiques Big Data\n\n\nCe(tte) \u0153uvre est mise \u00e0 disposition selon les termes de la \nLicence Creative Commons Attribution - Pas d\u2019Utilisation Commerciale - Partage dans les M\u00eames Conditions 4.0 International\n.\n\n\n\n\nGL4 - INSAT\n\n\n\n\n\n\nCours Disponible ici: \nhttp://liliasfaxi.wixsite.com/liliasfaxi/big-data\n\n\nEdmodo : \nhttps://edmodo.com/public/big-data/group_id/26293086\n\n\nRepo Github :  \nhttps://github.com/INSATunisia\n\n\n\n\nOverview\n\n\nL'objectif de ce cours est d'initier les \u00e9tudiants aux architectures Big Data: Les notions, technologies, patrons et bonnes pratiques.\n\nLe cours comportera les chapitres suivants:\n\n\n\n\nIntroduction au Big Data\n\n\nHadoop et Map Reduce\n\n\nTraitement de donn\u00e9es (Batch, Streaming, Temps R\u00e9el, etc.)\n\n\nBases de donn\u00e9es NoSQL\n\n\nPutting it all together\n\n\n\n\nCe cours comporte cinq s\u00e9ances de travaux pratiques:\n\n\n\n\nTBD", 
            "title": "Travaux Pratiques Big Data"
        }, 
        {
            "location": "/#travaux-pratiques-big-data", 
            "text": "Ce(tte) \u0153uvre est mise \u00e0 disposition selon les termes de la  Licence Creative Commons Attribution - Pas d\u2019Utilisation Commerciale - Partage dans les M\u00eames Conditions 4.0 International .", 
            "title": "Travaux Pratiques Big Data"
        }, 
        {
            "location": "/#gl4-insat", 
            "text": "Cours Disponible ici:  http://liliasfaxi.wixsite.com/liliasfaxi/big-data  Edmodo :  https://edmodo.com/public/big-data/group_id/26293086  Repo Github :   https://github.com/INSATunisia", 
            "title": "GL4 - INSAT"
        }, 
        {
            "location": "/#overview", 
            "text": "L'objectif de ce cours est d'initier les \u00e9tudiants aux architectures Big Data: Les notions, technologies, patrons et bonnes pratiques. \nLe cours comportera les chapitres suivants:   Introduction au Big Data  Hadoop et Map Reduce  Traitement de donn\u00e9es (Batch, Streaming, Temps R\u00e9el, etc.)  Bases de donn\u00e9es NoSQL  Putting it all together   Ce cours comporte cinq s\u00e9ances de travaux pratiques:   TBD", 
            "title": "Overview"
        }, 
        {
            "location": "/tp1/", 
            "text": "TP1 - Le traitement Batch avec Hadoop HDFS et Map Reduce\n\n\n\n\nT\u00e9l\u00e9charger PDF\n\n\n\n\nObjectifs du TP\n\n\nInitiation au framework hadoop et au patron MapReduce, utilisation de docker pour lancer un cluster hadoop de 3 noeuds.\n\n\nOutils et Versions\n\n\n\n\nApache Hadoop\n Version: 2.7.2.\n\n\nDocker\n Version 17.09.1\n\n\nIntelliJ IDEA\n Version Ultimate 2016.1 (ou tout autre IDE de votre choix)\n\n\nJava\n Version 1.8.\n\n\nUnix-like ou Unix-based Systems (Divers Linux et MacOS)\n\n\n\n\nHadoop\n\n\nPr\u00e9sentation\n\n\nApache Hadoop\n est un framework open-source pour stocker et traiter les donne\u0301es volumineuses sur un cluster. Il est utilise\u0301 par un grand nombre de contributeurs et utilisateurs. Il a une licence Apache 2.0.\n\n\n\n\nHadoop et Docker\n\n\nPour d\u00e9ployer le framework Hadoop, nous allons utiliser des contenaires \nDocker\n. L'utilisation des contenaires va garantir la consistance entre les environnements de d\u00e9veloppement et permettra de r\u00e9duire consid\u00e9rablement la complexit\u00e9 de configuration des machines (dans le cas d'un acc\u00e8s natif) ainsi que la lourdeur d'ex\u00e9cution (si on opte pour l'utilisation d'une machine virtuelle).\n\n\nNous avons pour le d\u00e9ploiement des ressources de ce TP suivi les instructions pr\u00e9sent\u00e9es \nici\n.\n\n\nInstallation\n\n\nNous allons utiliser tout au long de ce TP trois contenaires repr\u00e9sentant respectivement un noeud ma\u00eetre (Namenode) et deux noeuds esclaves (Datanodes).\n\n\nVous devez pour cela avoir install\u00e9 docker sur votre machine, et l'avoir correctement configur\u00e9. Ouvrir la ligne de commande, et taper les instructions suivantes:\n\n\n\n\nCloner le repo github contenant les fichiers n\u00e9cessaires pour le lancement des contenaires et leur configuration:\n\n\n  git clone https://github.com/liliasfaxi/hadoop-cluster-docker\n\n\n\nConstruire l'image Docker \u00e0 partir du fichier Dockerfile fourni.\n\n\n  \ncd\n hadoop-cluster-docker\n  ./build-image.sh\n\n\n\nD\u00e9marrer les trois contenaires:\n\n\n  sudo ./start-container.sh\n\n\n\n\n\n\n\nAttention\n\n\nLe script \nstart-container.sh\n va r\u00e9initialiser les trois contenaires. Si vous voulez red\u00e9marrer un contenaire d\u00e9j\u00e0 cr\u00e9\u00e9, il ne faut pas l'ex\u00e9cuter de nouveau: tout sera effac\u00e9. Au lieu de cela, utiliser plut\u00f4t \ndocker start \ncontainer_id>\n. Pour lancer le shell, taper simplement (pour le master container, par exemple):\n\n\n    docker \nexec\n -it hadoop-master bash\n\n\n\n\n\n\nLe r\u00e9sultat de cette ex\u00e9cution sera le suivant:\n\n\n  start hadoop-master container...\n  start hadoop-slave1 container...\n  start hadoop-slave2 container...\n  root@hadoop-master:~#\n\n\n\n\nVous vous retrouverez dans le shell du namenode, et vous pourrez ainsi manipuler le cluster \u00e0 votre guise. La premi\u00e8re chose \u00e0 faire, une fois dans le contenaire, est de lancer hadoop et yarn. Un script est fourni pour cela, appel\u00e9 \nstart-hadoop.sh\n. Lancer ce script.\n\n\n  ./start-hadoop.sh\n\n\n\n\nLe r\u00e9sultat devra ressembler \u00e0 ce qui suit:\n\n\n\n\nPremiers pas avec Hadoop\n\n\nToutes les commandes interagissant avec le syste\u0300me Hadoop commencent par hadoop fs. Ensuite, les options rajoute\u0301es sont tre\u0300s largement inspire\u0301es des commandes Unix standard.\n\n\n\n\nCre\u0301er un re\u0301pertoire dans HDFS, appele\u0301 \ninput\n. Pour cela, taper:\n\n\n  hadoop fs \u2013mkdir -p input\n\n\n\n\n\n\n\nErreur\n\n\nSi pour une raison ou une autre, vous n'arrivez pas \u00e0 cr\u00e9er le r\u00e9pertoire \ninput\n, avec un message ressemblant \u00e0 ceci: \nls: `.': No such file or directory\n, veiller \u00e0 construire l'arborescence de l'utilisateur principal (root), comme suit:\n\n\nhadoop fs -mkdir -p /user/root\n\n\n\n\n\n\nNous allons utiliser le fichier  \npurchases.txt\n comme entr\u00e9e pour le traitement MapReduce. Ce fichier se trouve d\u00e9j\u00e0 sous le r\u00e9pertoire principal de votre machine master.\n\n\nCharger le fichier purchases dans le r\u00e9pertoire input que vous avez cr\u00e9\u00e9:\n\n  \n  hadoop fs \u2013put purchases.txt input\n\n\n\nPour afficher le contenu du re\u0301pertoire \ninput\n, la commande est:\n\n  \n  hadoop fs \u2013ls input\n\n\n\nPour afficher les derni\u00e8res lignes du fichier purchases:\n\n  \n  hadoop fs -tail input/purchases.txt\n\n\n\n\n\nLe r\u00e9sultat suivant va donc s'afficher:\n\n    \n\n\nNous pr\u00e9sentons dans le tableau suivant les commandes les plus utilis\u00e9es pour manipuler les fichiers dans HDFS:\n\n\n\n\n\n\n\n\nInstruction\n\n\nFonctionnalit\u00e9\n\n\n\n\n\n\n\n\n\n\nhadoop fs \u2013ls\n\n\nAfficher le contenu du re\u0301pertoire racine\n\n\n\n\n\n\nhadoop fs \u2013put file.txt\n\n\nUpload un fichier dans hadoop (a\u0300 partir du re\u0301pertoire courant linux)\n\n\n\n\n\n\nhadoop fs \u2013get file.txt\n\n\nDownload un fichier a\u0300 partir de hadoop sur votre disque local\n\n\n\n\n\n\nhadoop fs \u2013tail file.txt\n\n\nLire les dernie\u0300res lignes du fichier\n\n\n\n\n\n\nhadoop fs \u2013cat file.txt\n\n\nAffiche tout le contenu du fichier\n\n\n\n\n\n\nhadoop fs \u2013mv file.txt newfile.txt\n\n\nRenommer le fichier\n\n\n\n\n\n\nhadoop fs \u2013rm newfile.txt\n\n\nSupprimer le fichier\n\n\n\n\n\n\nhadoop fs \u2013mkdir myinput\n\n\nCre\u0301er un re\u0301pertoire\n\n\n\n\n\n\nhadoop fs \u2013cat file.txt \\| less\n\n\nLire le fichier page par page\n\n\n\n\n\n\n\n\nInterfaces web pour Hadoop\n\n\nHadoop offre plusieurs interfaces web pour pouvoir observer le comportement de ses diff\u00e9rentes composantes. Vous pouvez afficher ces pages en local sur votre machine gr\u00e2ce \u00e0 l'option -p de la commande \ndocker run\n. En effet, cette option permet de publier un port du contenaire sur la machine h\u00f4te. Pour pouvoir publier tous les ports expos\u00e9s, vous pouvez lancer votre contenaire en utilisant l'option \n-P\n.\n\n\nEn regardant le contenu du fichier \nstart-container.sh\n fourni dans le projet, vous verrez que deux ports de la machine ma\u00eetre ont \u00e9t\u00e9 expos\u00e9s:\n\n\n\n\nLe port \n50070\n: qui permet d'afficher les informations de votre namenode.\n\n\nLe port \n8088\n: qui permet d'afficher les informations du resource manager de Yarn et visualiser le comportement des diff\u00e9rents jobs.\n\n\n\n\nUne fois votre cluster lanc\u00e9 et pr\u00eat \u00e0 l'emploi, vous pouvez, sur votre navigateur pr\u00e9f\u00e9r\u00e9 de votre machine h\u00f4te, aller \u00e0 : \nhttp://localhost:50070\n. Vous obtiendrez le r\u00e9sultat suivant:\n\n\n\n\nVous pouvez \u00e9galement visualiser l'avancement et les r\u00e9sultats de vos Jobs (Map Reduce ou autre) en allant \u00e0 l'adresse: \nhttp://localhost:8088\n\n\n\n\nMap Reduce\n\n\nPr\u00e9sentation\n\n\nUn Job Map-Reduce se compose principalement de deux types de programmes:\n\n\n\n\nMappers\n : permettent d\u2019extraire les donne\u0301es ne\u0301cessaires sous forme de clef/valeur, pour pouvoir ensuite les trier selon la clef\n\n\nReducers\n : prennent un ensemble de donne\u0301es trie\u0301es selon leur clef, et effectuent le traitement ne\u0301cessaire sur ces donne\u0301es (somme, moyenne, total...)\n\n\n\n\nWordcount\n\n\nNous allons tester un programme MapReduce gr\u00e2ce \u00e0 un exemple tr\u00e8s simple, le \nWordCount\n, l'\u00e9quivalent du \nHelloWorld\n pour les applications de traitement de donn\u00e9es. Le Wordcount permet de calculer le nombre de mots dans un fichier donn\u00e9, en d\u00e9composant le calcul en deux \u00e9tapes:\n\n\n\n\nL'\u00e9tape de \nMapping\n, qui permet de d\u00e9couper le texte en mots et de d\u00e9livrer en sortie un flux textuel, o\u00f9 chaque ligne contient le mot trouv\u00e9, suivi de la valeur 1 (pour dire que le mot a \u00e9t\u00e9 trouv\u00e9 une fois)\n\n\nL'\u00e9tape de \nReducing\n, qui permet de faire la somme des 1 pour chaque mot, pour trouver le nombre total d'occurrences de ce mot dans le texte.\n\n\n\n\nCommen\u00e7ons par cr\u00e9er un projet Maven dans IntelliJ IDEA. Nous utiliserons dans notre cas JDK 1.8.\n\n\n\n\nD\u00e9finir les valeurs suivantes pour votre projet:\n\n\nGroupId\n: hadoop.mapreduce\n\n\nArtifactId\n: wordcount\n\n\nVersion\n: 1\n\n\n\n\n\n\nOuvrir le fichier \npom.xml\n, et ajouter les d\u00e9pendances suivantes pour Hadoop, HDFS et Map Reduce:\n\n\n\n\n  \ndependencies\n\n      \ndependency\n\n          \ngroupId\norg.apache.hadoop\n/groupId\n\n          \nartifactId\nhadoop-common\n/artifactId\n\n          \nversion\n2.7.2\n/version\n\n      \n/dependency\n\n      \n!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-mapreduce-client-core --\n\n      \ndependency\n\n          \ngroupId\norg.apache.hadoop\n/groupId\n\n          \nartifactId\nhadoop-mapreduce-client-core\n/artifactId\n\n          \nversion\n2.7.2\n/version\n\n      \n/dependency\n\n      \n!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-hdfs --\n\n      \ndependency\n\n          \ngroupId\norg.apache.hadoop\n/groupId\n\n          \nartifactId\nhadoop-hdfs\n/artifactId\n\n          \nversion\n2.7.2\n/version\n\n      \n/dependency\n\n      \ndependency\n\n            \ngroupId\norg.apache.hadoop\n/groupId\n\n            \nartifactId\nhadoop-mapreduce-client-common\n/artifactId\n\n            \nversion\n2.7.2\n/version\n\n        \n/dependency\n\n  \n/dependencies\n\n\n\n\n\n\n\nCr\u00e9er un package \ntn.insat.tp1\n sous le r\u00e9pertoire \nsrc/main/java\n\n\nCr\u00e9er la classe \nTokenizerMapper\n, contenant ce code:\n\n\n\n\n  \npackage\n \ntn.insat.tp1\n;\n\n\n  \nimport\n \norg.apache.hadoop.io.IntWritable\n;\n\n  \nimport\n \norg.apache.hadoop.io.Text\n;\n\n  \nimport\n \norg.apache.hadoop.mapreduce.Mapper\n;\n\n\n  \nimport\n \njava.io.IOException\n;\n\n  \nimport\n \njava.util.StringTokenizer\n;\n\n\n  \npublic\n \nclass\n \nTokenizerMapper\n\n        \nextends\n \nMapper\nObject\n,\n \nText\n,\n \nText\n,\n \nIntWritable\n{\n\n\n    \nprivate\n \nfinal\n \nstatic\n \nIntWritable\n \none\n \n=\n \nnew\n \nIntWritable\n(\n1\n);\n\n    \nprivate\n \nText\n \nword\n \n=\n \nnew\n \nText\n();\n\n\n    \npublic\n \nvoid\n \nmap\n(\nObject\n \nkey\n,\n \nText\n \nvalue\n,\n \nMapper\n.\nContext\n \ncontext\n\n    \n)\n \nthrows\n \nIOException\n,\n \nInterruptedException\n \n{\n\n        \nStringTokenizer\n \nitr\n \n=\n \nnew\n \nStringTokenizer\n(\nvalue\n.\ntoString\n());\n\n        \nwhile\n \n(\nitr\n.\nhasMoreTokens\n())\n \n{\n\n            \nword\n.\nset\n(\nitr\n.\nnextToken\n());\n\n            \ncontext\n.\nwrite\n(\nword\n,\n \none\n);\n\n        \n}\n\n    \n}\n\n  \n}\n\n\n\n\n\n\n\nCr\u00e9er la classe \nIntSumReducer\n:\n\n\n\n\npackage\n \ntn.insat.tp1\n;\n\n\n\nimport\n \norg.apache.hadoop.io.IntWritable\n;\n\n\nimport\n \norg.apache.hadoop.io.Text\n;\n\n\nimport\n \norg.apache.hadoop.mapreduce.Reducer\n;\n\n\n\nimport\n \njava.io.IOException\n;\n\n\n\npublic\n \nclass\n \nIntSumReducer\n\n        \nextends\n \nReducer\nText\n,\nIntWritable\n,\nText\n,\nIntWritable\n \n{\n\n\n    \nprivate\n \nIntWritable\n \nresult\n \n=\n \nnew\n \nIntWritable\n();\n\n\n    \npublic\n \nvoid\n \nreduce\n(\nText\n \nkey\n,\n \nIterable\nIntWritable\n \nvalues\n,\n\n                       \nContext\n \ncontext\n\n    \n)\n \nthrows\n \nIOException\n,\n \nInterruptedException\n \n{\n\n        \nint\n \nsum\n \n=\n \n0\n;\n\n        \nfor\n \n(\nIntWritable\n \nval\n \n:\n \nvalues\n)\n \n{\n\n            \nSystem\n.\nout\n.\nprintln\n(\nvalue: \n+\nval\n.\nget\n());\n\n            \nsum\n \n+=\n \nval\n.\nget\n();\n\n        \n}\n\n        \nSystem\n.\nout\n.\nprintln\n(\n--\n Sum = \n+\nsum\n);\n\n        \nresult\n.\nset\n(\nsum\n);\n\n        \ncontext\n.\nwrite\n(\nkey\n,\n \nresult\n);\n\n    \n}\n\n\n}\n\n\n\n\n\n\n\nEnfin, cr\u00e9er la classe \nWordCount\n:\n\n\n\n\npackage\n \ntn.insat.tp1\n;\n\n\n\nimport\n \norg.apache.hadoop.conf.Configuration\n;\n\n\nimport\n \norg.apache.hadoop.fs.Path\n;\n\n\nimport\n \norg.apache.hadoop.io.IntWritable\n;\n\n\nimport\n \norg.apache.hadoop.io.Text\n;\n\n\nimport\n \norg.apache.hadoop.mapreduce.Job\n;\n\n\nimport\n \norg.apache.hadoop.mapreduce.lib.input.FileInputFormat\n;\n\n\nimport\n \norg.apache.hadoop.mapreduce.lib.output.FileOutputFormat\n;\n\n\n\npublic\n \nclass\n \nWordCount\n \n{\n\n    \npublic\n \nstatic\n \nvoid\n \nmain\n(\nString\n[]\n \nargs\n)\n \nthrows\n \nException\n \n{\n\n        \nConfiguration\n \nconf\n \n=\n \nnew\n \nConfiguration\n();\n\n        \nJob\n \njob\n \n=\n \nJob\n.\ngetInstance\n(\nconf\n,\n \nword count\n);\n\n        \njob\n.\nsetJarByClass\n(\nWordCount\n.\nclass\n);\n\n        \njob\n.\nsetMapperClass\n(\nTokenizerMapper\n.\nclass\n);\n\n        \njob\n.\nsetCombinerClass\n(\nIntSumReducer\n.\nclass\n);\n\n        \njob\n.\nsetReducerClass\n(\nIntSumReducer\n.\nclass\n);\n\n        \njob\n.\nsetOutputKeyClass\n(\nText\n.\nclass\n);\n\n        \njob\n.\nsetOutputValueClass\n(\nIntWritable\n.\nclass\n);\n\n        \nFileInputFormat\n.\naddInputPath\n(\njob\n,\n \nnew\n \nPath\n(\nargs\n[\n0\n]));\n\n        \nFileOutputFormat\n.\nsetOutputPath\n(\njob\n,\n \nnew\n \nPath\n(\nargs\n[\n1\n]));\n\n        \nSystem\n.\nexit\n(\njob\n.\nwaitForCompletion\n(\ntrue\n)\n \n?\n \n0\n \n:\n \n1\n);\n\n    \n}\n\n\n}\n\n\n\n\n\nTester Map Reduce en local\n\n\nDans votre projet sur IntelliJ:\n\n\n\n\nCr\u00e9er un r\u00e9pertoire \ninput\n sous le r\u00e9pertoire \nresources\n de votre projet.\n\n\nCr\u00e9er un fichier de test: \nfile.txt\n dans lequel vous ins\u00e8rerez les deux lignes:\n\n\n  Hello Wordcount!\n  Hello Hadoop!\n\n\n\nCr\u00e9er une configuration de type \nApplication\n (\nRun-\nEdit Configurations...-\n+-\nApplication\n).\n\n\nD\u00e9finir comme \nMain Class\n: tn.insat.tp1.WordCount, et comme \nProgram Arguments\n: \nsrc/main/resources/input/file.txt src/main/resources/output\n\n\nLancer le programme. Un r\u00e9pertoire \noutput\n sera cr\u00e9\u00e9 dans le r\u00e9pertoire \nresources\n, contenant notamment un fichier \npart-r-00000\n, dont le contenu devrait \u00eatre le suivant:\n\n\n\n\nHadoop! 1\nHello   2\nWordcount!  1\n\n\n\n\nLancer Map Reduce sur le cluster\n\n\nDans votre projet IntelliJ:\n\n\n\n\nCr\u00e9er une configuration Maven avec la ligne de commande: \npackage install\n\n\nLancer la configuration. Un fichier \nwordcount-1.jar\n sera cr\u00e9\u00e9 dans le r\u00e9pertoire \ntarget\n du projet.\n\n\n\n\nCopier le fichier jar cr\u00e9\u00e9 dans le contenaire master. Pour cela:\n\n\n\n\nOuvrir le terminal sur le r\u00e9pertoire du projet. Cela peut \u00eatre fait avec IntelliJ en ouvrant la vue \nTerminal\n situ\u00e9e en bas \u00e0 gauche de la fen\u00eatre principale.\n\n\n\n\nTaper la commande suivante:\n\n\n  docker cp target/wordcount-1.jar hadoop-master:/root/wordcount-1.jar\n\n\n\n\n\n\n\n\n\nRevenir au shell du contenaire master, et lancer le job map reduce avec cette commande:\n\n\n\n\n\n\n  hadoop jar wordcount-1.jar tn.insat.tp1.WordCount input output\n\n\n\n\nLe Job sera lanc\u00e9 sur le fichier \npurchases.txt\n que vous aviez pr\u00e9alablement charg\u00e9 dans le r\u00e9pertoire \ninput\n de HDFS. Une fois le Job termin\u00e9, un r\u00e9pertoire \noutput\n sera cr\u00e9\u00e9. Si tout se passe bien, vous obtiendrez un affichage ressemblant au suivant:\n\n\n\n\nEn affichant les derni\u00e8res lignes du fichier g\u00e9n\u00e9r\u00e9 \noutput/part-r-00000\n, avec \nhadoop fs -tail output/part-r-00000\n, vous obtiendrez l'affichage suivant:\n\n\n\n\nIl vous est possible de monitorer vos Jobs Map Reduce, en allant \u00e0 la page: \nhttp://localhost:8088\n. Vous trouverez votre Job dans la liste des applications comme suit:\n\n\n\n\nIl est \u00e9galement possible de voir le comportement des noeuds esclaves, en allant \u00e0 l'adresse: \nhttp://localhost:8041\n pour \nslave1\n, et \nhttp://localhost:8042\n pour \nslave2\n. Vous obtiendrez ce qui suit:\n\n\n\n\n\n\nApplication\n\n\n\u00c9crire un Job Map Reduce permettant, \u00e0 partir du fichier purchases initial, de de\u0301terminer le total des ventes par magasin. La structure du fichier purchases est de la forme suivante:\n\n\n  date   temps   magasin   produit   cout   paiement\n\n\nVeiller \u00e0 toujours tester votre code en local avant de lancer un job sur le cluster!\n\n\n\n\nHomework\n\n\nPour la s\u00e9ance prochaine, l'objectif est d'utiliser un cluster AWS-EMR (Elastic Map Reduce) de Amazon pour ex\u00e9cuter un Job Map Reduce de votre choix sur un vrai cluster distribu\u00e9. Pour cela, utiliser les comptes \nRosettaHub\n qui vous ont \u00e9t\u00e9 fournis.", 
            "title": "TP1"
        }, 
        {
            "location": "/tp1/#tp1-le-traitement-batch-avec-hadoop-hdfs-et-map-reduce", 
            "text": "", 
            "title": "TP1 - Le traitement Batch avec Hadoop HDFS et Map Reduce"
        }, 
        {
            "location": "/tp1/#telecharger-pdf", 
            "text": "", 
            "title": "T\u00e9l\u00e9charger PDF"
        }, 
        {
            "location": "/tp1/#objectifs-du-tp", 
            "text": "Initiation au framework hadoop et au patron MapReduce, utilisation de docker pour lancer un cluster hadoop de 3 noeuds.", 
            "title": "Objectifs du TP"
        }, 
        {
            "location": "/tp1/#outils-et-versions", 
            "text": "Apache Hadoop  Version: 2.7.2.  Docker  Version 17.09.1  IntelliJ IDEA  Version Ultimate 2016.1 (ou tout autre IDE de votre choix)  Java  Version 1.8.  Unix-like ou Unix-based Systems (Divers Linux et MacOS)", 
            "title": "Outils et Versions"
        }, 
        {
            "location": "/tp1/#hadoop", 
            "text": "", 
            "title": "Hadoop"
        }, 
        {
            "location": "/tp1/#presentation", 
            "text": "Apache Hadoop  est un framework open-source pour stocker et traiter les donne\u0301es volumineuses sur un cluster. Il est utilise\u0301 par un grand nombre de contributeurs et utilisateurs. Il a une licence Apache 2.0.", 
            "title": "Pr\u00e9sentation"
        }, 
        {
            "location": "/tp1/#hadoop-et-docker", 
            "text": "Pour d\u00e9ployer le framework Hadoop, nous allons utiliser des contenaires  Docker . L'utilisation des contenaires va garantir la consistance entre les environnements de d\u00e9veloppement et permettra de r\u00e9duire consid\u00e9rablement la complexit\u00e9 de configuration des machines (dans le cas d'un acc\u00e8s natif) ainsi que la lourdeur d'ex\u00e9cution (si on opte pour l'utilisation d'une machine virtuelle).  Nous avons pour le d\u00e9ploiement des ressources de ce TP suivi les instructions pr\u00e9sent\u00e9es  ici .", 
            "title": "Hadoop et Docker"
        }, 
        {
            "location": "/tp1/#installation", 
            "text": "Nous allons utiliser tout au long de ce TP trois contenaires repr\u00e9sentant respectivement un noeud ma\u00eetre (Namenode) et deux noeuds esclaves (Datanodes).  Vous devez pour cela avoir install\u00e9 docker sur votre machine, et l'avoir correctement configur\u00e9. Ouvrir la ligne de commande, et taper les instructions suivantes:   Cloner le repo github contenant les fichiers n\u00e9cessaires pour le lancement des contenaires et leur configuration:    git clone https://github.com/liliasfaxi/hadoop-cluster-docker  Construire l'image Docker \u00e0 partir du fichier Dockerfile fourni.     cd  hadoop-cluster-docker\n  ./build-image.sh  D\u00e9marrer les trois contenaires:    sudo ./start-container.sh    Attention  Le script  start-container.sh  va r\u00e9initialiser les trois contenaires. Si vous voulez red\u00e9marrer un contenaire d\u00e9j\u00e0 cr\u00e9\u00e9, il ne faut pas l'ex\u00e9cuter de nouveau: tout sera effac\u00e9. Au lieu de cela, utiliser plut\u00f4t  docker start  container_id> . Pour lancer le shell, taper simplement (pour le master container, par exemple):      docker  exec  -it hadoop-master bash   Le r\u00e9sultat de cette ex\u00e9cution sera le suivant:    start hadoop-master container...\n  start hadoop-slave1 container...\n  start hadoop-slave2 container...\n  root@hadoop-master:~#  Vous vous retrouverez dans le shell du namenode, et vous pourrez ainsi manipuler le cluster \u00e0 votre guise. La premi\u00e8re chose \u00e0 faire, une fois dans le contenaire, est de lancer hadoop et yarn. Un script est fourni pour cela, appel\u00e9  start-hadoop.sh . Lancer ce script.    ./start-hadoop.sh  Le r\u00e9sultat devra ressembler \u00e0 ce qui suit:", 
            "title": "Installation"
        }, 
        {
            "location": "/tp1/#premiers-pas-avec-hadoop", 
            "text": "Toutes les commandes interagissant avec le syste\u0300me Hadoop commencent par hadoop fs. Ensuite, les options rajoute\u0301es sont tre\u0300s largement inspire\u0301es des commandes Unix standard.   Cre\u0301er un re\u0301pertoire dans HDFS, appele\u0301  input . Pour cela, taper:    hadoop fs \u2013mkdir -p input    Erreur  Si pour une raison ou une autre, vous n'arrivez pas \u00e0 cr\u00e9er le r\u00e9pertoire  input , avec un message ressemblant \u00e0 ceci:  ls: `.': No such file or directory , veiller \u00e0 construire l'arborescence de l'utilisateur principal (root), comme suit:  hadoop fs -mkdir -p /user/root    Nous allons utiliser le fichier   purchases.txt  comme entr\u00e9e pour le traitement MapReduce. Ce fichier se trouve d\u00e9j\u00e0 sous le r\u00e9pertoire principal de votre machine master.  Charger le fichier purchases dans le r\u00e9pertoire input que vous avez cr\u00e9\u00e9: \n     hadoop fs \u2013put purchases.txt input  Pour afficher le contenu du re\u0301pertoire  input , la commande est: \n     hadoop fs \u2013ls input  Pour afficher les derni\u00e8res lignes du fichier purchases: \n     hadoop fs -tail input/purchases.txt   Le r\u00e9sultat suivant va donc s'afficher: \n      Nous pr\u00e9sentons dans le tableau suivant les commandes les plus utilis\u00e9es pour manipuler les fichiers dans HDFS:     Instruction  Fonctionnalit\u00e9      hadoop fs \u2013ls  Afficher le contenu du re\u0301pertoire racine    hadoop fs \u2013put file.txt  Upload un fichier dans hadoop (a\u0300 partir du re\u0301pertoire courant linux)    hadoop fs \u2013get file.txt  Download un fichier a\u0300 partir de hadoop sur votre disque local    hadoop fs \u2013tail file.txt  Lire les dernie\u0300res lignes du fichier    hadoop fs \u2013cat file.txt  Affiche tout le contenu du fichier    hadoop fs \u2013mv file.txt newfile.txt  Renommer le fichier    hadoop fs \u2013rm newfile.txt  Supprimer le fichier    hadoop fs \u2013mkdir myinput  Cre\u0301er un re\u0301pertoire    hadoop fs \u2013cat file.txt \\| less  Lire le fichier page par page", 
            "title": "Premiers pas avec Hadoop"
        }, 
        {
            "location": "/tp1/#interfaces-web-pour-hadoop", 
            "text": "Hadoop offre plusieurs interfaces web pour pouvoir observer le comportement de ses diff\u00e9rentes composantes. Vous pouvez afficher ces pages en local sur votre machine gr\u00e2ce \u00e0 l'option -p de la commande  docker run . En effet, cette option permet de publier un port du contenaire sur la machine h\u00f4te. Pour pouvoir publier tous les ports expos\u00e9s, vous pouvez lancer votre contenaire en utilisant l'option  -P .  En regardant le contenu du fichier  start-container.sh  fourni dans le projet, vous verrez que deux ports de la machine ma\u00eetre ont \u00e9t\u00e9 expos\u00e9s:   Le port  50070 : qui permet d'afficher les informations de votre namenode.  Le port  8088 : qui permet d'afficher les informations du resource manager de Yarn et visualiser le comportement des diff\u00e9rents jobs.   Une fois votre cluster lanc\u00e9 et pr\u00eat \u00e0 l'emploi, vous pouvez, sur votre navigateur pr\u00e9f\u00e9r\u00e9 de votre machine h\u00f4te, aller \u00e0 :  http://localhost:50070 . Vous obtiendrez le r\u00e9sultat suivant:   Vous pouvez \u00e9galement visualiser l'avancement et les r\u00e9sultats de vos Jobs (Map Reduce ou autre) en allant \u00e0 l'adresse:  http://localhost:8088", 
            "title": "Interfaces web pour Hadoop"
        }, 
        {
            "location": "/tp1/#map-reduce", 
            "text": "", 
            "title": "Map Reduce"
        }, 
        {
            "location": "/tp1/#presentation_1", 
            "text": "Un Job Map-Reduce se compose principalement de deux types de programmes:   Mappers  : permettent d\u2019extraire les donne\u0301es ne\u0301cessaires sous forme de clef/valeur, pour pouvoir ensuite les trier selon la clef  Reducers  : prennent un ensemble de donne\u0301es trie\u0301es selon leur clef, et effectuent le traitement ne\u0301cessaire sur ces donne\u0301es (somme, moyenne, total...)", 
            "title": "Pr\u00e9sentation"
        }, 
        {
            "location": "/tp1/#wordcount", 
            "text": "Nous allons tester un programme MapReduce gr\u00e2ce \u00e0 un exemple tr\u00e8s simple, le  WordCount , l'\u00e9quivalent du  HelloWorld  pour les applications de traitement de donn\u00e9es. Le Wordcount permet de calculer le nombre de mots dans un fichier donn\u00e9, en d\u00e9composant le calcul en deux \u00e9tapes:   L'\u00e9tape de  Mapping , qui permet de d\u00e9couper le texte en mots et de d\u00e9livrer en sortie un flux textuel, o\u00f9 chaque ligne contient le mot trouv\u00e9, suivi de la valeur 1 (pour dire que le mot a \u00e9t\u00e9 trouv\u00e9 une fois)  L'\u00e9tape de  Reducing , qui permet de faire la somme des 1 pour chaque mot, pour trouver le nombre total d'occurrences de ce mot dans le texte.   Commen\u00e7ons par cr\u00e9er un projet Maven dans IntelliJ IDEA. Nous utiliserons dans notre cas JDK 1.8.   D\u00e9finir les valeurs suivantes pour votre projet:  GroupId : hadoop.mapreduce  ArtifactId : wordcount  Version : 1    Ouvrir le fichier  pom.xml , et ajouter les d\u00e9pendances suivantes pour Hadoop, HDFS et Map Reduce:      dependencies \n       dependency \n           groupId org.apache.hadoop /groupId \n           artifactId hadoop-common /artifactId \n           version 2.7.2 /version \n       /dependency \n       !-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-mapreduce-client-core -- \n       dependency \n           groupId org.apache.hadoop /groupId \n           artifactId hadoop-mapreduce-client-core /artifactId \n           version 2.7.2 /version \n       /dependency \n       !-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-hdfs -- \n       dependency \n           groupId org.apache.hadoop /groupId \n           artifactId hadoop-hdfs /artifactId \n           version 2.7.2 /version \n       /dependency \n       dependency \n             groupId org.apache.hadoop /groupId \n             artifactId hadoop-mapreduce-client-common /artifactId \n             version 2.7.2 /version \n         /dependency \n   /dependencies    Cr\u00e9er un package  tn.insat.tp1  sous le r\u00e9pertoire  src/main/java  Cr\u00e9er la classe  TokenizerMapper , contenant ce code:      package   tn.insat.tp1 ; \n\n   import   org.apache.hadoop.io.IntWritable ; \n   import   org.apache.hadoop.io.Text ; \n   import   org.apache.hadoop.mapreduce.Mapper ; \n\n   import   java.io.IOException ; \n   import   java.util.StringTokenizer ; \n\n   public   class   TokenizerMapper \n         extends   Mapper Object ,   Text ,   Text ,   IntWritable { \n\n     private   final   static   IntWritable   one   =   new   IntWritable ( 1 ); \n     private   Text   word   =   new   Text (); \n\n     public   void   map ( Object   key ,   Text   value ,   Mapper . Context   context \n     )   throws   IOException ,   InterruptedException   { \n         StringTokenizer   itr   =   new   StringTokenizer ( value . toString ()); \n         while   ( itr . hasMoreTokens ())   { \n             word . set ( itr . nextToken ()); \n             context . write ( word ,   one ); \n         } \n     } \n   }    Cr\u00e9er la classe  IntSumReducer :   package   tn.insat.tp1 ;  import   org.apache.hadoop.io.IntWritable ;  import   org.apache.hadoop.io.Text ;  import   org.apache.hadoop.mapreduce.Reducer ;  import   java.io.IOException ;  public   class   IntSumReducer \n         extends   Reducer Text , IntWritable , Text , IntWritable   { \n\n     private   IntWritable   result   =   new   IntWritable (); \n\n     public   void   reduce ( Text   key ,   Iterable IntWritable   values , \n                        Context   context \n     )   throws   IOException ,   InterruptedException   { \n         int   sum   =   0 ; \n         for   ( IntWritable   val   :   values )   { \n             System . out . println ( value:  + val . get ()); \n             sum   +=   val . get (); \n         } \n         System . out . println ( --  Sum =  + sum ); \n         result . set ( sum ); \n         context . write ( key ,   result ); \n     }  }    Enfin, cr\u00e9er la classe  WordCount :   package   tn.insat.tp1 ;  import   org.apache.hadoop.conf.Configuration ;  import   org.apache.hadoop.fs.Path ;  import   org.apache.hadoop.io.IntWritable ;  import   org.apache.hadoop.io.Text ;  import   org.apache.hadoop.mapreduce.Job ;  import   org.apache.hadoop.mapreduce.lib.input.FileInputFormat ;  import   org.apache.hadoop.mapreduce.lib.output.FileOutputFormat ;  public   class   WordCount   { \n     public   static   void   main ( String []   args )   throws   Exception   { \n         Configuration   conf   =   new   Configuration (); \n         Job   job   =   Job . getInstance ( conf ,   word count ); \n         job . setJarByClass ( WordCount . class ); \n         job . setMapperClass ( TokenizerMapper . class ); \n         job . setCombinerClass ( IntSumReducer . class ); \n         job . setReducerClass ( IntSumReducer . class ); \n         job . setOutputKeyClass ( Text . class ); \n         job . setOutputValueClass ( IntWritable . class ); \n         FileInputFormat . addInputPath ( job ,   new   Path ( args [ 0 ])); \n         FileOutputFormat . setOutputPath ( job ,   new   Path ( args [ 1 ])); \n         System . exit ( job . waitForCompletion ( true )   ?   0   :   1 ); \n     }  }", 
            "title": "Wordcount"
        }, 
        {
            "location": "/tp1/#tester-map-reduce-en-local", 
            "text": "Dans votre projet sur IntelliJ:   Cr\u00e9er un r\u00e9pertoire  input  sous le r\u00e9pertoire  resources  de votre projet.  Cr\u00e9er un fichier de test:  file.txt  dans lequel vous ins\u00e8rerez les deux lignes:    Hello Wordcount!\n  Hello Hadoop!  Cr\u00e9er une configuration de type  Application  ( Run- Edit Configurations...- +- Application ).  D\u00e9finir comme  Main Class : tn.insat.tp1.WordCount, et comme  Program Arguments :  src/main/resources/input/file.txt src/main/resources/output  Lancer le programme. Un r\u00e9pertoire  output  sera cr\u00e9\u00e9 dans le r\u00e9pertoire  resources , contenant notamment un fichier  part-r-00000 , dont le contenu devrait \u00eatre le suivant:   Hadoop! 1\nHello   2\nWordcount!  1", 
            "title": "Tester Map Reduce en local"
        }, 
        {
            "location": "/tp1/#lancer-map-reduce-sur-le-cluster", 
            "text": "Dans votre projet IntelliJ:   Cr\u00e9er une configuration Maven avec la ligne de commande:  package install  Lancer la configuration. Un fichier  wordcount-1.jar  sera cr\u00e9\u00e9 dans le r\u00e9pertoire  target  du projet.   Copier le fichier jar cr\u00e9\u00e9 dans le contenaire master. Pour cela:   Ouvrir le terminal sur le r\u00e9pertoire du projet. Cela peut \u00eatre fait avec IntelliJ en ouvrant la vue  Terminal  situ\u00e9e en bas \u00e0 gauche de la fen\u00eatre principale.   Taper la commande suivante:    docker cp target/wordcount-1.jar hadoop-master:/root/wordcount-1.jar     Revenir au shell du contenaire master, et lancer le job map reduce avec cette commande:      hadoop jar wordcount-1.jar tn.insat.tp1.WordCount input output  Le Job sera lanc\u00e9 sur le fichier  purchases.txt  que vous aviez pr\u00e9alablement charg\u00e9 dans le r\u00e9pertoire  input  de HDFS. Une fois le Job termin\u00e9, un r\u00e9pertoire  output  sera cr\u00e9\u00e9. Si tout se passe bien, vous obtiendrez un affichage ressemblant au suivant:   En affichant les derni\u00e8res lignes du fichier g\u00e9n\u00e9r\u00e9  output/part-r-00000 , avec  hadoop fs -tail output/part-r-00000 , vous obtiendrez l'affichage suivant:   Il vous est possible de monitorer vos Jobs Map Reduce, en allant \u00e0 la page:  http://localhost:8088 . Vous trouverez votre Job dans la liste des applications comme suit:   Il est \u00e9galement possible de voir le comportement des noeuds esclaves, en allant \u00e0 l'adresse:  http://localhost:8041  pour  slave1 , et  http://localhost:8042  pour  slave2 . Vous obtiendrez ce qui suit:    Application  \u00c9crire un Job Map Reduce permettant, \u00e0 partir du fichier purchases initial, de de\u0301terminer le total des ventes par magasin. La structure du fichier purchases est de la forme suivante:    date   temps   magasin   produit   cout   paiement \nVeiller \u00e0 toujours tester votre code en local avant de lancer un job sur le cluster!", 
            "title": "Lancer Map Reduce sur le cluster"
        }, 
        {
            "location": "/tp1/#homework", 
            "text": "Pour la s\u00e9ance prochaine, l'objectif est d'utiliser un cluster AWS-EMR (Elastic Map Reduce) de Amazon pour ex\u00e9cuter un Job Map Reduce de votre choix sur un vrai cluster distribu\u00e9. Pour cela, utiliser les comptes  RosettaHub  qui vous ont \u00e9t\u00e9 fournis.", 
            "title": "Homework"
        }, 
        {
            "location": "/tp2/", 
            "text": "TP2 - Traitement par Lot et Streaming avec Spark\n\n\n\n\nT\u00e9l\u00e9charger PDF\n\n\n\n\nObjectifs du TP\n\n\nUtilisation de Spark pour r\u00e9aliser des traitements par lot et des traitements en streaming.\n\n\nOutils et Versions\n\n\n\n\nApache Hadoop\n Version: 2.7.2\n\n\nApache Spark\n Version: 2.2.1\n\n\nDocker\n Version 17.09.1\n\n\nIntelliJ IDEA\n Version Ultimate 2016.1 (ou tout autre IDE de votre choix)\n\n\nJava\n Version 1.8\n\n\nUnix-like ou Unix-based Systems (Divers Linux et MacOS)\n\n\n\n\nSpark\n\n\nPr\u00e9sentation\n\n\nSpark\n est un syst\u00e8me de traitement rapide et parall\u00e8le. Il fournit des APIs de haut niveau en Java, Scala, Python et R, et un moteur optimis\u00e9 qui supporte l'ex\u00e9cution des graphes. Il supporte \u00e9galement un ensemble d'outils de haut niveau tels que \nSpark SQL\n pour le support du traitement de donn\u00e9es structur\u00e9es, \nMLlib\n pour l'apprentissage des donn\u00e9es, \nGraphX\n pour le traitement des graphes, et \nSpark Streaming\n pour le traitment des donn\u00e9es en streaming.\n\n\n\n\n\n\nSpark et Hadoop\n\n\nSpark peut s'ex\u00e9cuter sur plusieurs plateformes: Hadoop, Mesos, en standalone ou sur le cloud. Il peut \u00e9galement acc\u00e9der diverses sources de donn\u00e9es, comme HDFS, Cassandra, HBase et S3.\n\n\nDans ce TP, nous allons ex\u00e9cuter Spark sur Hadoop YARN. YARN s'occupera ainsi de la gestion des ressources pour le d\u00e9clenchement et l'ex\u00e9cution des Jobs Spark.\n\n\nInstallation\n\n\nNous avons proc\u00e9d\u00e9 \u00e0 l'installation de Spark sur le cluster Hadoop utilis\u00e9 dans le \nTP1\n. Voici les \u00e9tapes n\u00e9cessaires pour le lancer:\n\n\n\n\nCloner le repo github contenant les fichiers n\u00e9cessaires pour le lancement des contenaires et leur configuration:\n\n\n  git clone https://github.com/liliasfaxi/hadoop-cluster-docker\n\n\n\nConstruire l'image Docker \u00e0 partir du fichier Dockerfile fourni.\n\n\n  \ncd\n hadoop-cluster-docker\n  ./build-image.sh\n\n\n\nD\u00e9marrer les trois contenaires:\n\n\n  sudo ./start-container.sh\n\n\nLe r\u00e9sultat de cette ex\u00e9cution sera le suivant:\n\n\n  start hadoop-master container...\n  start hadoop-slave1 container...\n  start hadoop-slave2 container...\n  root@hadoop-master:~#\n\n\n\nLancer les d\u00e9mons yarn et hdfs en lan\u00e7ant:\n\n\n  ./start-hadoop.sh\n\n\n\n\n\nVous pourrez v\u00e9rifier que tous les d\u00e9mons sont lanc\u00e9s en tapant: \njps\n. Un r\u00e9sultat semblable au suivant pourra \u00eatre visible:\n\n\n  \n880\n Jps\n  \n257\n NameNode\n  \n613\n ResourceManager\n  \n456\n SecondaryNameNode\n\n\n\nLa m\u00eame op\u00e9ration sur les noeuds esclaves (auquels vous acc\u00e9dez \u00e0 partir de votre machine h\u00f4te en tapant \ndocker exec -it hadoop-slave1 bash\n) devrait donner:\n\n\n  \n176\n NodeManager\n  \n65\n DataNode\n  \n311\n Jps\n\n\n\nTest de Spark avec Spark-Shell\n\n\nDans le but de tester l'ex\u00e9cution de spark, commencer par cr\u00e9er un fichier \nfile1.txt\n dans votre noeud master, contenant le texte suivant:\n\n\n  Hello Spark Wordcount!\n  Hello Hadoop Also :)\n\n\n\nCharger ensuite ce fichier dans HDFS:\n\n\n  hadoop fs -put file1.txt\n\n\n\nPour v\u00e9rifier que spark est bien install\u00e9, taper la commande suivante:\n\n\n  spark-shell\n\n\n\nVous devriez avoir un r\u00e9sultat semblable au suivant:\n\n\n\n\nVous pourrez tester spark avec un code scala simple comme suit (\u00e0 ex\u00e9cuter ligne par ligne):\n\n\n  \nval\n \nlines\n \n=\n \nsc\n.\ntextFile\n(\nfile1.txt\n)\n\n  \nval\n \nwords\n \n=\n \nlines\n.\nflatMap\n(\n_\n.\nsplit\n(\n\\\\s+\n))\n\n  \nval\n \nwc\n \n=\n \nwords\n.\nmap\n(\nw\n \n=\n \n(\nw\n,\n \n1\n)).\nreduceByKey\n(\n_\n \n+\n \n_\n)\n\n  \nwc\n.\nsaveAsTextFile\n(\nfile1.count\n)\n\n\n\n\n\nCe code vient de (1) charger le fichier \nfile1.txt\n de HDFS, (2) s\u00e9parer les mots selon les caract\u00e8res d'espacement, (3) appliquer un \nmap\n sur les mots obtenus qui produit le couple (\nmot>\n, 1), puis un \nreduce\n qui permet de faire la somme des 1 des mots identiques.\n\n\nPour afficher le r\u00e9sultat, sortir de spark-shell en cliquant sur \nCtrl-C\n. T\u00e9l\u00e9charger ensuite le r\u00e9pertoire \nfile1.count\n cr\u00e9\u00e9 dans HDFS comme suit:\n\n\n  hadoop fs -get file1.count\n\n\nLe contenu des deux fichiers \npart-00000\n et \npart-00001\n ressemble \u00e0 ce qui suit:\n\n\n\n\nL'API de Spark\n\n\nA un haut niveau d'abstraction, chaque application Spark consiste en un programme \ndriver\n qui ex\u00e9cute la fonction \nmain\n de l'utilisateur et lance plusieurs op\u00e9rations parall\u00e8les sur le cluster. L'abstraction principale fournie par Spark est un RDD (\nResilient Distributed Dataset\n), qui repr\u00e9sente une collection d'\u00e9l\u00e9ments partitionn\u00e9s \u00e0 travers les noeuds du cluster, et sur lesquelles on peut op\u00e9rer en parall\u00e8le. Les RDDs sont cr\u00e9\u00e9s \u00e0 partir d'un fichier dans HDFS par exemple, puis le transforment. Les utilisateurs peuvent demander \u00e0 Spark de sauvegarder un RDD en m\u00e9moire, lui permettant ainsi d'\u00eatre r\u00e9utilis\u00e9 efficacement \u00e0 travers plusieurs op\u00e9rations parall\u00e8les.\n\n\n\n\nLes RDDs supportent deux types d'op\u00e9rations:\n\n\n\n\nles \ntransformations\n, qui permettent de cr\u00e9er un nouveau Dataset \u00e0 partir d'un Dataset existant\n\n\nles \nactions\n, qui retournent une valeur au programme \ndriver\n ar\u00e8s avoir ex\u00e9cut\u00e9 un calcul sur le Dataset.\n\n\n\n\nPar exemple, un \nmap\n est une transformation qui passe chaque \u00e9l\u00e9ment du dataset via une fontion, et retourne un nouvel RDD repr\u00e9sentant les r\u00e9sultats. Un \nreduce\n est une action qui agr\u00e8ge tous les \u00e9l\u00e9ments du RDD en utilisant une certaine fonction et retourne le r\u00e9sultat final au programme.\n\n\nToutes les transformations dans Spark sont \nlazy\n, car elles ne calculent pas le r\u00e9sultat imm\u00e9diatement. Elles se souviennent des transformations appliqu\u00e9es \u00e0 un dataset de base (par ex. un fichier). Les transformations ne sont calcul\u00e9es que quand une action n\u00e9cessite qu'un r\u00e9sultat soit retourn\u00e9 au programme principal. Cela permet \u00e0 Spark de s'ex\u00e9cuter plus efficacement.\n\n\n\n\nExemple\n\n\nL'exemple que nous allons pr\u00e9senter ici par \u00e9tapes permet de relever les mots les plus fr\u00e9quents dans un fichier. Pour cela, le code suivant est utilis\u00e9:\n\n\n  \n//Etape 1 - Cr\u00e9er un RDD \u00e0 partir d\nun fichier texte de Hadoop\n\n  \nval\n \ndocs\n \n=\n \nspark\n.\ntextFile\n(\n/docs\n)\n\n\n\n\n\n\n  \n//Etape 2 - Convertir les lignes en minuscule\n\n  \nval\n \nlower\n \n=\n \ndocs\n.\nmap\n(\nline\n \n=\n \nline\n.\ntoLowerCase\n)\n\n\n\n\n\n\n  \n//Etape 3 - S\u00e9parer les lignes en mots\n\n  \nval\n \nwords\n \n=\n \nlower\n.\nflatMap\n(\nline\n \n=\n \nline\n.\nsplit\n(\n\\\\s+\n))\n\n\n\n\n\n\n  \n//Etape 4 - produire les tuples (mot, 1)\n\n  \nval\n \ncounts\n \n=\n \nwords\n.\nmap\n(\nword\n \n=\n \n(\nword\n,\n1\n))\n\n\n\n\n\n\n  \n//Etape 5 - Compter tous les mots\n\n  \nval\n \nfreq\n \n=\n \ncounts\n.\nreduceByKey\n(\n_\n \n+\n \n_\n)\n\n\n\n\n\n\n  \n//Etape 6 - Inverser les tuples (transformation avec swap)\n\n  \nfreq\n.\nmap\n(\n_\n.\nswap\n)\n\n\n\n\n\n\n  \n//Etape 6 - Inverser les tuples (action de s\u00e9lection des n premiers)\n\n  \nval\n \ntop\n \n=\n \nfreq\n.\nmap\n(\n_swap\n).\ntop\n(\nN\n)\n\n\n\n\n\n\nSpark Batch en Java\n\n\nPr\u00e9paration de l'environnement et Code\n\n\nNous allons dans cette partie cr\u00e9er un projet Spark Batch en Java (un simple WordCount), le charger sur le cluster et lancer le job.\n\n\n\n\nCr\u00e9er un projet Maven avec IntelliJ IDEA, en utilisant la config suivante:\n\n  \n  \ngroupId\nspark.batch\n/groupId\n\n  \nartifactId\nwordcount\n/artifactId\n\n  \nversion\n1\n/version\n\n\n\n\nRajouter dans le fichier pom les d\u00e9pendances n\u00e9cessaires, et indiquer la version du compilateur Java:\n\n  \n  \nproperties\n\n      \nmaven.compiler.source\n1.8\n/maven.compiler.source\n\n      \nmaven.compiler.target\n1.8\n/maven.compiler.target\n\n  \n/properties\n\n  \ndependencies\n\n      \ndependency\n\n          \ngroupId\norg.apache.spark\n/groupId\n\n          \nartifactId\nspark-core_2.11\n/artifactId\n\n          \nversion\n2.1.0\n/version\n\n      \n/dependency\n\n      \ndependency\n\n          \ngroupId\norg.slf4j\n/groupId\n\n          \nartifactId\nslf4j-log4j12\n/artifactId\n\n          \nversion\n1.7.22\n/version\n\n      \n/dependency\n\n  \n/dependencies\n\n\n\n\nSous le r\u00e9pertoire java, cr\u00e9er un package que vous appellerez \ntn.insat.tp21\n, et dedans, une classe appel\u00e9e \nWordCountTask\n.\n\n\n\n\n\u00c9crire le code suivant dans \nWordCountTask\n:\n\n  \npublic\n \nclass\n \nWordCountTask\n \n{\n\n      \nprivate\n \nstatic\n \nfinal\n \nLogger\n \nLOGGER\n \n=\n \nLoggerFactory\n.\ngetLogger\n(\nWordCountTask\n.\nclass\n);\n\n\n      \npublic\n \nstatic\n \nvoid\n \nmain\n(\nString\n[]\n \nargs\n)\n \n{\n\n          \ncheckArgument\n(\nargs\n.\nlength\n \n \n1\n,\n \nPlease provide the path of input file and output dir as parameters.\n);\n\n          \nnew\n \nWordCountTask\n().\nrun\n(\nargs\n[\n0\n],\n \nargs\n[\n1\n]);\n\n      \n}\n\n\n      \npublic\n \nvoid\n \nrun\n(\nString\n \ninputFilePath\n,\n \nString\n \noutputDir\n)\n \n{\n\n          \nString\n \nmaster\n \n=\n \nlocal[*]\n;\n\n          \nSparkConf\n \nconf\n \n=\n \nnew\n \nSparkConf\n()\n\n                  \n.\nsetAppName\n(\nWordCountTask\n.\nclass\n.\ngetName\n())\n\n                  \n.\nsetMaster\n(\nmaster\n);\n\n          \nJavaSparkContext\n \nsc\n \n=\n \nnew\n \nJavaSparkContext\n(\nconf\n);\n\n\n          \nJavaRDD\nString\n \ntextFile\n \n=\n \nsc\n.\ntextFile\n(\ninputFilePath\n);\n\n          \nJavaPairRDD\nString\n,\n \nInteger\n \ncounts\n \n=\n \ntextFile\n\n                  \n.\nflatMap\n(\ns\n \n-\n \nArrays\n.\nasList\n(\ns\n.\nsplit\n(\n \n)).\niterator\n())\n\n                  \n.\nmapToPair\n(\nword\n \n-\n \nnew\n \nTuple2\n(\nword\n,\n \n1\n))\n\n                  \n.\nreduceByKey\n((\na\n,\n \nb\n)\n \n-\n \na\n \n+\n \nb\n);\n\n          \ncounts\n.\nsaveAsTextFile\n(\noutputDir\n);\n\n      \n}\n\n  \n}\n\n\n\n  La premi\u00e8re chose \u00e0 faire dans un programme Spark est de cr\u00e9er un objet \nJavaSparkContext\n, qui indique \u00e0 Spark comment acc\u00e9der \u00e0 un cluster. Pour cr\u00e9er ce contexte, vous aurez besoin de construire un objet \nSparkConf\n qui contient toutes les informations sur l'application.\n\n\n\n\nappName\n: est de nom de l'application\n\n\nmaster\n est une URL d'un cluster Spark, Mesos ou YARN, ou bien une cha\u00eene sp\u00e9ciale \nlocal\n pour lancer le job en mode local.\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nNous avons indiqu\u00e9 ici que notre master est \nlocal\n pour les besoins du test, mais plus tard, en le packageant pour le cluster, nous allons enlever cette indication. Il est en effet d\u00e9conseill\u00e9 de la hard-coder dans le programme, il faudrait plut\u00f4t l'indiquer comme option de commande \u00e0 chaque fois que nous lan\u00e7ons le job.\n\n\nLe reste du code de l'application est la version en Java de l'exemple en scala que nous avions fait avec spark-shell.\n\n\n\n\nTest du code en local\n\n\nPour tester le code sur votre machine, proc\u00e9der aux \u00e9tapes suivantes:\n\n\n\n\nIns\u00e9rer un fichier texte de votre choix (par exemple le fameux \nloremipsum.txt\n) dans le r\u00e9pertoire src/main/resources.\n\n\nCr\u00e9er une nouvelle configuration de type \"Application\" (\nRun-\nEdit Configurations\n) que vous appellerez \nWordCountTask\n, et d\u00e9finir les arguments suivants (fichier de d\u00e9part et r\u00e9pertoire d'arriv\u00e9e) comme \nProgram arguments\n:\n\n  \n  src/main/resources/loremipsum.txt src/main/resources/out\n\n\n\nCliquer sur OK, et lancer la configuration. Si tout se passe bien, un r\u00e9pertoire \nout\n sera cr\u00e9\u00e9 sous \nresources\n, qui contient deux fichiers: part-00000, part-00001.\n\n\n\n\n\n\nLancement du code sur le cluster\n\n\nPour ex\u00e9cuter le code sur le cluster, modifier comme indiqu\u00e9 les lignes en jaune dans ce qui suit:\n\n\npublic\n \nclass\n \nWordCountTask\n \n{\n\n  \nprivate\n \nstatic\n \nfinal\n \nLogger\n \nLOGGER\n \n=\n \nLoggerFactory\n.\ngetLogger\n(\nWordCountTask\n.\nclass\n);\n\n\n  \npublic\n \nstatic\n \nvoid\n \nmain\n(\nString\n[]\n \nargs\n)\n \n{\n\n      \ncheckArgument\n(\nargs\n.\nlength\n \n \n1\n,\n \nPlease provide the path of input file and output dir as parameters.\n);\n\n      \nnew\n \nWordCountTask\n().\nrun\n(\nargs\n[\n0\n],\n \nargs\n[\n1\n]);\n\n  \n}\n\n\n  \npublic\n \nvoid\n \nrun\n(\nString\n \ninputFilePath\n,\n \nString\n \noutputDir\n)\n \n{\n\n\n\n      \nSparkConf\n \nconf\n \n=\n \nnew\n \nSparkConf\n()\n\n\n              \n.\nsetAppName\n(\nWordCountTask\n.\nclass\n.\ngetName\n());\n\n\n\n      \nJavaSparkContext\n \nsc\n \n=\n \nnew\n \nJavaSparkContext\n(\nconf\n);\n\n\n      \nJavaRDD\nString\n \ntextFile\n \n=\n \nsc\n.\ntextFile\n(\ninputFilePath\n);\n\n      \nJavaPairRDD\nString\n,\n \nInteger\n \ncounts\n \n=\n \ntextFile\n\n\n              \n.\nflatMap\n(\ns\n \n-\n \nArrays\n.\nasList\n(\ns\n.\nsplit\n(\n\\t\n)).\niterator\n())\n\n\n              \n.\nmapToPair\n(\nword\n \n-\n \nnew\n \nTuple2\n(\nword\n,\n \n1\n))\n\n              \n.\nreduceByKey\n((\na\n,\n \nb\n)\n \n-\n \na\n \n+\n \nb\n);\n\n      \ncounts\n.\nsaveAsTextFile\n(\noutputDir\n);\n\n  \n}\n\n\n}\n\n\n\n\n\nLancer ensuite une configuration de type Maven, avec les commandes \npackage install\n. Un fichier intitul\u00e9 \nworcount-1.jar\n sera cr\u00e9\u00e9 sous le r\u00e9pertoire target.\n\n\nNous allons maintenant copier ce fichier dans docker. Pour cela, naviguer vers le r\u00e9pertoire du projet avec votre terminal (ou plus simplement utiliser le terminal dans IntelliJ), et taper la commande suivante:\n\n\n  docker cp target/wordcount-1 hadoop-master:/root/wordcount-1.jar\n\n\n\n\nRevenir \u00e0 votre contenaire master, et lancer un job Spark en utilisant ce fichier jar g\u00e9n\u00e9r\u00e9, avec la commande \nspark-submit\n, un script utilis\u00e9 pour lancer des applications spark sur un cluster.\n\n\n  spark-submit  --class tn.insat.tp21.WordCountTask\n                --master \nlocal\n\n                --driver-memory 4g --executor-memory 2g --executor-cores \n1\n\n                wordcount.jar\n                input/purchases.txt\n                output\n\n\n\n\n\n\nNous allons lancer le job en mode local, pour commencer.\n\n\nLe fichier en entr\u00e9e est le fichier purchases.txt (que vous trouverez d\u00e9j\u00e0 charg\u00e9 sur le contenaire master), et le r\u00e9sultat sera stock\u00e9 dans un r\u00e9pertoire \noutput\n.\n\n\n\n\n\n\nAttention\n\n\nV\u00e9rifiez bien que le fichier \npurchases\n existe dans le r\u00e9pertoire input de HDFS, et que le r\u00e9pertoire \noutput\n n'existe pas!\n\n\n\n\nSi tout se passe bien, vous devriez trouver, dans le r\u00e9pertoire \noutput\n, deux fichiers part-00000 et part-00001, qui ressemblent \u00e0 ce qui suit:\n\n\n\n\nNous allons maintenant tester le comportement de \nspark-submit\n si on l'ex\u00e9cute en mode \ncluster\n sur YARN. Pour cela, ex\u00e9cuter le code suivant:\n\n\n  spark-submit  --class tn.insat.tp21.WordCountTask\n                --master yarn\n                --deploy-mode cluster\n                --driver-memory 4g --executor-memory 2g --executor-cores \n1\n\n                wordcount.jar\n                input/purchases.txt\n                output2\n\n\n\n\n\nEn lan\u00e7ant le job sur Yarn, deux modes de d\u00e9ploiement sont possibles:\n\n\nMode cluster: o\u00f9 tout le job s'ex\u00e9cute dans le cluster, c'est \u00e0 dire les Spark Executors (qui ex\u00e9cutent les vraies t\u00e2ches) et le Spark Driver (qui ordonnance les Executors). Ce dernier sera encapsul\u00e9 dans un YARN Application Master.\n\n\nMode client : o\u00f9 Spark Driver s'ex\u00e9cute sur la machine cliente (tel que votre propre ordinateur portable). Si votre machine s'\u00e9teint, le job s'arr\u00eate. Ce mode est appropri\u00e9 pour les jobs interactifs.\n\n\n\n\n\n\n\n\nSi tout se passe bien, vous devriez obtenir un r\u00e9pertoire output2 dans HDFS avec les fichiers usuels.\n\n\n\n\nErreur\n\n\nEn cas d'erreur ou d'interruption du job sur Yarn, vous pourrez consulter les fichiers logs pour chercher le message d'erreur (le message affich\u00e9 sur la console n'est pas assez explicite). Pour cela, sur votre navigateur, aller \u00e0 l'adresse: \nhttp://localhost:8041/logs/userlogs\net suivez toujours les derniers liens jusqu'\u00e0 \nstderr\n.\n\n\n\n\nSpark Streaming\n\n\nSpark est connu pour supporter \u00e9galement le traitement des donn\u00e9es en streaming. Les donn\u00e9es peuvent \u00eatre lues \u00e0 partir de plusieurs sources tel que Kafka, Flume, Kinesis ou des sockets TCP, et peuvent \u00eatre trait\u00e9es en utilisant des algorithmes complexes. Ensuite, les donn\u00e9es trait\u00e9es peuvent \u00eatre stock\u00e9es sur des syst\u00e8mes de fichiers, des bases de donn\u00e9es ou des dashboards. Il est m\u00eame possible de r\u00e9aliser des algorithmes de machine learning et du traitement de graphes sur les flux de donn\u00e9es.\n\n\n\n\nEn interne, il fonctionne comme suit. Spark Streaming re\u00e7oit des donn\u00e9es en streaming et les divise en micro-batches, qui sont ensuite calcul\u00e9s par le moteur de spark pour g\u00e9n\u00e9rer le flux final de r\u00e9sultats.\n\n\n\n\nEnvironnement et Code\n\n\nNous allons commencer par tester le streaming en local, comme d'habitude. Pour cela:\n\n\n\n\nCommencer par cr\u00e9er un nouveau projet Maven, avec le fichier pom suivant:\n\n  \n?xml version=\n1.0\n encoding=\nUTF-8\n?\n\n\nproject\n \nxmlns=\nhttp://maven.apache.org/POM/4.0.0\n\n       \nxmlns:xsi=\nhttp://www.w3.org/2001/XMLSchema-instance\n\n       \nxsi:schemaLocation=\nhttp://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\n\n  \nmodelVersion\n4.0.0\n/modelVersion\n\n\n  \ngroupId\nspark.streaming\n/groupId\n\n  \nartifactId\nstream\n/artifactId\n\n  \nversion\n1\n/version\n\n\n  \ndependencies\n\n      \ndependency\n\n          \ngroupId\norg.apache.spark\n/groupId\n\n          \nartifactId\nspark-core_2.11\n/artifactId\n\n          \nversion\n2.2.1\n/version\n\n      \n/dependency\n\n      \ndependency\n\n          \ngroupId\norg.apache.spark\n/groupId\n\n          \nartifactId\nspark-streaming_2.11\n/artifactId\n\n          \nversion\n2.2.1\n/version\n\n      \n/dependency\n\n  \n/dependencies\n\n  \nbuild\n\n      \nplugins\n\n          \nplugin\n\n              \ngroupId\norg.apache.maven.plugins\n/groupId\n\n              \nartifactId\nmaven-compiler-plugin\n/artifactId\n\n              \nversion\n3.1\n/version\n\n              \nconfiguration\n\n                  \nsource\n1.8\n/source\n\n                  \ntarget\n1.8\n/target\n\n              \n/configuration\n\n          \n/plugin\n\n      \n/plugins\n\n  \n/build\n\n\n\n/project\n\n\n\n\nCr\u00e9er une classe \ntn.insat.tp22.Stream\n avec le code suivant:\n\n\n\n\n    \npublic\n \nclass\n \nStream\n \n{\n\n    \npublic\n \nstatic\n \nvoid\n \nmain\n(\nString\n[]\n \nargs\n)\n \nthrows\n \nInterruptedException\n \n{\n\n        \nSparkConf\n \nconf\n \n=\n \nnew\n \nSparkConf\n()\n\n            \n.\nsetAppName\n(\nNetworkWordCount\n)\n\n            \n.\nsetMaster\n(\nlocal\n);\n\n        \nJavaStreamingContext\n \njssc\n \n=\n\n            \nnew\n \nJavaStreamingContext\n(\nconf\n,\n \nDurations\n.\nseconds\n(\n1\n));\n\n\n        \nJavaReceiverInputDStream\nString\n \nlines\n \n=\n\n            \njssc\n.\nsocketTextStream\n(\nlocalhost\n,\n \n9999\n);\n\n\n        \nJavaDStream\nString\n \nwords\n \n=\n\n            \nlines\n.\nflatMap\n(\nx\n \n-\n \nArrays\n.\nasList\n(\nx\n.\nsplit\n(\n \n)).\niterator\n());\n\n        \nJavaPairDStream\nString\n,\n \nInteger\n \npairs\n \n=\n\n            \nwords\n.\nmapToPair\n(\ns\n \n-\n \nnew\n \nTuple2\n(\ns\n,\n \n1\n));\n\n        \nJavaPairDStream\nString\n,\n \nInteger\n \nwordCounts\n \n=\n\n            \npairs\n.\nreduceByKey\n((\ni1\n,\n \ni2\n)\n \n-\n \ni1\n \n+\n \ni2\n);\n\n\n        \nwordCounts\n.\nprint\n();\n\n        \njssc\n.\nstart\n();\n\n        \njssc\n.\nawaitTermination\n();\n\n    \n}\n\n  \n}\n\n\n\n\n\nCe code permet de calculer le nombre de mots dans un stream de donn\u00e9es toutes les secondes.\n\n\nTest du code en Local\n\n\nLe stream ici sera diffus\u00e9 par une petite commande utilitaire qui se trouve dans la majorit\u00e9 des syst\u00e8mes Unix-like.\n\n\n\n\nOuvrir un terminal, et taper la commande suivante pour cr\u00e9er le stream:\n\n  \n  nc -lk \n9999\n\n\n\n  Vous pourrez alors taper les entr\u00e9es de votre choix.\n\n\nEx\u00e9cuter votre classe \nStream\n. Vous verrez d\u00e9filer sur votre console des lignes en continu: l'application est en \u00e9coute sur localhost:9999.\n\n\n\n\nA chaque fois que vous entrez quelque chose sur le terminal, l'application l'intercepte, et l'affichage sur l'\u00e9cran de la console change, comme suit:\n\n\n\n\nEnsuite, pour voir le r\u00e9sultat final du comptage, arr\u00eater l'ex\u00e9cution en cliquant sur le carr\u00e9 rouge, puis observer la console, vous verrez un affichage qui ressemble \u00e0 ceci:\n\n\n\n\nLancement du code sur le cluster\n\n\nPour lancer le code pr\u00e9c\u00e9dent sur le cluster, il faudra d'abord faire des petites modifications:\n\n\n\n\nAttention\n\n\nVeillez \u00e0 mettre l'IP de votre machine locale (sur laquelle vous allez lancer le flux avec \nnc\n) \u00e0 la place de \nvotre-ip>. Vous pourrez trouver votre IP avec la commande ifconfig.\n\n\n\n\n\n\nLancer un \nmvn package install\npour cr\u00e9er le fichier jar.\n\n\nCopier le fichier jar sur le contenaire hadoop.\n\n\nLancer la commande suivante:\n\n\n\n\n    spark-submit --class tn.insat.tp22.Stream\n                 --master \nlocal\n\n                 --driver-memory 4g --executor-memory 2g --executor-cores \n1\n\n                 stream-1.jar\n\n\nObserver le r\u00e9sultat.\n\n\nHomework\n\n\nVous allez, pour ce cours, r\u00e9aliser un projet en bin\u00f4me, qui consiste en la construction d'une architecture Big Data supportant le streaming, le batch processing, et le dashboarding temps r\u00e9el. Pour la s\u00e9ance prochaine, vous allez r\u00e9fl\u00e9chir au type de traitement que vous voulez r\u00e9aliser (le flux de donn\u00e9es en entr\u00e9e, et les r\u00e9sultats en sortie). Vous allez commencer par utiliser Spark pour r\u00e9aliser ces traitements, avec un stockage sur HDFS au besoin.", 
            "title": "TP2"
        }, 
        {
            "location": "/tp2/#tp2-traitement-par-lot-et-streaming-avec-spark", 
            "text": "", 
            "title": "TP2 - Traitement par Lot et Streaming avec Spark"
        }, 
        {
            "location": "/tp2/#telecharger-pdf", 
            "text": "", 
            "title": "T\u00e9l\u00e9charger PDF"
        }, 
        {
            "location": "/tp2/#objectifs-du-tp", 
            "text": "Utilisation de Spark pour r\u00e9aliser des traitements par lot et des traitements en streaming.", 
            "title": "Objectifs du TP"
        }, 
        {
            "location": "/tp2/#outils-et-versions", 
            "text": "Apache Hadoop  Version: 2.7.2  Apache Spark  Version: 2.2.1  Docker  Version 17.09.1  IntelliJ IDEA  Version Ultimate 2016.1 (ou tout autre IDE de votre choix)  Java  Version 1.8  Unix-like ou Unix-based Systems (Divers Linux et MacOS)", 
            "title": "Outils et Versions"
        }, 
        {
            "location": "/tp2/#spark", 
            "text": "", 
            "title": "Spark"
        }, 
        {
            "location": "/tp2/#presentation", 
            "text": "Spark  est un syst\u00e8me de traitement rapide et parall\u00e8le. Il fournit des APIs de haut niveau en Java, Scala, Python et R, et un moteur optimis\u00e9 qui supporte l'ex\u00e9cution des graphes. Il supporte \u00e9galement un ensemble d'outils de haut niveau tels que  Spark SQL  pour le support du traitement de donn\u00e9es structur\u00e9es,  MLlib  pour l'apprentissage des donn\u00e9es,  GraphX  pour le traitement des graphes, et  Spark Streaming  pour le traitment des donn\u00e9es en streaming.", 
            "title": "Pr\u00e9sentation"
        }, 
        {
            "location": "/tp2/#spark-et-hadoop", 
            "text": "Spark peut s'ex\u00e9cuter sur plusieurs plateformes: Hadoop, Mesos, en standalone ou sur le cloud. Il peut \u00e9galement acc\u00e9der diverses sources de donn\u00e9es, comme HDFS, Cassandra, HBase et S3.  Dans ce TP, nous allons ex\u00e9cuter Spark sur Hadoop YARN. YARN s'occupera ainsi de la gestion des ressources pour le d\u00e9clenchement et l'ex\u00e9cution des Jobs Spark.", 
            "title": "Spark et Hadoop"
        }, 
        {
            "location": "/tp2/#installation", 
            "text": "Nous avons proc\u00e9d\u00e9 \u00e0 l'installation de Spark sur le cluster Hadoop utilis\u00e9 dans le  TP1 . Voici les \u00e9tapes n\u00e9cessaires pour le lancer:   Cloner le repo github contenant les fichiers n\u00e9cessaires pour le lancement des contenaires et leur configuration:    git clone https://github.com/liliasfaxi/hadoop-cluster-docker  Construire l'image Docker \u00e0 partir du fichier Dockerfile fourni.     cd  hadoop-cluster-docker\n  ./build-image.sh  D\u00e9marrer les trois contenaires:    sudo ./start-container.sh \nLe r\u00e9sultat de cette ex\u00e9cution sera le suivant:    start hadoop-master container...\n  start hadoop-slave1 container...\n  start hadoop-slave2 container...\n  root@hadoop-master:~#  Lancer les d\u00e9mons yarn et hdfs en lan\u00e7ant:    ./start-hadoop.sh   Vous pourrez v\u00e9rifier que tous les d\u00e9mons sont lanc\u00e9s en tapant:  jps . Un r\u00e9sultat semblable au suivant pourra \u00eatre visible:     880  Jps\n   257  NameNode\n   613  ResourceManager\n   456  SecondaryNameNode  La m\u00eame op\u00e9ration sur les noeuds esclaves (auquels vous acc\u00e9dez \u00e0 partir de votre machine h\u00f4te en tapant  docker exec -it hadoop-slave1 bash ) devrait donner:     176  NodeManager\n   65  DataNode\n   311  Jps", 
            "title": "Installation"
        }, 
        {
            "location": "/tp2/#test-de-spark-avec-spark-shell", 
            "text": "Dans le but de tester l'ex\u00e9cution de spark, commencer par cr\u00e9er un fichier  file1.txt  dans votre noeud master, contenant le texte suivant:    Hello Spark Wordcount!\n  Hello Hadoop Also :)  Charger ensuite ce fichier dans HDFS:    hadoop fs -put file1.txt  Pour v\u00e9rifier que spark est bien install\u00e9, taper la commande suivante:    spark-shell  Vous devriez avoir un r\u00e9sultat semblable au suivant:   Vous pourrez tester spark avec un code scala simple comme suit (\u00e0 ex\u00e9cuter ligne par ligne):     val   lines   =   sc . textFile ( file1.txt ) \n   val   words   =   lines . flatMap ( _ . split ( \\\\s+ )) \n   val   wc   =   words . map ( w   =   ( w ,   1 )). reduceByKey ( _   +   _ ) \n   wc . saveAsTextFile ( file1.count )   Ce code vient de (1) charger le fichier  file1.txt  de HDFS, (2) s\u00e9parer les mots selon les caract\u00e8res d'espacement, (3) appliquer un  map  sur les mots obtenus qui produit le couple ( mot> , 1), puis un  reduce  qui permet de faire la somme des 1 des mots identiques.  Pour afficher le r\u00e9sultat, sortir de spark-shell en cliquant sur  Ctrl-C . T\u00e9l\u00e9charger ensuite le r\u00e9pertoire  file1.count  cr\u00e9\u00e9 dans HDFS comme suit:    hadoop fs -get file1.count \nLe contenu des deux fichiers  part-00000  et  part-00001  ressemble \u00e0 ce qui suit:", 
            "title": "Test de Spark avec Spark-Shell"
        }, 
        {
            "location": "/tp2/#lapi-de-spark", 
            "text": "A un haut niveau d'abstraction, chaque application Spark consiste en un programme  driver  qui ex\u00e9cute la fonction  main  de l'utilisateur et lance plusieurs op\u00e9rations parall\u00e8les sur le cluster. L'abstraction principale fournie par Spark est un RDD ( Resilient Distributed Dataset ), qui repr\u00e9sente une collection d'\u00e9l\u00e9ments partitionn\u00e9s \u00e0 travers les noeuds du cluster, et sur lesquelles on peut op\u00e9rer en parall\u00e8le. Les RDDs sont cr\u00e9\u00e9s \u00e0 partir d'un fichier dans HDFS par exemple, puis le transforment. Les utilisateurs peuvent demander \u00e0 Spark de sauvegarder un RDD en m\u00e9moire, lui permettant ainsi d'\u00eatre r\u00e9utilis\u00e9 efficacement \u00e0 travers plusieurs op\u00e9rations parall\u00e8les.   Les RDDs supportent deux types d'op\u00e9rations:   les  transformations , qui permettent de cr\u00e9er un nouveau Dataset \u00e0 partir d'un Dataset existant  les  actions , qui retournent une valeur au programme  driver  ar\u00e8s avoir ex\u00e9cut\u00e9 un calcul sur le Dataset.   Par exemple, un  map  est une transformation qui passe chaque \u00e9l\u00e9ment du dataset via une fontion, et retourne un nouvel RDD repr\u00e9sentant les r\u00e9sultats. Un  reduce  est une action qui agr\u00e8ge tous les \u00e9l\u00e9ments du RDD en utilisant une certaine fonction et retourne le r\u00e9sultat final au programme.  Toutes les transformations dans Spark sont  lazy , car elles ne calculent pas le r\u00e9sultat imm\u00e9diatement. Elles se souviennent des transformations appliqu\u00e9es \u00e0 un dataset de base (par ex. un fichier). Les transformations ne sont calcul\u00e9es que quand une action n\u00e9cessite qu'un r\u00e9sultat soit retourn\u00e9 au programme principal. Cela permet \u00e0 Spark de s'ex\u00e9cuter plus efficacement.", 
            "title": "L'API de Spark"
        }, 
        {
            "location": "/tp2/#exemple", 
            "text": "L'exemple que nous allons pr\u00e9senter ici par \u00e9tapes permet de relever les mots les plus fr\u00e9quents dans un fichier. Pour cela, le code suivant est utilis\u00e9:     //Etape 1 - Cr\u00e9er un RDD \u00e0 partir d un fichier texte de Hadoop \n   val   docs   =   spark . textFile ( /docs )       //Etape 2 - Convertir les lignes en minuscule \n   val   lower   =   docs . map ( line   =   line . toLowerCase )       //Etape 3 - S\u00e9parer les lignes en mots \n   val   words   =   lower . flatMap ( line   =   line . split ( \\\\s+ ))       //Etape 4 - produire les tuples (mot, 1) \n   val   counts   =   words . map ( word   =   ( word , 1 ))       //Etape 5 - Compter tous les mots \n   val   freq   =   counts . reduceByKey ( _   +   _ )       //Etape 6 - Inverser les tuples (transformation avec swap) \n   freq . map ( _ . swap )       //Etape 6 - Inverser les tuples (action de s\u00e9lection des n premiers) \n   val   top   =   freq . map ( _swap ). top ( N )", 
            "title": "Exemple"
        }, 
        {
            "location": "/tp2/#spark-batch-en-java", 
            "text": "", 
            "title": "Spark Batch en Java"
        }, 
        {
            "location": "/tp2/#preparation-de-lenvironnement-et-code", 
            "text": "Nous allons dans cette partie cr\u00e9er un projet Spark Batch en Java (un simple WordCount), le charger sur le cluster et lancer le job.   Cr\u00e9er un projet Maven avec IntelliJ IDEA, en utilisant la config suivante: \n      groupId spark.batch /groupId \n   artifactId wordcount /artifactId \n   version 1 /version   Rajouter dans le fichier pom les d\u00e9pendances n\u00e9cessaires, et indiquer la version du compilateur Java: \n      properties \n       maven.compiler.source 1.8 /maven.compiler.source \n       maven.compiler.target 1.8 /maven.compiler.target \n   /properties \n   dependencies \n       dependency \n           groupId org.apache.spark /groupId \n           artifactId spark-core_2.11 /artifactId \n           version 2.1.0 /version \n       /dependency \n       dependency \n           groupId org.slf4j /groupId \n           artifactId slf4j-log4j12 /artifactId \n           version 1.7.22 /version \n       /dependency \n   /dependencies   Sous le r\u00e9pertoire java, cr\u00e9er un package que vous appellerez  tn.insat.tp21 , et dedans, une classe appel\u00e9e  WordCountTask .   \u00c9crire le code suivant dans  WordCountTask : \n   public   class   WordCountTask   { \n       private   static   final   Logger   LOGGER   =   LoggerFactory . getLogger ( WordCountTask . class ); \n\n       public   static   void   main ( String []   args )   { \n           checkArgument ( args . length     1 ,   Please provide the path of input file and output dir as parameters. ); \n           new   WordCountTask (). run ( args [ 0 ],   args [ 1 ]); \n       } \n\n       public   void   run ( String   inputFilePath ,   String   outputDir )   { \n           String   master   =   local[*] ; \n           SparkConf   conf   =   new   SparkConf () \n                   . setAppName ( WordCountTask . class . getName ()) \n                   . setMaster ( master ); \n           JavaSparkContext   sc   =   new   JavaSparkContext ( conf ); \n\n           JavaRDD String   textFile   =   sc . textFile ( inputFilePath ); \n           JavaPairRDD String ,   Integer   counts   =   textFile \n                   . flatMap ( s   -   Arrays . asList ( s . split (   )). iterator ()) \n                   . mapToPair ( word   -   new   Tuple2 ( word ,   1 )) \n                   . reduceByKey (( a ,   b )   -   a   +   b ); \n           counts . saveAsTextFile ( outputDir ); \n       } \n   }  \n  La premi\u00e8re chose \u00e0 faire dans un programme Spark est de cr\u00e9er un objet  JavaSparkContext , qui indique \u00e0 Spark comment acc\u00e9der \u00e0 un cluster. Pour cr\u00e9er ce contexte, vous aurez besoin de construire un objet  SparkConf  qui contient toutes les informations sur l'application.   appName : est de nom de l'application  master  est une URL d'un cluster Spark, Mesos ou YARN, ou bien une cha\u00eene sp\u00e9ciale  local  pour lancer le job en mode local.      Warning  Nous avons indiqu\u00e9 ici que notre master est  local  pour les besoins du test, mais plus tard, en le packageant pour le cluster, nous allons enlever cette indication. Il est en effet d\u00e9conseill\u00e9 de la hard-coder dans le programme, il faudrait plut\u00f4t l'indiquer comme option de commande \u00e0 chaque fois que nous lan\u00e7ons le job.  Le reste du code de l'application est la version en Java de l'exemple en scala que nous avions fait avec spark-shell.", 
            "title": "Pr\u00e9paration de l'environnement et Code"
        }, 
        {
            "location": "/tp2/#test-du-code-en-local", 
            "text": "Pour tester le code sur votre machine, proc\u00e9der aux \u00e9tapes suivantes:   Ins\u00e9rer un fichier texte de votre choix (par exemple le fameux  loremipsum.txt ) dans le r\u00e9pertoire src/main/resources.  Cr\u00e9er une nouvelle configuration de type \"Application\" ( Run- Edit Configurations ) que vous appellerez  WordCountTask , et d\u00e9finir les arguments suivants (fichier de d\u00e9part et r\u00e9pertoire d'arriv\u00e9e) comme  Program arguments : \n     src/main/resources/loremipsum.txt src/main/resources/out  Cliquer sur OK, et lancer la configuration. Si tout se passe bien, un r\u00e9pertoire  out  sera cr\u00e9\u00e9 sous  resources , qui contient deux fichiers: part-00000, part-00001.", 
            "title": "Test du code en local"
        }, 
        {
            "location": "/tp2/#lancement-du-code-sur-le-cluster", 
            "text": "Pour ex\u00e9cuter le code sur le cluster, modifier comme indiqu\u00e9 les lignes en jaune dans ce qui suit:  public   class   WordCountTask   { \n   private   static   final   Logger   LOGGER   =   LoggerFactory . getLogger ( WordCountTask . class ); \n\n   public   static   void   main ( String []   args )   { \n       checkArgument ( args . length     1 ,   Please provide the path of input file and output dir as parameters. ); \n       new   WordCountTask (). run ( args [ 0 ],   args [ 1 ]); \n   } \n\n   public   void   run ( String   inputFilePath ,   String   outputDir )   {         SparkConf   conf   =   new   SparkConf ()                 . setAppName ( WordCountTask . class . getName ());  \n       JavaSparkContext   sc   =   new   JavaSparkContext ( conf ); \n\n       JavaRDD String   textFile   =   sc . textFile ( inputFilePath ); \n       JavaPairRDD String ,   Integer   counts   =   textFile                 . flatMap ( s   -   Arrays . asList ( s . split ( \\t )). iterator ())                 . mapToPair ( word   -   new   Tuple2 ( word ,   1 )) \n               . reduceByKey (( a ,   b )   -   a   +   b ); \n       counts . saveAsTextFile ( outputDir ); \n   }  }   Lancer ensuite une configuration de type Maven, avec les commandes  package install . Un fichier intitul\u00e9  worcount-1.jar  sera cr\u00e9\u00e9 sous le r\u00e9pertoire target.  Nous allons maintenant copier ce fichier dans docker. Pour cela, naviguer vers le r\u00e9pertoire du projet avec votre terminal (ou plus simplement utiliser le terminal dans IntelliJ), et taper la commande suivante:    docker cp target/wordcount-1 hadoop-master:/root/wordcount-1.jar  Revenir \u00e0 votre contenaire master, et lancer un job Spark en utilisant ce fichier jar g\u00e9n\u00e9r\u00e9, avec la commande  spark-submit , un script utilis\u00e9 pour lancer des applications spark sur un cluster.    spark-submit  --class tn.insat.tp21.WordCountTask\n                --master  local \n                --driver-memory 4g --executor-memory 2g --executor-cores  1 \n                wordcount.jar\n                input/purchases.txt\n                output   Nous allons lancer le job en mode local, pour commencer.  Le fichier en entr\u00e9e est le fichier purchases.txt (que vous trouverez d\u00e9j\u00e0 charg\u00e9 sur le contenaire master), et le r\u00e9sultat sera stock\u00e9 dans un r\u00e9pertoire  output .    Attention  V\u00e9rifiez bien que le fichier  purchases  existe dans le r\u00e9pertoire input de HDFS, et que le r\u00e9pertoire  output  n'existe pas!   Si tout se passe bien, vous devriez trouver, dans le r\u00e9pertoire  output , deux fichiers part-00000 et part-00001, qui ressemblent \u00e0 ce qui suit:   Nous allons maintenant tester le comportement de  spark-submit  si on l'ex\u00e9cute en mode  cluster  sur YARN. Pour cela, ex\u00e9cuter le code suivant:    spark-submit  --class tn.insat.tp21.WordCountTask\n                --master yarn\n                --deploy-mode cluster\n                --driver-memory 4g --executor-memory 2g --executor-cores  1 \n                wordcount.jar\n                input/purchases.txt\n                output2   En lan\u00e7ant le job sur Yarn, deux modes de d\u00e9ploiement sont possibles:  Mode cluster: o\u00f9 tout le job s'ex\u00e9cute dans le cluster, c'est \u00e0 dire les Spark Executors (qui ex\u00e9cutent les vraies t\u00e2ches) et le Spark Driver (qui ordonnance les Executors). Ce dernier sera encapsul\u00e9 dans un YARN Application Master.  Mode client : o\u00f9 Spark Driver s'ex\u00e9cute sur la machine cliente (tel que votre propre ordinateur portable). Si votre machine s'\u00e9teint, le job s'arr\u00eate. Ce mode est appropri\u00e9 pour les jobs interactifs.     Si tout se passe bien, vous devriez obtenir un r\u00e9pertoire output2 dans HDFS avec les fichiers usuels.   Erreur  En cas d'erreur ou d'interruption du job sur Yarn, vous pourrez consulter les fichiers logs pour chercher le message d'erreur (le message affich\u00e9 sur la console n'est pas assez explicite). Pour cela, sur votre navigateur, aller \u00e0 l'adresse:  http://localhost:8041/logs/userlogs et suivez toujours les derniers liens jusqu'\u00e0  stderr .", 
            "title": "Lancement du code sur le cluster"
        }, 
        {
            "location": "/tp2/#spark-streaming", 
            "text": "Spark est connu pour supporter \u00e9galement le traitement des donn\u00e9es en streaming. Les donn\u00e9es peuvent \u00eatre lues \u00e0 partir de plusieurs sources tel que Kafka, Flume, Kinesis ou des sockets TCP, et peuvent \u00eatre trait\u00e9es en utilisant des algorithmes complexes. Ensuite, les donn\u00e9es trait\u00e9es peuvent \u00eatre stock\u00e9es sur des syst\u00e8mes de fichiers, des bases de donn\u00e9es ou des dashboards. Il est m\u00eame possible de r\u00e9aliser des algorithmes de machine learning et du traitement de graphes sur les flux de donn\u00e9es.   En interne, il fonctionne comme suit. Spark Streaming re\u00e7oit des donn\u00e9es en streaming et les divise en micro-batches, qui sont ensuite calcul\u00e9s par le moteur de spark pour g\u00e9n\u00e9rer le flux final de r\u00e9sultats.", 
            "title": "Spark Streaming"
        }, 
        {
            "location": "/tp2/#environnement-et-code", 
            "text": "Nous allons commencer par tester le streaming en local, comme d'habitude. Pour cela:   Commencer par cr\u00e9er un nouveau projet Maven, avec le fichier pom suivant: \n   ?xml version= 1.0  encoding= UTF-8 ?  project   xmlns= http://maven.apache.org/POM/4.0.0 \n        xmlns:xsi= http://www.w3.org/2001/XMLSchema-instance \n        xsi:schemaLocation= http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd \n   modelVersion 4.0.0 /modelVersion \n\n   groupId spark.streaming /groupId \n   artifactId stream /artifactId \n   version 1 /version \n\n   dependencies \n       dependency \n           groupId org.apache.spark /groupId \n           artifactId spark-core_2.11 /artifactId \n           version 2.2.1 /version \n       /dependency \n       dependency \n           groupId org.apache.spark /groupId \n           artifactId spark-streaming_2.11 /artifactId \n           version 2.2.1 /version \n       /dependency \n   /dependencies \n   build \n       plugins \n           plugin \n               groupId org.apache.maven.plugins /groupId \n               artifactId maven-compiler-plugin /artifactId \n               version 3.1 /version \n               configuration \n                   source 1.8 /source \n                   target 1.8 /target \n               /configuration \n           /plugin \n       /plugins \n   /build  /project   Cr\u00e9er une classe  tn.insat.tp22.Stream  avec le code suivant:        public   class   Stream   { \n     public   static   void   main ( String []   args )   throws   InterruptedException   { \n         SparkConf   conf   =   new   SparkConf () \n             . setAppName ( NetworkWordCount ) \n             . setMaster ( local ); \n         JavaStreamingContext   jssc   = \n             new   JavaStreamingContext ( conf ,   Durations . seconds ( 1 )); \n\n         JavaReceiverInputDStream String   lines   = \n             jssc . socketTextStream ( localhost ,   9999 ); \n\n         JavaDStream String   words   = \n             lines . flatMap ( x   -   Arrays . asList ( x . split (   )). iterator ()); \n         JavaPairDStream String ,   Integer   pairs   = \n             words . mapToPair ( s   -   new   Tuple2 ( s ,   1 )); \n         JavaPairDStream String ,   Integer   wordCounts   = \n             pairs . reduceByKey (( i1 ,   i2 )   -   i1   +   i2 ); \n\n         wordCounts . print (); \n         jssc . start (); \n         jssc . awaitTermination (); \n     } \n   }   Ce code permet de calculer le nombre de mots dans un stream de donn\u00e9es toutes les secondes.", 
            "title": "Environnement et Code"
        }, 
        {
            "location": "/tp2/#test-du-code-en-local_1", 
            "text": "Le stream ici sera diffus\u00e9 par une petite commande utilitaire qui se trouve dans la majorit\u00e9 des syst\u00e8mes Unix-like.   Ouvrir un terminal, et taper la commande suivante pour cr\u00e9er le stream: \n     nc -lk  9999  \n  Vous pourrez alors taper les entr\u00e9es de votre choix.  Ex\u00e9cuter votre classe  Stream . Vous verrez d\u00e9filer sur votre console des lignes en continu: l'application est en \u00e9coute sur localhost:9999.   A chaque fois que vous entrez quelque chose sur le terminal, l'application l'intercepte, et l'affichage sur l'\u00e9cran de la console change, comme suit:   Ensuite, pour voir le r\u00e9sultat final du comptage, arr\u00eater l'ex\u00e9cution en cliquant sur le carr\u00e9 rouge, puis observer la console, vous verrez un affichage qui ressemble \u00e0 ceci:", 
            "title": "Test du code en Local"
        }, 
        {
            "location": "/tp2/#lancement-du-code-sur-le-cluster_1", 
            "text": "Pour lancer le code pr\u00e9c\u00e9dent sur le cluster, il faudra d'abord faire des petites modifications:   Attention  Veillez \u00e0 mettre l'IP de votre machine locale (sur laquelle vous allez lancer le flux avec  nc ) \u00e0 la place de  votre-ip>. Vous pourrez trouver votre IP avec la commande ifconfig.    Lancer un  mvn package install pour cr\u00e9er le fichier jar.  Copier le fichier jar sur le contenaire hadoop.  Lancer la commande suivante:       spark-submit --class tn.insat.tp22.Stream\n                 --master  local \n                 --driver-memory 4g --executor-memory 2g --executor-cores  1 \n                 stream-1.jar \nObserver le r\u00e9sultat.", 
            "title": "Lancement du code sur le cluster"
        }, 
        {
            "location": "/tp2/#homework", 
            "text": "Vous allez, pour ce cours, r\u00e9aliser un projet en bin\u00f4me, qui consiste en la construction d'une architecture Big Data supportant le streaming, le batch processing, et le dashboarding temps r\u00e9el. Pour la s\u00e9ance prochaine, vous allez r\u00e9fl\u00e9chir au type de traitement que vous voulez r\u00e9aliser (le flux de donn\u00e9es en entr\u00e9e, et les r\u00e9sultats en sortie). Vous allez commencer par utiliser Spark pour r\u00e9aliser ces traitements, avec un stockage sur HDFS au besoin.", 
            "title": "Homework"
        }, 
        {
            "location": "/tp3/", 
            "text": "TP3 - La Collecte de donn\u00e9es avec le Bus Kafka\n\n\n\n\nT\u00e9l\u00e9charger PDF\n\n\n\n\nObjectifs du TP\n\n\nUtilisation de Kafka pour une collecte de donn\u00e9es distribu\u00e9e, et int\u00e9gration avec Spark.\n\n\nOutils et Versions\n\n\n\n\nApache Kafka\n Version 2.11-0.8.2.1\n\n\nApache Hadoop\n Version: 2.7.2\n\n\nApache Spark\n Version: 2.2.1\n\n\nDocker\n Version 17.09.1\n\n\nIntelliJ IDEA\n Version Ultimate 2016.1 (ou tout autre IDE de votre choix)\n\n\nJava\n Version 1.8\n\n\nUnix-like ou Unix-based Systems (Divers Linux et MacOS)\n\n\n\n\nKafka\n\n\nQu'est-ce qu'un syst\u00e8me de messaging?\n\n\nUn syst\u00e8me de messaging (\nMessaging System\n) est responsable du transfert de donn\u00e9es d'une application \u00e0 une autre, de mani\u00e8re \u00e0 ce que les applications puissent se concentrer sur les donn\u00e9es sans s'inqui\u00e9ter de la mani\u00e8re de les partager ou de les collecter. Le messaging distribu\u00e9 est bas\u00e9 sur le principe de file de message fiable. Les messages sont stock\u00e9s de mani\u00e8re asynchrone dans des files d'attente entre les applications clientes et le syst\u00e8me de messaging.\n\n\nDeux types de patrons de messaging existent: Les syst\u00e8mes \"\npoint \u00e0 point\n\" et les syst\u00e8mes \"\npublish-subscribe\n\".\n\n\n1. Syst\u00e8mes de messaging Point \u00e0 Point\n\n\nDans un syst\u00e8me point \u00e0 point, les messages sont stock\u00e9s dans une file. un ou plusieurs consommateurs peuvent consommer les message dans la file, mais un message ne peut \u00eatre consomm\u00e9 que par un seul consommateur \u00e0 la fois. Une fois le consommateur lit le message, ce dernier dispara\u00eet de la file.\n\n\n\n\n\n\n\n\n2. Syst\u00e8mes de messaging Publish/Subscribe\n\n\nDans un syst\u00e8me publish-subscribe, les messages sont stock\u00e9s dans un \"\ntopic\n\". Contrairement \u00e0 un syst\u00e8me point \u00e0 point, les consommateurs peuvent souscrire \u00e0 un ou plusieurs topics et consommer tous les messages de ce topic.\n\n\n\n\n\n\n\n\nPr\u00e9sentation de Kafka\n\n\nApache \nKafka\n est une plateforme de streaming qui b\u00e9n\u00e9ficie de trois fonctionnalit\u00e9s:\n\n\n\n\nElle vous permet de publier et souscrire \u00e0 un flux d'enregistrements. Elle ressemble ainsi \u00e0 une file demessage ou un syst\u00e8me de messaging d'entreprise.\n\n\nElle permet de stocker des flux d'enregistrements d'une fa\u00e7on tol\u00e9rante aux pannes.\n\n\nElle vous permet de traiter (au besoin) les enregistrements au fur et \u00e0 mesure qu'ils arrivent.\n\n\n\n\n\n\nLes principaux avantages de Kafka sont:\n\n\n\n\nLa fiabliti\u00e9\n: Kafka est distribu\u00e9, partitionn\u00e9, r\u00e9pliqu\u00e9 et tol\u00e9rent aux fautes.\n\n\nLa scalabilit\u00e9\n: Kafka se met \u00e0 l'\u00e9chelle facilement et sans temps d'arr\u00eat.\n\n\nLa durabilit\u00e9\n: Kafka utilise un \ncommit log\n distribu\u00e9, ce qui permet de stocker les messages sur le disque le plus vite possible.\n\n\nLa performance\n: Kafka a un d\u00e9bit \u00e9lev\u00e9 pour la publication et l'abonnement.\n\n\n\n\nArchitecture de Kafka\n\n\nPour comprendre le fonctionnement de Kafka, il faut d'abord se familiariser avec le vocabulaire suivant:\n\n\n\n\nTopic\n: Un flux de messages appartenant \u00e0 une cat\u00e9gorie particuli\u00e8re. Les donn\u00e9es sont stock\u00e9es dans des topics.\n\n\nPartitions\n: Chaque topic est divis\u00e9 en partitions. Pour chaque topic, Kafka conserve un minimum d'une partition. Chaque partition contient des messages dans une s\u00e9quence ordonn\u00e9e immuable. Une partition est impl\u00e9ment\u00e9e comme un ensemble de s\u00e8gments de tailles \u00e9gales.\n\n\nOffset\n: Les enregistrements d'une partition ont chacun un identifiant s\u00e9quentiel appel\u00e9 \noffset\n, qui permet de l'identifier de mani\u00e8re unique dans la partition.\n\n\nR\u00e9pliques\n: Les r\u00e9pliques sont des \nbackups\n d'une partition. Elles ne sont jamais lues ni modifi\u00e9es par les acteurs externes, elles servent uniquement \u00e0 pr\u00e9venir la perte de donn\u00e9es.\n\n\nBrokers\n: Les \nbrokers\n (ou courtiers) sont de simples syst\u00e8mes responsables de maintenir les donn\u00e9es publi\u00e9es. Chaque courtier peut avoir z\u00e9ro ou plusieurs partitions par topic. Si un topic admet N partitions et N courtiers, chaque courtier va avoir une seule partition. Si le nombre de courtiers est plus grand que celui des partitions, certains n'auront aucune partition de ce topic.\n\n\nCluster\n: Un syst\u00e8me Kafka ayant plus qu'un seul Broker est appel\u00e9 \ncluster Kafka\n. L'ajout de nouveau brokers est fait de mani\u00e8re transparente sans temps d'arr\u00eat.\n\n\nProducers\n: Les producteurs sont les \u00e9diteurs de messages \u00e0 un ou plusieurs topics Kafka. Ils envoient des donn\u00e9es aux courtiers Kafka. Chaque fois qu'un producteur publie un message \u00e0 un courtier, ce dernier rattache le message au dernier s\u00e8gment, ajout\u00e9 ainsi \u00e0 une partition. Un producteur peut \u00e9galement envoyer un message \u00e0 une partition particuli\u00e8re.\n\n\nConsumers\n: Les consommateurs lisent les donn\u00e9es \u00e0 partir des brokers. Ils souscrivent \u00e0 un ou plusieurs topics, et consomment les messages publi\u00e9s en extrayant les donn\u00e9es \u00e0 partir des brokers.\n\n\nLeaders\n: Le leader est le noeud responsable de toutes les lectures et \u00e9critures d'une partition donn\u00e9e. Chaque partition a un serveur jouant le r\u00f4le de leader.\n\n\nFollower\n: C'est un noeud qui suit les instructions du leader. Si le leader tombe en panne, l'un des followers deviendra automatiquement le nouveau leader.\n\n\n\n\nLa figure suivante montre un exemple de flux entre les diff\u00e9rentes parties d'un syst\u00e8me Kafka:\n\n\n\n\n\n\n\n\nDans cet exemple, un topic est configur\u00e9 en trois partitions. La partition 1 admet deux enregistrements d'offsets respectifs 0 et 1, la partition 2 a quatre enregistrements et la partition 3 a un seul enregistrement.\n\n\nEn supposant que, si le facteur de r\u00e9plication du topic est de 3, alors Kafka va cr\u00e9er trois r\u00e9pliques identiques de chaque partition et les placer dans le cluster pour les rendre disponibles pour toutes les op\u00e9rations. L'identifiant de la r\u00e9plique est le m\u00eame que l'identifiant du serveur qui l'h\u00e9berge. Pour \u00e9quilibrer la charge dans le cluster, chaque broker stocke une ou plusieurs de ces partitions. Plusieurs producteurs et consommateurs peuvent publier et extraire les messages au m\u00eame moment.\n\n\nKafka et Zookeeper\n\n\nZookeeper\n est un service centralis\u00e9 permettant de maintenir l'information de configuration, de nommage, de synchronisation et de services de groupe. Ces services sont utilis\u00e9s par les applications distribu\u00e9es en g\u00e9n\u00e9ral, et par Kafka en particulier. Pour \u00e9viter la complexit\u00e9 et difficult\u00e9 de leur impl\u00e9mentation manuelle, Zookeeper est utilis\u00e9.\n\n\n\n\nUn cluster Kafka consiste typiquement en plusieurs courtiers (Brokers) pour maintenir la r\u00e9partition de charge. Ces courtiers sont stateless, c'est pour cela qu'ils utilisent Zookeeper pour maintenir l'\u00e9tat du cluster. Un courtier peut g\u00e9rer des centaines de milliers de lectures et \u00e9critures par seconde, et chaque courtier peut g\u00e9rer des t\u00e9ra-octets de messages sans impact sur la performance.\n\n\nZookeeper est utilis\u00e9 pour g\u00e9rer et coordonner les courtiers Kafka. Il permet de notifier les producteurs et consommateurs de messages de la pr\u00e9sence de tout nouveau courtier, ou de l'\u00e9chec d'un courtier dans le cluster.\n\n\nInstallation\n\n\nKafka a \u00e9t\u00e9 install\u00e9 sur le m\u00eame cluster que les deux TP pr\u00e9c\u00e9dents. Si vous disposez des contenaires, vous n'avez rien \u00e0 faire. Sinon, vous pourrez les installer avec Docker comme suit:\n\n\n\n\nCloner le repo github contenant les fichiers n\u00e9cessaires pour le lancement des contenaires et leur configuration:\n\n\n  git clone https://github.com/liliasfaxi/hadoop-cluster-docker\n\n\n\nConstruire l'image Docker \u00e0 partir du fichier Dockerfile fourni.\n\n\n  \ncd\n hadoop-cluster-docker\n  ./build-image.sh\n\n\n\nD\u00e9marrer les trois contenaires:\n\n\n  sudo ./start-container.sh\n\n\nLe r\u00e9sultat de cette ex\u00e9cution sera le suivant:\n\n\n  start hadoop-master container...\n  start hadoop-slave1 container...\n  start hadoop-slave2 container...\n  root@hadoop-master:~#\n\n\n\nLancer Kafka et Zookeeper en tapant :\n\n\n  ./start-kafka-zookeeper.sh\n\n\nLes deux d\u00e9mons Kafka et Zookeeper seront lanc\u00e9s. Vous pourrez v\u00e9rifier cela en tapant \njps\n pour voir quels processus Java sont en ex\u00e9cution, vous devriez trouver les processus suivants:\n\n\n  \n2756\n Kafka\n  \n53\n QuorumPeerMain\n  \n6349\n Jps\n\n\n\n\n\nPremi\u00e8re utilisation de Kafka\n\n\nCr\u00e9ation d'un topic\n\n\nPour g\u00e9rer les topics, Kafka fournit une commande appel\u00e9e \nkafka-topics.sh\n.\n\nDans un nouveau terminal, taper la commande suivante pour cr\u00e9er un nouveau topic appel\u00e9 \"Hello-Kafka\".\n\n\n  kafka-topics.sh --create --zookeeper localhost:2181\n                  --replication-factor \n1\n --partitions \n1\n\n                  --topic Hello-Kafka\n\n\n\n\n\n\nAttention\n\n\nCette commande fonctionne car nous avions rajout\u00e9 /usr/local/kafka/bin \u00e0 la variable d'environnement PATH. Si ce n'\u00e9tait pas le cas, on aurait du appeler /usr/local/kafka/bin/kafka-topics.sh\n\n\n\n\nPour afficher la liste des topics existants, il faudra utiliser:\n\n\n  kafka-topics.sh --list --zookeeper localhost:2181\n\n\n\n\nLe r\u00e9sultat devrait \u00eatre (parmi un grand nombre de lignes d'INFO):\n\n\n  Hello-Kafka\n\n\n\n\nExemple Producteur Consommateur\n\n\nKafka fournit un exemple de producteur standard que vous pouvez directement utiliser. Il suffit de taper:\n\n\n  kafka-console-producer.sh --broker-list localhost:9092 --topic Hello-Kafka\n\n\n\n\nTout ce que vous taperez dor\u00e9navant sur la console sera envoy\u00e9 \u00e0 Kafka. L'option \n--broker-list\n permet de d\u00e9finir la liste des courtiers auxquels vous enverrez le message. Pour l'instant, vous n'en disposez que d'un, et il est d\u00e9ploy\u00e9 \u00e0 l'adresse localhost:9092.\n\n\nPour lancer un consommateur, utiliser:\n\n\n  kafka-console-consumer.sh --zookeeper localhost:2181 \u2014topic Hello-Kafka\n--from-beginning\n\n\n\n\nLe r\u00e9sultat devra ressembler au suivant:\n\n\n\n\nConfiguration de plusieurs brokers\n\n\nDans ce qui pr\u00e9c\u00e8de, nous avons configur\u00e9 Kafka pour lancer un seul broker. Pour cr\u00e9er plusieurs brokers, il suffit de dupliquer le fichier \n$KAFKA_HOME/config/server.properties\n autant de fois que n\u00e9cessaire. Dans notre cas, nous allons cr\u00e9er deux autre fichiers: \nserver-one.properties\n et \nserver-two.properties\n, puis nous modifions les param\u00e8tres suivants comme suit:\n\n\n  \n### config/server-one.properties\n\n\n  broker.id\n \n=\n \n1\n\n\n  listeners\n=\nPLAINTEXT://localhost:9093\n\n\n  log.dirs\n=\n/tmp/kafka-logs-1\n\n\n  \n### config/server-two.properties\n\n\n  broker.id\n \n=\n \n2\n\n\n  listeners\n=\nPLAINTEXT://localhost:9094\n\n\n  log.dirs\n=\n/tmp/kafka-logs-2\n\n\n\n\n\nPour d\u00e9marrer les diff\u00e9rents brokers, il suffit d'appeler \nkafka-server-start.sh\n avec les nouveaux fichiers de configuration.\n\n\n kafka-server-start.sh \n$KAFKA_HOME\n/config/server.properties \n\n kafka-server-start.sh \n$KAFKA_HOME\n/config/server-one.properties \n\n kafka-server-start.sh \n$KAFKA_HOME\n/config/server-two.properties \n\n\n\n\n\nLancer \njps\n pour voir les trois serveurs s'ex\u00e9cuter.\n\n\nCr\u00e9ation d'une application personnalis\u00e9e\n\n\nNous allons dans cette partie cr\u00e9er une application pour publier et consommer des messages de Kafka. Pour cela, nous allons utiliser KafkaProducer API et KafkaConsumer API.\n\n\nProducteur\n\n\nPour cr\u00e9er un producteur Kafka, cr\u00e9er un fichier dans un r\u00e9pertoire de votre choix dans le contenaire master, intitul\u00e9 \nSimpleProducer.java\n. Son code est le suivant:\n\n\nimport\n \njava.util.Properties\n;\n\n\nimport\n \norg.apache.kafka.clients.producer.Producer\n;\n\n\nimport\n \norg.apache.kafka.clients.producer.KafkaProducer\n;\n\n\nimport\n \norg.apache.kafka.clients.producer.ProducerRecord\n;\n\n\n \npublic\n \nclass\n \nSimpleProducer\n \n{\n\n\n   \npublic\n \nstatic\n \nvoid\n \nmain\n(\nString\n[]\n \nargs\n)\n \nthrows\n \nException\n{\n\n\n      \n// Verifier que le topic est donne en argument\n\n      \nif\n(\nargs\n.\nlength\n \n==\n \n0\n){\n\n         \nSystem\n.\nout\n.\nprintln\n(\nEntrer le nom du topic\n);\n\n         \nreturn\n;\n\n      \n}\n\n\n      \n// Assigner topicName a une variable\n\n      \nString\n \ntopicName\n \n=\n \nargs\n[\n0\n].\ntoString\n();\n\n\n      \n// Creer une instance de proprietes pour acceder aux configurations du producteur\n\n      \nProperties\n \nprops\n \n=\n \nnew\n \nProperties\n();\n\n\n      \n// Assigner l\nidentifiant du serveur kafka\n\n      \nprops\n.\nput\n(\nbootstrap.servers\n,\n \nlocalhost:9092\n);\n\n\n      \n// Definir un acquittement pour les requetes du producteur\n\n      \nprops\n.\nput\n(\nacks\n,\n \nall\n);\n\n\n      \n// Si la requete echoue, le producteur peut reessayer automatiquemt\n\n      \nprops\n.\nput\n(\nretries\n,\n \n0\n);\n\n\n      \n// Specifier la taille du buffer size dans la config\n\n      \nprops\n.\nput\n(\nbatch.size\n,\n \n16384\n);\n\n\n      \n// buffer.memory controle le montant total de memoire disponible au producteur pour le buffering\n\n      \nprops\n.\nput\n(\nbuffer.memory\n,\n \n33554432\n);\n\n\n      \nprops\n.\nput\n(\nkey.serializer\n,\n\n         \norg.apache.kafka.common.serialization.StringSerializer\n);\n\n\n      \nprops\n.\nput\n(\nvalue.serializer\n,\n\n         \norg.apache.kafka.common.serialization.StringSerializer\n);\n\n\n      \nProducer\nString\n,\n \nString\n \nproducer\n \n=\n \nnew\n \nKafkaProducer\n\n         \nString\n,\n \nString\n(\nprops\n);\n\n\n      \nfor\n(\nint\n \ni\n \n=\n \n0\n;\n \ni\n \n \n10\n;\n \ni\n++)\n\n         \nproducer\n.\nsend\n(\nnew\n \nProducerRecord\nString\n,\n \nString\n(\ntopicName\n,\n\n            \nInteger\n.\ntoString\n(\ni\n),\n \nInteger\n.\ntoString\n(\ni\n)));\n\n               \nSystem\n.\nout\n.\nprintln\n(\nMessage envoye avec succes\n);\n\n               \nproducer\n.\nclose\n();\n\n   \n}\n\n \n}\n\n\n\n\n\nProducerRecord est une paire clef/valeur envoy\u00e9e au cluster Kafka. Son constructeur peut prendre 4, 3 ou 2 param\u00e8tres, selon le besoin. Les signatures autoris\u00e9es sont comme suit:\n\n\n  \npublic\n \nProducerRecord\n \n(\nstring\n \ntopic\n,\n \nint\n \npartition\n,\n \nk\n \nkey\n,\n \nv\n \nvalue\n){...}\n\n  \npublic\n \nProducerRecord\n \n(\nstring\n \ntopic\n,\n \nk\n \nkey\n,\n \nv\n \nvalue\n){...}\n\n  \npublic\n \nProducerRecord\n \n(\nstring\n \ntopic\n,\n \nv\n \nvalue\n){...}\n\n\n\n\n\nPour compiler ce code, taper dans la console (en vous positionnant dans le r\u00e9pertoire qui contient le fichier SimpleProducer.java):\n\n\n   javac -cp \n$KAFKA_HOME\n/libs/*\n:. SimpleProducer.java\n\n\n\n\nLancer ensuite le producer en tapant:\n\n\n   java -cp \n$KAFKA_HOME\n/libs/*\n:. SimpleProducer Hello-Kafka\n\n\n\n\nPour voir le r\u00e9sultat saisi dans Kafka, il est possible d'utiliser le consommateur pr\u00e9d\u00e9fini de Kafka, \u00e0 condition d'utiliser le m\u00eame topic:\n\n\n  kafka-console-consumer.sh --zookeeper localhost:2181 --topic Hello-Kafka --from-beginning\n\n\n\n\nLe r\u00e9sultat devrait ressembler au suivant:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\nConsommateur\n\n\nPour cr\u00e9er un consommateur, proc\u00e9der de m\u00eame. Cr\u00e9er un fichier SimpleConsumer.java, avec le code suivant:\n\n\nimport\n \njava.util.Properties\n;\n\n\nimport\n \njava.util.Arrays\n;\n\n\nimport\n \norg.apache.kafka.clients.consumer.KafkaConsumer\n;\n\n\nimport\n \norg.apache.kafka.clients.consumer.ConsumerRecords\n;\n\n\nimport\n \norg.apache.kafka.clients.consumer.ConsumerRecord\n;\n\n\n\npublic\n \nclass\n \nSimpleConsumer\n \n{\n\n  \npublic\n \nstatic\n \nvoid\n \nmain\n(\nString\n[]\n \nargs\n)\n \nthrows\n \nException\n \n{\n\n    \nif\n(\nargs\n.\nlength\n \n==\n \n0\n){\n\n       \nSystem\n.\nout\n.\nprintln\n(\nEntrer le nom du topic\n);\n\n       \nreturn\n;\n\n    \n}\n\n    \nString\n \ntopicName\n \n=\n \nargs\n[\n0\n].\ntoString\n();\n\n    \nProperties\n \nprops\n \n=\n \nnew\n \nProperties\n();\n\n\n    \nprops\n.\nput\n(\nbootstrap.servers\n,\n \nlocalhost:9092\n);\n\n    \nprops\n.\nput\n(\ngroup.id\n,\n \ntest\n);\n\n    \nprops\n.\nput\n(\nenable.auto.commit\n,\n \ntrue\n);\n\n    \nprops\n.\nput\n(\nauto.commit.interval.ms\n,\n \n1000\n);\n\n    \nprops\n.\nput\n(\nsession.timeout.ms\n,\n \n30000\n);\n\n    \nprops\n.\nput\n(\nkey.deserializer\n,\n\n       \norg.apache.kafka.common.serialization.StringDeserializer\n);\n\n    \nprops\n.\nput\n(\nvalue.deserializer\n,\n\n       \norg.apache.kafka.common.serialization.StringDeserializer\n);\n\n\n    \nKafkaConsumer\nString\n,\n \nString\n \nconsumer\n \n=\n \nnew\n \nKafkaConsumer\n\n       \nString\n,\n \nString\n(\nprops\n);\n\n\n    \n// Kafka Consumer va souscrire a la liste de topics ici\n\n    \nconsumer\n.\nsubscribe\n(\nArrays\n.\nasList\n(\ntopicName\n));\n\n\n    \n// Afficher le nom du topic\n\n    \nSystem\n.\nout\n.\nprintln\n(\nSouscris au topic \n \n+\n \ntopicName\n);\n\n    \nint\n \ni\n \n=\n \n0\n;\n\n\n    \nwhile\n \n(\ntrue\n)\n \n{\n\n       \nConsumerRecords\nString\n,\n \nString\n \nrecords\n \n=\n \nconsumer\n.\npoll\n(\n100\n);\n\n       \nfor\n \n(\nConsumerRecord\nString\n,\n \nString\n \nrecord\n \n:\n \nrecords\n)\n\n\n       \n// Afficher l\noffset, clef et valeur des enregistrements du consommateur\n\n       \nSystem\n.\nout\n.\nprintf\n(\noffset = %d, key = %s, value = %s\\n\n,\n\n          \nrecord\n.\noffset\n(),\n \nrecord\n.\nkey\n(),\n \nrecord\n.\nvalue\n());\n\n    \n}\n\n  \n}\n\n\n}\n\n\n\n\n\nCompiler le consommateur avec:\n\n\n  javac -cp \n$KAFKA_HOME\n/libs/*\n:. SimpleConsumer.java\n\n\n\nPuis l'ex\u00e9cuter:\n\n\n  java -cp \n$KAFKA_HOME\n/libs/*\n:. SimpleConsumer Hello-Kafka\n\n\n\nLe consommateur est maintenant \u00e0 l'\u00e9coute du serveur de messagerie.\n\n\nOuvrir un nouveau terminal et relancer le producteur que vous aviez d\u00e9velopp\u00e9 tout \u00e0 l'heure. Le r\u00e9sultat dans le consommateur devrait ressembler \u00e0 ceci.\n\n\noffset = 32, key = 0, value = 0\noffset = 33, key = 1, value = 1\noffset = 34, key = 2, value = 2\noffset = 35, key = 3, value = 3\noffset = 36, key = 4, value = 4\noffset = 37, key = 5, value = 5\noffset = 38, key = 6, value = 6\noffset = 39, key = 7, value = 7\noffset = 40, key = 8, value = 8\noffset = 41, key = 9, value = 9\n\n\n\n\nInt\u00e9gration de Kafka avec Spark\n\n\nUtilit\u00e9\n\n\nKafka repr\u00e9sente une plateforme potentielle pour le messaging et l'int\u00e9gration de Spark streaming. Kafka agit comme \u00e9tant le hub central pour les flux de donn\u00e9es en temps r\u00e9el, qui sont ensuite trait\u00e9s avec des algorithmes complexes par Spark Streaming. Une fois les donn\u00e9es trait\u00e9es, Spark Streaming peut publier les r\u00e9sultats dans un autre topic Kafka ou les stokcer dans HDFS, d'autres bases de donn\u00e9es ou des dashboards.\n\n\nR\u00e9alisation\n\n\nPour faire cela, nous allons r\u00e9aliser un exemple simple, o\u00f9 Spark Streaming consomme des donn\u00e9es de Kafka pour r\u00e9aliser l'\u00e9ternel wordcount.\n\n\nDans votre machine locale, ouvrir IntelliJ IDEA (ou tout autre IDE de votre choix) et cr\u00e9er un nouveau projet Maven, avec les propri\u00e9t\u00e9s suivantes:\n\n\n  groupId\n:\n \nspark.kafka\n\n\n  artifactId\n:\n \nstream-kafka-spark\n\n\n  version\n:\n \n1\n\n\n\n\n\nUne fois le projet cr\u00e9\u00e9, modifier le fichier pom.xml pour qu'il ressemble \u00e0 ce qui suit:\n\n\n?xml version=\n1.0\n encoding=\nUTF-8\n?\n\n\nproject\n \nxmlns=\nhttp://maven.apache.org/POM/4.0.0\n\n         \nxmlns:xsi=\nhttp://www.w3.org/2001/XMLSchema-instance\n\n         \nxsi:schemaLocation=\nhttp://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\n\n    \nmodelVersion\n4.0.0\n/modelVersion\n\n\n    \ngroupId\nspark.kafka\n/groupId\n\n    \nartifactId\nstream-kafka-spark\n/artifactId\n\n    \nversion\n1\n/version\n\n\n    \ndependencies\n\n        \ndependency\n\n            \ngroupId\norg.apache.spark\n/groupId\n\n            \nartifactId\nspark-core_2.11\n/artifactId\n\n            \nversion\n2.2.1\n/version\n\n        \n/dependency\n\n        \ndependency\n\n            \ngroupId\norg.apache.spark\n/groupId\n\n            \nartifactId\nspark-streaming_2.11\n/artifactId\n\n            \nversion\n2.2.1\n/version\n\n        \n/dependency\n\n        \ndependency\n\n            \ngroupId\norg.apache.spark\n/groupId\n\n            \nartifactId\nspark-streaming-kafka-0-8_2.11\n/artifactId\n\n            \nversion\n2.2.0\n/version\n\n        \n/dependency\n\n        \ndependency\n\n            \ngroupId\norg.apache.kafka\n/groupId\n\n            \nartifactId\nkafka-clients\n/artifactId\n\n            \nversion\n0.8.2.0\n/version\n\n        \n/dependency\n\n    \n/dependencies\n\n\n    \nbuild\n\n        \nsourceDirectory\nsrc/main/java\n/sourceDirectory\n\n        \ntestSourceDirectory\nsrc/test/java\n/testSourceDirectory\n\n        \nplugins\n\n            \nplugin\n\n                \ngroupId\norg.apache.maven.plugins\n/groupId\n\n                \nartifactId\nmaven-compiler-plugin\n/artifactId\n\n                \nconfiguration\n\n                    \nsource\n1.8\n/source\n\n                    \ntarget\n1.8\n/target\n\n                \n/configuration\n\n            \n/plugin\n\n            \n!--\n\n\n                         Bind the maven-assembly-plugin to the package phase\n\n\n              this will create a jar file without the storm dependencies\n\n\n              suitable for deployment to a cluster.\n\n\n             --\n\n            \nplugin\n\n                \nartifactId\nmaven-assembly-plugin\n/artifactId\n\n                \nconfiguration\n\n                    \narchive\n\n                        \nmanifest\n\n                            \nmainClass\ntn.insat.tp3.SparkKafkaWordCount\n/mainClass\n\n                        \n/manifest\n\n                    \n/archive\n\n                    \ndescriptorRefs\n\n                        \ndescriptorRef\njar-with-dependencies\n/descriptorRef\n\n                    \n/descriptorRefs\n\n                \n/configuration\n\n            \n/plugin\n\n        \n/plugins\n\n    \n/build\n\n\n\n/project\n\n\n\nLe plugin \nmaven-assembly-plugin\n est utile pour pouvoir cr\u00e9er un jar contenant toutes les d\u00e9pendances du projet.\n\n\nCr\u00e9er ensuite un package \ntn.insat.tp3\n et une classe \nSparkKafkaWordCount\n. Le code de cette classe sera comme suit:\n\n\npackage\n \ntn.insat.tp3\n;\n\n\n\nimport\n \norg.apache.spark.SparkConf\n;\n\n\nimport\n \norg.apache.spark.streaming.Duration\n;\n\n\nimport\n \norg.apache.spark.streaming.api.java.*\n;\n\n\nimport\n \norg.apache.spark.streaming.kafka.KafkaUtils\n;\n\n\n\nimport\n \nscala.Tuple2\n;\n\n\nimport\n \njava.util.Arrays\n;\n\n\nimport\n \njava.util.HashMap\n;\n\n\nimport\n \njava.util.Map\n;\n\n\nimport\n \njava.util.regex.Pattern\n;\n\n\n\npublic\n \nclass\n \nSparkKafkaWordCount\n \n{\n\n    \nprivate\n \nstatic\n \nfinal\n \nPattern\n \nSPACE\n \n=\n \nPattern\n.\ncompile\n(\n \n);\n\n\n    \nprivate\n \nSparkKafkaWordCount\n()\n \n{\n\n    \n}\n\n\n    \npublic\n \nstatic\n \nvoid\n \nmain\n(\nString\n[]\n \nargs\n)\n \nthrows\n \nException\n \n{\n\n        \nif\n \n(\nargs\n.\nlength\n \n \n4\n)\n \n{\n\n            \nSystem\n.\nerr\n.\nprintln\n(\nUsage: SparkKafkaWordCount \nzkQuorum\n \ngroup\n \ntopics\n \nnumThreads\n);\n\n            \nSystem\n.\nexit\n(\n1\n);\n\n        \n}\n\n\n        \nSparkConf\n \nsparkConf\n \n=\n \nnew\n \nSparkConf\n().\nsetAppName\n(\nSparkKafkaWordCount\n);\n\n        \n// Creer le contexte avec une taille de batch de 2 secondes\n\n        \nJavaStreamingContext\n \njssc\n \n=\n \nnew\n \nJavaStreamingContext\n(\nsparkConf\n,\n\n            \nnew\n \nDuration\n(\n2000\n));\n\n\n        \nint\n \nnumThreads\n \n=\n \nInteger\n.\nparseInt\n(\nargs\n[\n3\n]);\n\n        \nMap\nString\n,\n \nInteger\n \ntopicMap\n \n=\n \nnew\n \nHashMap\n();\n\n        \nString\n[]\n \ntopics\n \n=\n \nargs\n[\n2\n].\nsplit\n(\n,\n);\n\n        \nfor\n \n(\nString\n \ntopic\n:\n \ntopics\n)\n \n{\n\n            \ntopicMap\n.\nput\n(\ntopic\n,\n \nnumThreads\n);\n\n        \n}\n\n\n        \nJavaPairReceiverInputDStream\nString\n,\n \nString\n \nmessages\n \n=\n\n                \nKafkaUtils\n.\ncreateStream\n(\njssc\n,\n \nargs\n[\n0\n],\n \nargs\n[\n1\n],\n \ntopicMap\n);\n\n\n        \nJavaDStream\nString\n \nlines\n \n=\n \nmessages\n.\nmap\n(\nTuple2\n::\n_2\n);\n\n\n        \nJavaDStream\nString\n \nwords\n \n=\n\n                \nlines\n.\nflatMap\n(\nx\n \n-\n \nArrays\n.\nasList\n(\nSPACE\n.\nsplit\n(\nx\n)).\niterator\n());\n\n\n        \nJavaPairDStream\nString\n,\n \nInteger\n \nwordCounts\n \n=\n\n                \nwords\n.\nmapToPair\n(\ns\n \n-\n \nnew\n \nTuple2\n(\ns\n,\n \n1\n))\n\n                     \n.\nreduceByKey\n((\ni1\n,\n \ni2\n)\n \n-\n \ni1\n \n+\n \ni2\n);\n\n\n        \nwordCounts\n.\nprint\n();\n\n        \njssc\n.\nstart\n();\n\n        \njssc\n.\nawaitTermination\n();\n\n    \n}\n\n\n}\n\n\n\n\n\nKafkaUtils API est utilis\u00e9e pour connecter le cluster Kafka \u00e0 Spark Streaming. La m\u00e9thode \ncreateStream\n est utilis\u00e9e, pour cr\u00e9er un flux en entr\u00e9e, qui extrait les messages des courtiers Kafka. Elle prend en param\u00e8tres:\n\n\n\n\nL' objet StreamingContext\n\n\nLe(s) serveur(s) Zookeeper\n\n\nL'identifiant du groupe du consommateur courant\n\n\nUne Map des topics \u00e0 consommateur\n\n\n\n\nCr\u00e9er une configuration Maven pour lancer la commande:\n\n\n  mvn clean compile assembly:single\n\n\n\n\nDans le r\u00e9pertoire target, un fichier stream-kafka-spark-1-jar-with-dependencies.jar est cr\u00e9\u00e9. Copier ce fichier dans le contenaire master, en utilisant le terminal d'IntelliJ:\n\n\n  docker cp target/stream-kafka-spark-1-jar-with-dependencies.jar hadoop-master:/root\n\n\n\n\nRevenir \u00e0 votre contenaire master, et lancer la commande spark-submit pour lancer l'\u00e9couteur de streaming spark.\n\n\nspark-submit --class tn.insat.tp3.SparkKafkaWordCount\n             --master local\n[\n2\n]\n\n             stream-kafka-spark-1-jar-with-dependencies.jar\n             localhost:2181 \ntest\n Hello-Kafka \n1\n \n out\n\n\n\n\nLes quatre options \u00e0 la fin de la commande sont requises par la classe  SparkKafkaWordCount et repr\u00e9sentent respectivement l'adresse de zookeeper, le nom du groupe auquel appartient le consommateur, le nom du topic et le nombre de threads utilis\u00e9s.\n\n\n\n\nRemarque\n\n\n>\nout est utilis\u00e9e pour stocker les r\u00e9sultats produits par spark streaming dans un fichier appel\u00e9 out.\n\n\n\n\nDans un autre terminal, lancer le producteur pr\u00e9d\u00e9fini de Kafka pour tester la r\u00e9action du consommateur spark streaming:\n\n\nkafka-console-producer.sh --broker-list localhost:9092 --topic Hello-Kafka\n\n\n\n\nEcrire du texte dans la fen\u00eatre du producteur. Ensuite, arr\u00eater le flux de spark-submit, et observer le contenu du fichier out. Il devra ressembler \u00e0 ce qui suit:\n\n\n\n\nHomework\n\n\nPour votre projet, vous allez utiliser Kafka pour g\u00e9rer les flux entrants et les envoyer \u00e0 Spark. Ces m\u00eames donn\u00e9es (ou une partie de ces donn\u00e9es) seront \u00e9galement stock\u00e9s dans HDFS pour un traitement par lot ult\u00e9rieur.\n\nR\u00e9aliser les liaisons n\u00e9cessaires entre Kafka et Spark, puis Kafka et HDFS. Vous devez avoir une id\u00e9e sur les traitements par lot que vous allez r\u00e9aliser, pour savoir quelles donn\u00e9es seront stock\u00e9es dans HDFS.", 
            "title": "TP3"
        }, 
        {
            "location": "/tp3/#tp3-la-collecte-de-donnees-avec-le-bus-kafka", 
            "text": "", 
            "title": "TP3 - La Collecte de donn\u00e9es avec le Bus Kafka"
        }, 
        {
            "location": "/tp3/#telecharger-pdf", 
            "text": "", 
            "title": "T\u00e9l\u00e9charger PDF"
        }, 
        {
            "location": "/tp3/#objectifs-du-tp", 
            "text": "Utilisation de Kafka pour une collecte de donn\u00e9es distribu\u00e9e, et int\u00e9gration avec Spark.", 
            "title": "Objectifs du TP"
        }, 
        {
            "location": "/tp3/#outils-et-versions", 
            "text": "Apache Kafka  Version 2.11-0.8.2.1  Apache Hadoop  Version: 2.7.2  Apache Spark  Version: 2.2.1  Docker  Version 17.09.1  IntelliJ IDEA  Version Ultimate 2016.1 (ou tout autre IDE de votre choix)  Java  Version 1.8  Unix-like ou Unix-based Systems (Divers Linux et MacOS)", 
            "title": "Outils et Versions"
        }, 
        {
            "location": "/tp3/#kafka", 
            "text": "", 
            "title": "Kafka"
        }, 
        {
            "location": "/tp3/#quest-ce-quun-systeme-de-messaging", 
            "text": "Un syst\u00e8me de messaging ( Messaging System ) est responsable du transfert de donn\u00e9es d'une application \u00e0 une autre, de mani\u00e8re \u00e0 ce que les applications puissent se concentrer sur les donn\u00e9es sans s'inqui\u00e9ter de la mani\u00e8re de les partager ou de les collecter. Le messaging distribu\u00e9 est bas\u00e9 sur le principe de file de message fiable. Les messages sont stock\u00e9s de mani\u00e8re asynchrone dans des files d'attente entre les applications clientes et le syst\u00e8me de messaging.  Deux types de patrons de messaging existent: Les syst\u00e8mes \" point \u00e0 point \" et les syst\u00e8mes \" publish-subscribe \".", 
            "title": "Qu'est-ce qu'un syst\u00e8me de messaging?"
        }, 
        {
            "location": "/tp3/#1-systemes-de-messaging-point-a-point", 
            "text": "Dans un syst\u00e8me point \u00e0 point, les messages sont stock\u00e9s dans une file. un ou plusieurs consommateurs peuvent consommer les message dans la file, mais un message ne peut \u00eatre consomm\u00e9 que par un seul consommateur \u00e0 la fois. Une fois le consommateur lit le message, ce dernier dispara\u00eet de la file.", 
            "title": "1. Syst\u00e8mes de messaging Point \u00e0 Point"
        }, 
        {
            "location": "/tp3/#2-systemes-de-messaging-publishsubscribe", 
            "text": "Dans un syst\u00e8me publish-subscribe, les messages sont stock\u00e9s dans un \" topic \". Contrairement \u00e0 un syst\u00e8me point \u00e0 point, les consommateurs peuvent souscrire \u00e0 un ou plusieurs topics et consommer tous les messages de ce topic.", 
            "title": "2. Syst\u00e8mes de messaging Publish/Subscribe"
        }, 
        {
            "location": "/tp3/#presentation-de-kafka", 
            "text": "Apache  Kafka  est une plateforme de streaming qui b\u00e9n\u00e9ficie de trois fonctionnalit\u00e9s:   Elle vous permet de publier et souscrire \u00e0 un flux d'enregistrements. Elle ressemble ainsi \u00e0 une file demessage ou un syst\u00e8me de messaging d'entreprise.  Elle permet de stocker des flux d'enregistrements d'une fa\u00e7on tol\u00e9rante aux pannes.  Elle vous permet de traiter (au besoin) les enregistrements au fur et \u00e0 mesure qu'ils arrivent.    Les principaux avantages de Kafka sont:   La fiabliti\u00e9 : Kafka est distribu\u00e9, partitionn\u00e9, r\u00e9pliqu\u00e9 et tol\u00e9rent aux fautes.  La scalabilit\u00e9 : Kafka se met \u00e0 l'\u00e9chelle facilement et sans temps d'arr\u00eat.  La durabilit\u00e9 : Kafka utilise un  commit log  distribu\u00e9, ce qui permet de stocker les messages sur le disque le plus vite possible.  La performance : Kafka a un d\u00e9bit \u00e9lev\u00e9 pour la publication et l'abonnement.", 
            "title": "Pr\u00e9sentation de Kafka"
        }, 
        {
            "location": "/tp3/#architecture-de-kafka", 
            "text": "Pour comprendre le fonctionnement de Kafka, il faut d'abord se familiariser avec le vocabulaire suivant:   Topic : Un flux de messages appartenant \u00e0 une cat\u00e9gorie particuli\u00e8re. Les donn\u00e9es sont stock\u00e9es dans des topics.  Partitions : Chaque topic est divis\u00e9 en partitions. Pour chaque topic, Kafka conserve un minimum d'une partition. Chaque partition contient des messages dans une s\u00e9quence ordonn\u00e9e immuable. Une partition est impl\u00e9ment\u00e9e comme un ensemble de s\u00e8gments de tailles \u00e9gales.  Offset : Les enregistrements d'une partition ont chacun un identifiant s\u00e9quentiel appel\u00e9  offset , qui permet de l'identifier de mani\u00e8re unique dans la partition.  R\u00e9pliques : Les r\u00e9pliques sont des  backups  d'une partition. Elles ne sont jamais lues ni modifi\u00e9es par les acteurs externes, elles servent uniquement \u00e0 pr\u00e9venir la perte de donn\u00e9es.  Brokers : Les  brokers  (ou courtiers) sont de simples syst\u00e8mes responsables de maintenir les donn\u00e9es publi\u00e9es. Chaque courtier peut avoir z\u00e9ro ou plusieurs partitions par topic. Si un topic admet N partitions et N courtiers, chaque courtier va avoir une seule partition. Si le nombre de courtiers est plus grand que celui des partitions, certains n'auront aucune partition de ce topic.  Cluster : Un syst\u00e8me Kafka ayant plus qu'un seul Broker est appel\u00e9  cluster Kafka . L'ajout de nouveau brokers est fait de mani\u00e8re transparente sans temps d'arr\u00eat.  Producers : Les producteurs sont les \u00e9diteurs de messages \u00e0 un ou plusieurs topics Kafka. Ils envoient des donn\u00e9es aux courtiers Kafka. Chaque fois qu'un producteur publie un message \u00e0 un courtier, ce dernier rattache le message au dernier s\u00e8gment, ajout\u00e9 ainsi \u00e0 une partition. Un producteur peut \u00e9galement envoyer un message \u00e0 une partition particuli\u00e8re.  Consumers : Les consommateurs lisent les donn\u00e9es \u00e0 partir des brokers. Ils souscrivent \u00e0 un ou plusieurs topics, et consomment les messages publi\u00e9s en extrayant les donn\u00e9es \u00e0 partir des brokers.  Leaders : Le leader est le noeud responsable de toutes les lectures et \u00e9critures d'une partition donn\u00e9e. Chaque partition a un serveur jouant le r\u00f4le de leader.  Follower : C'est un noeud qui suit les instructions du leader. Si le leader tombe en panne, l'un des followers deviendra automatiquement le nouveau leader.   La figure suivante montre un exemple de flux entre les diff\u00e9rentes parties d'un syst\u00e8me Kafka:     Dans cet exemple, un topic est configur\u00e9 en trois partitions. La partition 1 admet deux enregistrements d'offsets respectifs 0 et 1, la partition 2 a quatre enregistrements et la partition 3 a un seul enregistrement.  En supposant que, si le facteur de r\u00e9plication du topic est de 3, alors Kafka va cr\u00e9er trois r\u00e9pliques identiques de chaque partition et les placer dans le cluster pour les rendre disponibles pour toutes les op\u00e9rations. L'identifiant de la r\u00e9plique est le m\u00eame que l'identifiant du serveur qui l'h\u00e9berge. Pour \u00e9quilibrer la charge dans le cluster, chaque broker stocke une ou plusieurs de ces partitions. Plusieurs producteurs et consommateurs peuvent publier et extraire les messages au m\u00eame moment.", 
            "title": "Architecture de Kafka"
        }, 
        {
            "location": "/tp3/#kafka-et-zookeeper", 
            "text": "Zookeeper  est un service centralis\u00e9 permettant de maintenir l'information de configuration, de nommage, de synchronisation et de services de groupe. Ces services sont utilis\u00e9s par les applications distribu\u00e9es en g\u00e9n\u00e9ral, et par Kafka en particulier. Pour \u00e9viter la complexit\u00e9 et difficult\u00e9 de leur impl\u00e9mentation manuelle, Zookeeper est utilis\u00e9.   Un cluster Kafka consiste typiquement en plusieurs courtiers (Brokers) pour maintenir la r\u00e9partition de charge. Ces courtiers sont stateless, c'est pour cela qu'ils utilisent Zookeeper pour maintenir l'\u00e9tat du cluster. Un courtier peut g\u00e9rer des centaines de milliers de lectures et \u00e9critures par seconde, et chaque courtier peut g\u00e9rer des t\u00e9ra-octets de messages sans impact sur la performance.  Zookeeper est utilis\u00e9 pour g\u00e9rer et coordonner les courtiers Kafka. Il permet de notifier les producteurs et consommateurs de messages de la pr\u00e9sence de tout nouveau courtier, ou de l'\u00e9chec d'un courtier dans le cluster.", 
            "title": "Kafka et Zookeeper"
        }, 
        {
            "location": "/tp3/#installation", 
            "text": "Kafka a \u00e9t\u00e9 install\u00e9 sur le m\u00eame cluster que les deux TP pr\u00e9c\u00e9dents. Si vous disposez des contenaires, vous n'avez rien \u00e0 faire. Sinon, vous pourrez les installer avec Docker comme suit:   Cloner le repo github contenant les fichiers n\u00e9cessaires pour le lancement des contenaires et leur configuration:    git clone https://github.com/liliasfaxi/hadoop-cluster-docker  Construire l'image Docker \u00e0 partir du fichier Dockerfile fourni.     cd  hadoop-cluster-docker\n  ./build-image.sh  D\u00e9marrer les trois contenaires:    sudo ./start-container.sh \nLe r\u00e9sultat de cette ex\u00e9cution sera le suivant:    start hadoop-master container...\n  start hadoop-slave1 container...\n  start hadoop-slave2 container...\n  root@hadoop-master:~#  Lancer Kafka et Zookeeper en tapant :    ./start-kafka-zookeeper.sh \nLes deux d\u00e9mons Kafka et Zookeeper seront lanc\u00e9s. Vous pourrez v\u00e9rifier cela en tapant  jps  pour voir quels processus Java sont en ex\u00e9cution, vous devriez trouver les processus suivants:     2756  Kafka\n   53  QuorumPeerMain\n   6349  Jps", 
            "title": "Installation"
        }, 
        {
            "location": "/tp3/#premiere-utilisation-de-kafka", 
            "text": "", 
            "title": "Premi\u00e8re utilisation de Kafka"
        }, 
        {
            "location": "/tp3/#creation-dun-topic", 
            "text": "Pour g\u00e9rer les topics, Kafka fournit une commande appel\u00e9e  kafka-topics.sh . \nDans un nouveau terminal, taper la commande suivante pour cr\u00e9er un nouveau topic appel\u00e9 \"Hello-Kafka\".    kafka-topics.sh --create --zookeeper localhost:2181\n                  --replication-factor  1  --partitions  1 \n                  --topic Hello-Kafka   Attention  Cette commande fonctionne car nous avions rajout\u00e9 /usr/local/kafka/bin \u00e0 la variable d'environnement PATH. Si ce n'\u00e9tait pas le cas, on aurait du appeler /usr/local/kafka/bin/kafka-topics.sh   Pour afficher la liste des topics existants, il faudra utiliser:    kafka-topics.sh --list --zookeeper localhost:2181  Le r\u00e9sultat devrait \u00eatre (parmi un grand nombre de lignes d'INFO):    Hello-Kafka", 
            "title": "Cr\u00e9ation d'un topic"
        }, 
        {
            "location": "/tp3/#exemple-producteur-consommateur", 
            "text": "Kafka fournit un exemple de producteur standard que vous pouvez directement utiliser. Il suffit de taper:    kafka-console-producer.sh --broker-list localhost:9092 --topic Hello-Kafka  Tout ce que vous taperez dor\u00e9navant sur la console sera envoy\u00e9 \u00e0 Kafka. L'option  --broker-list  permet de d\u00e9finir la liste des courtiers auxquels vous enverrez le message. Pour l'instant, vous n'en disposez que d'un, et il est d\u00e9ploy\u00e9 \u00e0 l'adresse localhost:9092.  Pour lancer un consommateur, utiliser:    kafka-console-consumer.sh --zookeeper localhost:2181 \u2014topic Hello-Kafka\n--from-beginning  Le r\u00e9sultat devra ressembler au suivant:", 
            "title": "Exemple Producteur Consommateur"
        }, 
        {
            "location": "/tp3/#configuration-de-plusieurs-brokers", 
            "text": "Dans ce qui pr\u00e9c\u00e8de, nous avons configur\u00e9 Kafka pour lancer un seul broker. Pour cr\u00e9er plusieurs brokers, il suffit de dupliquer le fichier  $KAFKA_HOME/config/server.properties  autant de fois que n\u00e9cessaire. Dans notre cas, nous allons cr\u00e9er deux autre fichiers:  server-one.properties  et  server-two.properties , puis nous modifions les param\u00e8tres suivants comme suit:     ### config/server-one.properties    broker.id   =   1    listeners = PLAINTEXT://localhost:9093    log.dirs = /tmp/kafka-logs-1 \n\n   ### config/server-two.properties    broker.id   =   2    listeners = PLAINTEXT://localhost:9094    log.dirs = /tmp/kafka-logs-2   Pour d\u00e9marrer les diff\u00e9rents brokers, il suffit d'appeler  kafka-server-start.sh  avec les nouveaux fichiers de configuration.   kafka-server-start.sh  $KAFKA_HOME /config/server.properties  \n kafka-server-start.sh  $KAFKA_HOME /config/server-one.properties  \n kafka-server-start.sh  $KAFKA_HOME /config/server-two.properties    Lancer  jps  pour voir les trois serveurs s'ex\u00e9cuter.", 
            "title": "Configuration de plusieurs brokers"
        }, 
        {
            "location": "/tp3/#creation-dune-application-personnalisee", 
            "text": "Nous allons dans cette partie cr\u00e9er une application pour publier et consommer des messages de Kafka. Pour cela, nous allons utiliser KafkaProducer API et KafkaConsumer API.", 
            "title": "Cr\u00e9ation d'une application personnalis\u00e9e"
        }, 
        {
            "location": "/tp3/#producteur", 
            "text": "Pour cr\u00e9er un producteur Kafka, cr\u00e9er un fichier dans un r\u00e9pertoire de votre choix dans le contenaire master, intitul\u00e9  SimpleProducer.java . Son code est le suivant:  import   java.util.Properties ;  import   org.apache.kafka.clients.producer.Producer ;  import   org.apache.kafka.clients.producer.KafkaProducer ;  import   org.apache.kafka.clients.producer.ProducerRecord ; \n\n  public   class   SimpleProducer   { \n\n    public   static   void   main ( String []   args )   throws   Exception { \n\n       // Verifier que le topic est donne en argument \n       if ( args . length   ==   0 ){ \n          System . out . println ( Entrer le nom du topic ); \n          return ; \n       } \n\n       // Assigner topicName a une variable \n       String   topicName   =   args [ 0 ]. toString (); \n\n       // Creer une instance de proprietes pour acceder aux configurations du producteur \n       Properties   props   =   new   Properties (); \n\n       // Assigner l identifiant du serveur kafka \n       props . put ( bootstrap.servers ,   localhost:9092 ); \n\n       // Definir un acquittement pour les requetes du producteur \n       props . put ( acks ,   all ); \n\n       // Si la requete echoue, le producteur peut reessayer automatiquemt \n       props . put ( retries ,   0 ); \n\n       // Specifier la taille du buffer size dans la config \n       props . put ( batch.size ,   16384 ); \n\n       // buffer.memory controle le montant total de memoire disponible au producteur pour le buffering \n       props . put ( buffer.memory ,   33554432 ); \n\n       props . put ( key.serializer , \n          org.apache.kafka.common.serialization.StringSerializer ); \n\n       props . put ( value.serializer , \n          org.apache.kafka.common.serialization.StringSerializer ); \n\n       Producer String ,   String   producer   =   new   KafkaProducer \n          String ,   String ( props ); \n\n       for ( int   i   =   0 ;   i     10 ;   i ++) \n          producer . send ( new   ProducerRecord String ,   String ( topicName , \n             Integer . toString ( i ),   Integer . toString ( i ))); \n                System . out . println ( Message envoye avec succes ); \n                producer . close (); \n    } \n  }   ProducerRecord est une paire clef/valeur envoy\u00e9e au cluster Kafka. Son constructeur peut prendre 4, 3 ou 2 param\u00e8tres, selon le besoin. Les signatures autoris\u00e9es sont comme suit:     public   ProducerRecord   ( string   topic ,   int   partition ,   k   key ,   v   value ){...} \n   public   ProducerRecord   ( string   topic ,   k   key ,   v   value ){...} \n   public   ProducerRecord   ( string   topic ,   v   value ){...}   Pour compiler ce code, taper dans la console (en vous positionnant dans le r\u00e9pertoire qui contient le fichier SimpleProducer.java):     javac -cp  $KAFKA_HOME /libs/* :. SimpleProducer.java  Lancer ensuite le producer en tapant:     java -cp  $KAFKA_HOME /libs/* :. SimpleProducer Hello-Kafka  Pour voir le r\u00e9sultat saisi dans Kafka, il est possible d'utiliser le consommateur pr\u00e9d\u00e9fini de Kafka, \u00e0 condition d'utiliser le m\u00eame topic:    kafka-console-consumer.sh --zookeeper localhost:2181 --topic Hello-Kafka --from-beginning  Le r\u00e9sultat devrait ressembler au suivant:  1\n2\n3\n4\n5\n6\n7\n8\n9\n10", 
            "title": "Producteur"
        }, 
        {
            "location": "/tp3/#consommateur", 
            "text": "Pour cr\u00e9er un consommateur, proc\u00e9der de m\u00eame. Cr\u00e9er un fichier SimpleConsumer.java, avec le code suivant:  import   java.util.Properties ;  import   java.util.Arrays ;  import   org.apache.kafka.clients.consumer.KafkaConsumer ;  import   org.apache.kafka.clients.consumer.ConsumerRecords ;  import   org.apache.kafka.clients.consumer.ConsumerRecord ;  public   class   SimpleConsumer   { \n   public   static   void   main ( String []   args )   throws   Exception   { \n     if ( args . length   ==   0 ){ \n        System . out . println ( Entrer le nom du topic ); \n        return ; \n     } \n     String   topicName   =   args [ 0 ]. toString (); \n     Properties   props   =   new   Properties (); \n\n     props . put ( bootstrap.servers ,   localhost:9092 ); \n     props . put ( group.id ,   test ); \n     props . put ( enable.auto.commit ,   true ); \n     props . put ( auto.commit.interval.ms ,   1000 ); \n     props . put ( session.timeout.ms ,   30000 ); \n     props . put ( key.deserializer , \n        org.apache.kafka.common.serialization.StringDeserializer ); \n     props . put ( value.deserializer , \n        org.apache.kafka.common.serialization.StringDeserializer ); \n\n     KafkaConsumer String ,   String   consumer   =   new   KafkaConsumer \n        String ,   String ( props ); \n\n     // Kafka Consumer va souscrire a la liste de topics ici \n     consumer . subscribe ( Arrays . asList ( topicName )); \n\n     // Afficher le nom du topic \n     System . out . println ( Souscris au topic    +   topicName ); \n     int   i   =   0 ; \n\n     while   ( true )   { \n        ConsumerRecords String ,   String   records   =   consumer . poll ( 100 ); \n        for   ( ConsumerRecord String ,   String   record   :   records ) \n\n        // Afficher l offset, clef et valeur des enregistrements du consommateur \n        System . out . printf ( offset = %d, key = %s, value = %s\\n , \n           record . offset (),   record . key (),   record . value ()); \n     } \n   }  }   Compiler le consommateur avec:    javac -cp  $KAFKA_HOME /libs/* :. SimpleConsumer.java  Puis l'ex\u00e9cuter:    java -cp  $KAFKA_HOME /libs/* :. SimpleConsumer Hello-Kafka  Le consommateur est maintenant \u00e0 l'\u00e9coute du serveur de messagerie.  Ouvrir un nouveau terminal et relancer le producteur que vous aviez d\u00e9velopp\u00e9 tout \u00e0 l'heure. Le r\u00e9sultat dans le consommateur devrait ressembler \u00e0 ceci.  offset = 32, key = 0, value = 0\noffset = 33, key = 1, value = 1\noffset = 34, key = 2, value = 2\noffset = 35, key = 3, value = 3\noffset = 36, key = 4, value = 4\noffset = 37, key = 5, value = 5\noffset = 38, key = 6, value = 6\noffset = 39, key = 7, value = 7\noffset = 40, key = 8, value = 8\noffset = 41, key = 9, value = 9", 
            "title": "Consommateur"
        }, 
        {
            "location": "/tp3/#integration-de-kafka-avec-spark", 
            "text": "", 
            "title": "Int\u00e9gration de Kafka avec Spark"
        }, 
        {
            "location": "/tp3/#utilite", 
            "text": "Kafka repr\u00e9sente une plateforme potentielle pour le messaging et l'int\u00e9gration de Spark streaming. Kafka agit comme \u00e9tant le hub central pour les flux de donn\u00e9es en temps r\u00e9el, qui sont ensuite trait\u00e9s avec des algorithmes complexes par Spark Streaming. Une fois les donn\u00e9es trait\u00e9es, Spark Streaming peut publier les r\u00e9sultats dans un autre topic Kafka ou les stokcer dans HDFS, d'autres bases de donn\u00e9es ou des dashboards.", 
            "title": "Utilit\u00e9"
        }, 
        {
            "location": "/tp3/#realisation", 
            "text": "Pour faire cela, nous allons r\u00e9aliser un exemple simple, o\u00f9 Spark Streaming consomme des donn\u00e9es de Kafka pour r\u00e9aliser l'\u00e9ternel wordcount.  Dans votre machine locale, ouvrir IntelliJ IDEA (ou tout autre IDE de votre choix) et cr\u00e9er un nouveau projet Maven, avec les propri\u00e9t\u00e9s suivantes:    groupId :   spark.kafka    artifactId :   stream-kafka-spark    version :   1   Une fois le projet cr\u00e9\u00e9, modifier le fichier pom.xml pour qu'il ressemble \u00e0 ce qui suit:  ?xml version= 1.0  encoding= UTF-8 ?  project   xmlns= http://maven.apache.org/POM/4.0.0 \n          xmlns:xsi= http://www.w3.org/2001/XMLSchema-instance \n          xsi:schemaLocation= http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd \n     modelVersion 4.0.0 /modelVersion \n\n     groupId spark.kafka /groupId \n     artifactId stream-kafka-spark /artifactId \n     version 1 /version \n\n     dependencies \n         dependency \n             groupId org.apache.spark /groupId \n             artifactId spark-core_2.11 /artifactId \n             version 2.2.1 /version \n         /dependency \n         dependency \n             groupId org.apache.spark /groupId \n             artifactId spark-streaming_2.11 /artifactId \n             version 2.2.1 /version \n         /dependency \n         dependency \n             groupId org.apache.spark /groupId \n             artifactId spark-streaming-kafka-0-8_2.11 /artifactId \n             version 2.2.0 /version \n         /dependency \n         dependency \n             groupId org.apache.kafka /groupId \n             artifactId kafka-clients /artifactId \n             version 0.8.2.0 /version \n         /dependency \n     /dependencies \n\n     build \n         sourceDirectory src/main/java /sourceDirectory \n         testSourceDirectory src/test/java /testSourceDirectory \n         plugins \n             plugin \n                 groupId org.apache.maven.plugins /groupId \n                 artifactId maven-compiler-plugin /artifactId \n                 configuration \n                     source 1.8 /source \n                     target 1.8 /target \n                 /configuration \n             /plugin \n             !--                           Bind the maven-assembly-plugin to the package phase                this will create a jar file without the storm dependencies                suitable for deployment to a cluster.               -- \n             plugin \n                 artifactId maven-assembly-plugin /artifactId \n                 configuration \n                     archive \n                         manifest \n                             mainClass tn.insat.tp3.SparkKafkaWordCount /mainClass \n                         /manifest \n                     /archive \n                     descriptorRefs \n                         descriptorRef jar-with-dependencies /descriptorRef \n                     /descriptorRefs \n                 /configuration \n             /plugin \n         /plugins \n     /build  /project  \nLe plugin  maven-assembly-plugin  est utile pour pouvoir cr\u00e9er un jar contenant toutes les d\u00e9pendances du projet.  Cr\u00e9er ensuite un package  tn.insat.tp3  et une classe  SparkKafkaWordCount . Le code de cette classe sera comme suit:  package   tn.insat.tp3 ;  import   org.apache.spark.SparkConf ;  import   org.apache.spark.streaming.Duration ;  import   org.apache.spark.streaming.api.java.* ;  import   org.apache.spark.streaming.kafka.KafkaUtils ;  import   scala.Tuple2 ;  import   java.util.Arrays ;  import   java.util.HashMap ;  import   java.util.Map ;  import   java.util.regex.Pattern ;  public   class   SparkKafkaWordCount   { \n     private   static   final   Pattern   SPACE   =   Pattern . compile (   ); \n\n     private   SparkKafkaWordCount ()   { \n     } \n\n     public   static   void   main ( String []   args )   throws   Exception   { \n         if   ( args . length     4 )   { \n             System . err . println ( Usage: SparkKafkaWordCount  zkQuorum   group   topics   numThreads ); \n             System . exit ( 1 ); \n         } \n\n         SparkConf   sparkConf   =   new   SparkConf (). setAppName ( SparkKafkaWordCount ); \n         // Creer le contexte avec une taille de batch de 2 secondes \n         JavaStreamingContext   jssc   =   new   JavaStreamingContext ( sparkConf , \n             new   Duration ( 2000 )); \n\n         int   numThreads   =   Integer . parseInt ( args [ 3 ]); \n         Map String ,   Integer   topicMap   =   new   HashMap (); \n         String []   topics   =   args [ 2 ]. split ( , ); \n         for   ( String   topic :   topics )   { \n             topicMap . put ( topic ,   numThreads ); \n         } \n\n         JavaPairReceiverInputDStream String ,   String   messages   = \n                 KafkaUtils . createStream ( jssc ,   args [ 0 ],   args [ 1 ],   topicMap ); \n\n         JavaDStream String   lines   =   messages . map ( Tuple2 :: _2 ); \n\n         JavaDStream String   words   = \n                 lines . flatMap ( x   -   Arrays . asList ( SPACE . split ( x )). iterator ()); \n\n         JavaPairDStream String ,   Integer   wordCounts   = \n                 words . mapToPair ( s   -   new   Tuple2 ( s ,   1 )) \n                      . reduceByKey (( i1 ,   i2 )   -   i1   +   i2 ); \n\n         wordCounts . print (); \n         jssc . start (); \n         jssc . awaitTermination (); \n     }  }   KafkaUtils API est utilis\u00e9e pour connecter le cluster Kafka \u00e0 Spark Streaming. La m\u00e9thode  createStream  est utilis\u00e9e, pour cr\u00e9er un flux en entr\u00e9e, qui extrait les messages des courtiers Kafka. Elle prend en param\u00e8tres:   L' objet StreamingContext  Le(s) serveur(s) Zookeeper  L'identifiant du groupe du consommateur courant  Une Map des topics \u00e0 consommateur   Cr\u00e9er une configuration Maven pour lancer la commande:    mvn clean compile assembly:single  Dans le r\u00e9pertoire target, un fichier stream-kafka-spark-1-jar-with-dependencies.jar est cr\u00e9\u00e9. Copier ce fichier dans le contenaire master, en utilisant le terminal d'IntelliJ:    docker cp target/stream-kafka-spark-1-jar-with-dependencies.jar hadoop-master:/root  Revenir \u00e0 votre contenaire master, et lancer la commande spark-submit pour lancer l'\u00e9couteur de streaming spark.  spark-submit --class tn.insat.tp3.SparkKafkaWordCount\n             --master local [ 2 ] \n             stream-kafka-spark-1-jar-with-dependencies.jar\n             localhost:2181  test  Hello-Kafka  1    out  Les quatre options \u00e0 la fin de la commande sont requises par la classe  SparkKafkaWordCount et repr\u00e9sentent respectivement l'adresse de zookeeper, le nom du groupe auquel appartient le consommateur, le nom du topic et le nombre de threads utilis\u00e9s.   Remarque  > out est utilis\u00e9e pour stocker les r\u00e9sultats produits par spark streaming dans un fichier appel\u00e9 out.   Dans un autre terminal, lancer le producteur pr\u00e9d\u00e9fini de Kafka pour tester la r\u00e9action du consommateur spark streaming:  kafka-console-producer.sh --broker-list localhost:9092 --topic Hello-Kafka  Ecrire du texte dans la fen\u00eatre du producteur. Ensuite, arr\u00eater le flux de spark-submit, et observer le contenu du fichier out. Il devra ressembler \u00e0 ce qui suit:", 
            "title": "R\u00e9alisation"
        }, 
        {
            "location": "/tp3/#homework", 
            "text": "Pour votre projet, vous allez utiliser Kafka pour g\u00e9rer les flux entrants et les envoyer \u00e0 Spark. Ces m\u00eames donn\u00e9es (ou une partie de ces donn\u00e9es) seront \u00e9galement stock\u00e9s dans HDFS pour un traitement par lot ult\u00e9rieur. \nR\u00e9aliser les liaisons n\u00e9cessaires entre Kafka et Spark, puis Kafka et HDFS. Vous devez avoir une id\u00e9e sur les traitements par lot que vous allez r\u00e9aliser, pour savoir quelles donn\u00e9es seront stock\u00e9es dans HDFS.", 
            "title": "Homework"
        }
    ]
}